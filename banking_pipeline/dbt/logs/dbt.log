[0m17:06:39.161605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001531F7E5E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015321802650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015321803670>]}


============================== 17:06:39.167078 | b05ee03e-dac6-4f15-90bd-cc483242874e ==============================
[0m17:06:39.167078 [info ] [MainThread]: Running with dbt=1.10.17
[0m17:06:39.167078 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'True', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Supriya\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt seed --full-refresh --debug', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Supriya\\Documents\\banking_pipeline - Copy\\dbt\\logs'}
[0m17:06:41.598304 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:06:41.600822 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:06:41.600822 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:06:42.911784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153337C1870>]}
[0m17:06:43.034441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015321886A10>]}
[0m17:06:43.044513 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m17:06:43.638193 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:06:43.639203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015321800940>]}
[0m17:06:43.655827 [debug] [MainThread]: checksum: a50d730a31f4c1af850aa8110f398f1e1657a0325ec5753dae2929b817189a3e, vars: {}, profile: , target: , version: 1.10.17
[0m17:06:43.657772 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:06:43.657772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015334B00B20>]}
[0m17:06:47.358723 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:06:47.379404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015323F56E60>]}
[0m17:06:47.617918 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:06:47.628131 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:06:47.648713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001533539CA30>]}
[0m17:06:47.650691 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 716 macros, 3 unit tests
[0m17:06:47.651927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001533539CBE0>]}
[0m17:06:47.653917 [info ] [MainThread]: 
[0m17:06:47.653917 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:06:47.653917 [info ] [MainThread]: 
[0m17:06:47.658796 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:06:47.658796 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:06:47.670673 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:06:47.670673 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:06:47.690060 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:06:47.690060 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:06:47.690060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:48.219977 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4ea8-15d6-8fd9-b8088a03a6d9) - Created
[0m17:06:48.739220 [debug] [ThreadPool]: SQL status: OK in 1.050 seconds
[0m17:06:48.742213 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-4ea8-15d6-8fd9-b8088a03a6d9, command-id=01f0ec2e-4eba-1311-8874-06a488e949df) - Closing
[0m17:06:48.742213 [debug] [ThreadPool]: On list_workspace: Close
[0m17:06:48.747375 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4ea8-15d6-8fd9-b8088a03a6d9) - Closing
[0m17:06:48.884462 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m17:06:48.886415 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m17:06:48.906985 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m17:06:48.906985 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m17:06:48.906985 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:49.345243 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4f53-1346-9973-585497b1450a) - Created
[0m17:06:49.834054 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m17:06:49.834054 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-4f53-1346-9973-585497b1450a, command-id=01f0ec2e-4f68-1961-bfb3-502b3487e404) - Closing
[0m17:06:49.834054 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m17:06:49.842350 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4f53-1346-9973-585497b1450a) - Closing
[0m17:06:50.018298 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m17:06:50.025230 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m17:06:50.042495 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m17:06:50.042495 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m17:06:50.042495 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:50.465177 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4fff-1d54-8427-2c55211de570) - Created
[0m17:06:50.973398 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m17:06:50.978143 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-4fff-1d54-8427-2c55211de570, command-id=01f0ec2e-5013-12f4-a096-016aef39f14e) - Closing
[0m17:06:50.978143 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m17:06:50.978143 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-4fff-1d54-8427-2c55211de570) - Closing
[0m17:06:51.167095 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m17:06:51.167095 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m17:06:51.184531 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m17:06:51.186994 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m17:06:51.186994 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:51.608095 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-50ac-1147-9ac2-7dc3b260f4d5) - Created
[0m17:06:52.119068 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m17:06:52.126041 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-50ac-1147-9ac2-7dc3b260f4d5, command-id=01f0ec2e-50c0-1058-b2f8-c5ae1a3fd0ba) - Closing
[0m17:06:52.126041 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m17:06:52.130293 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-50ac-1147-9ac2-7dc3b260f4d5) - Closing
[0m17:06:52.265136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153218848B0>]}
[0m17:06:52.275278 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m17:06:52.275278 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m17:06:52.280400 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m17:06:52.280981 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m17:06:52.282164 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m17:06:52.282164 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m17:06:52.286264 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:06:52.286264 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153355C10F0>]}
[0m17:06:52.363137 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m17:06:52.364095 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m17:06:52.365097 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m17:06:52.827229 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec2e-5169-1366-9a37-d564d7780301) - Created
[0m17:06:54.575947 [debug] [Thread-5 (]: SQL status: OK in 2.200 seconds
[0m17:06:54.580361 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec2e-5169-1366-9a37-d564d7780301, command-id=01f0ec2e-5179-1772-9f8b-88a2f94b19c2) - Closing
[0m17:06:54.601930 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m17:06:54.601930 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m17:06:56.459914 [debug] [Thread-5 (]: SQL status: OK in 1.860 seconds
[0m17:06:56.459914 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec2e-5169-1366-9a37-d564d7780301, command-id=01f0ec2e-5299-1cd1-93db-a184df627276) - Closing
[0m17:06:56.464594 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m17:06:56.502011 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m17:06:56.503011 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec2e-5169-1366-9a37-d564d7780301) - Closing
[0m17:06:56.646335 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153352E8AC0>]}
[0m17:06:56.650212 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 4.35s]
[0m17:06:56.651023 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m17:06:56.651023 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m17:06:56.657203 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m17:06:56.660194 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m17:06:56.660505 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m17:06:56.660505 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m17:06:56.660505 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m17:06:56.677289 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m17:06:56.677289 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m17:06:56.677289 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m17:06:57.089399 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec2e-53f4-114b-b1bc-0b545e3f4f4d) - Created
[0m17:06:58.588945 [debug] [Thread-5 (]: SQL status: OK in 1.910 seconds
[0m17:06:58.588945 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec2e-53f4-114b-b1bc-0b545e3f4f4d, command-id=01f0ec2e-5412-108c-ad12-9714fbbc5419) - Closing
[0m17:06:58.588945 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m17:06:58.588945 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m17:07:00.240960 [debug] [Thread-5 (]: SQL status: OK in 1.650 seconds
[0m17:07:00.240960 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec2e-53f4-114b-b1bc-0b545e3f4f4d, command-id=01f0ec2e-54ee-1fc4-8137-36ce626d4fc8) - Closing
[0m17:07:00.240960 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m17:07:00.248235 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m17:07:00.248235 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec2e-53f4-114b-b1bc-0b545e3f4f4d) - Closing
[0m17:07:00.381075 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b05ee03e-dac6-4f15-90bd-cc483242874e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001531F3D5870>]}
[0m17:07:00.387008 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 3.72s]
[0m17:07:00.387008 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m17:07:00.400132 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:07:00.404290 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:07:00.406747 [info ] [MainThread]: 
[0m17:07:00.408832 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 12.75 seconds (12.75s).
[0m17:07:00.412206 [debug] [MainThread]: Command end result
[0m17:07:00.468515 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:07:00.473550 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:07:00.485511 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\run_results.json
[0m17:07:00.486503 [info ] [MainThread]: 
[0m17:07:00.487447 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:07:00.487447 [info ] [MainThread]: 
[0m17:07:00.487447 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m17:07:00.492372 [debug] [MainThread]: Command `dbt seed` succeeded at 17:07:00.492372 after 21.53 seconds
[0m17:07:00.492372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001531F7E5E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001531F8BED70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153217BEE30>]}
[0m17:07:00.492372 [debug] [MainThread]: Flushing usage events
[0m17:07:00.893134 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:07:08.492863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9DFD5ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9F023B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9F020040>]}


============================== 17:07:08.492863 | 60be5e63-4367-4a8e-87c8-6a19e2c71283 ==============================
[0m17:07:08.492863 [info ] [MainThread]: Running with dbt=1.10.17
[0m17:07:08.492863 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Supriya\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --full-refresh', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Supriya\\Documents\\banking_pipeline - Copy\\dbt\\logs'}
[0m17:07:10.883764 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:07:10.884449 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:07:10.884449 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:07:12.141292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9BC2B7F0>]}
[0m17:07:12.265269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB2311180>]}
[0m17:07:12.265269 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m17:07:12.841705 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:07:12.841705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB20D7B50>]}
[0m17:07:12.852041 [debug] [MainThread]: checksum: a50d730a31f4c1af850aa8110f398f1e1657a0325ec5753dae2929b817189a3e, vars: {}, profile: , target: , version: 1.10.17
[0m17:07:13.234920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:07:13.234920 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:07:13.245056 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:07:13.367949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB28B0130>]}
[0m17:07:13.607366 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:07:13.607366 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:07:13.617699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB289A170>]}
[0m17:07:13.627968 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 716 macros, 3 unit tests
[0m17:07:13.627968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB2899780>]}
[0m17:07:13.630371 [info ] [MainThread]: 
[0m17:07:13.630371 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:07:13.630371 [info ] [MainThread]: 
[0m17:07:13.630371 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:07:13.630371 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:07:13.646253 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:07:13.646253 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:07:13.668369 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:07:13.668369 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:07:13.668369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:14.302043 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5e35-1253-894f-fba6d703b2af) - Created
[0m17:07:14.618893 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m17:07:14.628948 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-5e35-1253-894f-fba6d703b2af, command-id=01f0ec2e-5e46-1b3e-b587-ce0bc681319f) - Closing
[0m17:07:14.633847 [debug] [ThreadPool]: On list_workspace: Close
[0m17:07:14.633847 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5e35-1253-894f-fba6d703b2af) - Closing
[0m17:07:14.780852 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:07:14.780852 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:07:14.780852 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:07:14.780852 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:07:14.780852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:15.198789 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5ebe-1d8d-b3b8-49a26eb30c42) - Created
[0m17:07:15.550330 [debug] [ThreadPool]: SQL status: OK in 0.770 seconds
[0m17:07:15.554711 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-5ebe-1d8d-b3b8-49a26eb30c42, command-id=01f0ec2e-5ed8-163b-ac21-5b8248dd26e2) - Closing
[0m17:07:15.555342 [debug] [ThreadPool]: On list_workspace: Close
[0m17:07:15.555342 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5ebe-1d8d-b3b8-49a26eb30c42) - Closing
[0m17:07:15.685078 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:07:15.685078 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:07:15.705345 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:07:15.705662 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:07:15.705662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:16.219295 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5f59-1b42-9fa9-ce481fabec72) - Created
[0m17:07:16.536472 [debug] [ThreadPool]: SQL status: OK in 0.830 seconds
[0m17:07:16.538802 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-5f59-1b42-9fa9-ce481fabec72, command-id=01f0ec2e-5f6b-155a-924e-621a344a3189) - Closing
[0m17:07:16.538802 [debug] [ThreadPool]: On list_workspace: Close
[0m17:07:16.538802 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5f59-1b42-9fa9-ce481fabec72) - Closing
[0m17:07:16.686126 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_workspace_staging) - Creating connection
[0m17:07:16.687174 [debug] [ThreadPool]: Acquiring new databricks connection 'create_workspace_staging'
[0m17:07:16.688172 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "staging"
"
[0m17:07:16.695172 [debug] [ThreadPool]: Using databricks connection "create_workspace_staging"
[0m17:07:16.695172 [debug] [ThreadPool]: On create_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "create_workspace_staging"} */
create schema if not exists `workspace`.`staging`
  
[0m17:07:16.695172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:17.171852 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5feb-1586-be84-d4585f4b061d) - Created
[0m17:07:17.688136 [debug] [ThreadPool]: SQL status: OK in 0.990 seconds
[0m17:07:17.688136 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-5feb-1586-be84-d4585f4b061d, command-id=01f0ec2e-5ffc-11d3-b66e-2c85cafd453e) - Closing
[0m17:07:17.688136 [debug] [ThreadPool]: On create_workspace_staging: Close
[0m17:07:17.688136 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-5feb-1586-be84-d4585f4b061d) - Closing
[0m17:07:17.818105 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_workspace_marts) - Creating connection
[0m17:07:17.818105 [debug] [ThreadPool]: Acquiring new databricks connection 'create_workspace_marts'
[0m17:07:17.828538 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "marts"
"
[0m17:07:17.838985 [debug] [ThreadPool]: Using databricks connection "create_workspace_marts"
[0m17:07:17.838985 [debug] [ThreadPool]: On create_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "create_workspace_marts"} */
create schema if not exists `workspace`.`marts`
  
[0m17:07:17.838985 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:18.282304 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6094-1681-8469-8b325e32bb1e) - Created
[0m17:07:18.735709 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m17:07:18.735709 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-6094-1681-8469-8b325e32bb1e, command-id=01f0ec2e-60a6-1317-ad9c-86370021756e) - Closing
[0m17:07:18.735709 [debug] [ThreadPool]: On create_workspace_marts: Close
[0m17:07:18.735709 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6094-1681-8469-8b325e32bb1e) - Closing
[0m17:07:18.889020 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m17:07:18.889020 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m17:07:18.909815 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m17:07:18.909815 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m17:07:18.909815 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:19.369948 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6139-167d-8e5d-547290f14233) - Created
[0m17:07:19.731207 [debug] [ThreadPool]: SQL status: OK in 0.820 seconds
[0m17:07:19.741603 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-6139-167d-8e5d-547290f14233, command-id=01f0ec2e-614c-104a-ab12-945ed47679ff) - Closing
[0m17:07:19.741603 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m17:07:19.741603 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6139-167d-8e5d-547290f14233) - Closing
[0m17:07:19.881224 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m17:07:19.883277 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m17:07:19.886452 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m17:07:19.886452 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m17:07:19.886452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:20.290544 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-61c7-1a91-a289-f2b7ac739511) - Created
[0m17:07:20.754443 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m17:07:20.754443 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-61c7-1a91-a289-f2b7ac739511, command-id=01f0ec2e-61d7-1f4d-91a2-c461ef24ef42) - Closing
[0m17:07:20.759634 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m17:07:20.759634 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-61c7-1a91-a289-f2b7ac739511) - Closing
[0m17:07:20.889933 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m17:07:20.889933 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m17:07:20.910752 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m17:07:20.916058 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m17:07:20.917311 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:21.403221 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6270-19bb-aab9-5acfc5f18125) - Created
[0m17:07:21.890918 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m17:07:21.898490 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-6270-19bb-aab9-5acfc5f18125, command-id=01f0ec2e-6283-17c5-aa73-90cf4ae6bebd) - Closing
[0m17:07:21.898490 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m17:07:21.898490 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-6270-19bb-aab9-5acfc5f18125) - Closing
[0m17:07:22.039123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9BBA7FA0>]}
[0m17:07:22.051535 [debug] [Thread-9 (]: Began running node model.banking_pipeline.stg_accounts
[0m17:07:22.059919 [info ] [Thread-9 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m17:07:22.059919 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m17:07:22.059919 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m17:07:22.059919 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m17:07:22.084346 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m17:07:22.087361 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.stg_accounts
[0m17:07:22.103055 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:07:22.103055 [warn ] [Thread-9 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:07:22.111505 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB2ACA980>]}
[0m17:07:22.127217 [debug] [Thread-9 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m17:07:22.139281 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m17:07:22.142886 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m17:07:22.142886 [debug] [Thread-9 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m17:07:22.142886 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:07:22.551076 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-6320-1184-9bec-9eff67de3526) - Created
[0m17:07:23.332613 [debug] [Thread-9 (]: SQL status: OK in 1.190 seconds
[0m17:07:23.332613 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2e-6320-1184-9bec-9eff67de3526, command-id=01f0ec2e-633f-197c-aaad-f14d7fc0c074) - Closing
[0m17:07:23.363970 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:07:23.363970 [debug] [Thread-9 (]: On model.banking_pipeline.stg_accounts: Close
[0m17:07:23.363970 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-6320-1184-9bec-9eff67de3526) - Closing
[0m17:07:23.502324 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9B0F7D90>]}
[0m17:07:23.505320 [info ] [Thread-9 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.44s]
[0m17:07:23.505320 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.stg_accounts
[0m17:07:23.505320 [debug] [Thread-9 (]: Began running node model.banking_pipeline.stg_customers
[0m17:07:23.505320 [info ] [Thread-9 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m17:07:23.508448 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m17:07:23.508448 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m17:07:23.508448 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.stg_customers
[0m17:07:23.514674 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m17:07:23.516681 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.stg_customers
[0m17:07:23.520967 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:07:23.521567 [debug] [Thread-9 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m17:07:23.523485 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m17:07:23.523990 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m17:07:23.523990 [debug] [Thread-9 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m17:07:23.523990 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:07:23.996087 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-63fc-1e28-9c3e-7931e0681298) - Created
[0m17:07:24.667951 [debug] [Thread-9 (]: SQL status: OK in 1.140 seconds
[0m17:07:24.667951 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2e-63fc-1e28-9c3e-7931e0681298, command-id=01f0ec2e-641d-1007-8878-a4fde3b2b567) - Closing
[0m17:07:24.667951 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:07:24.673906 [debug] [Thread-9 (]: On model.banking_pipeline.stg_customers: Close
[0m17:07:24.673906 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-63fc-1e28-9c3e-7931e0681298) - Closing
[0m17:07:24.778940 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB222A5F0>]}
[0m17:07:24.778940 [info ] [Thread-9 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.27s]
[0m17:07:24.788283 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.stg_customers
[0m17:07:24.788283 [debug] [Thread-9 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m17:07:24.788283 [info ] [Thread-9 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m17:07:24.788283 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m17:07:24.788283 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m17:07:24.788283 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m17:07:24.798936 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m17:07:24.798936 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m17:07:24.803847 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:07:24.808500 [debug] [Thread-9 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m17:07:24.808500 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m17:07:24.808500 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m17:07:24.808500 [debug] [Thread-9 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m17:07:24.808500 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:07:25.217515 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-64b6-1fd0-82a5-a0c2183deea1) - Created
[0m17:07:26.013300 [debug] [Thread-9 (]: SQL status: OK in 1.200 seconds
[0m17:07:26.013300 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2e-64b6-1fd0-82a5-a0c2183deea1, command-id=01f0ec2e-64c7-13d9-8599-e468852aad29) - Closing
[0m17:07:26.023162 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:07:26.027752 [debug] [Thread-9 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m17:07:26.028532 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-64b6-1fd0-82a5-a0c2183deea1) - Closing
[0m17:07:26.163399 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9B0F7D90>]}
[0m17:07:26.163742 [info ] [Thread-9 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.37s]
[0m17:07:26.163742 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m17:07:26.175301 [debug] [Thread-9 (]: Began running node model.banking_pipeline.account_interest_summary
[0m17:07:26.175301 [info ] [Thread-9 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m17:07:26.175301 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m17:07:26.175301 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m17:07:26.178474 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m17:07:26.187730 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m17:07:26.187730 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m17:07:26.213911 [debug] [Thread-9 (]: MATERIALIZING TABLE
[0m17:07:26.251413 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m17:07:26.253344 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m17:07:26.254353 [debug] [Thread-9 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m17:07:26.254353 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:07:26.685786 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-6596-19cd-b7db-1b0dfddb74ef) - Created
[0m17:07:31.162916 [debug] [Thread-9 (]: SQL status: OK in 4.910 seconds
[0m17:07:31.162916 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2e-6596-19cd-b7db-1b0dfddb74ef, command-id=01f0ec2e-65a8-11c4-b53c-8ad49866538b) - Closing
[0m17:07:31.162916 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:07:31.182578 [debug] [Thread-9 (]: On model.banking_pipeline.account_interest_summary: Close
[0m17:07:31.182578 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2e-6596-19cd-b7db-1b0dfddb74ef) - Closing
[0m17:07:31.313524 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60be5e63-4367-4a8e-87c8-6a19e2c71283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9CBF49D0>]}
[0m17:07:31.327856 [info ] [Thread-9 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 5.14s]
[0m17:07:31.331862 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m17:07:31.335590 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:07:31.336812 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:07:31.337423 [info ] [MainThread]: 
[0m17:07:31.339185 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 17.71 seconds (17.71s).
[0m17:07:31.342911 [debug] [MainThread]: Command end result
[0m17:07:31.397099 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:07:31.397099 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:07:31.412612 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\run_results.json
[0m17:07:31.412612 [info ] [MainThread]: 
[0m17:07:31.413763 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:07:31.414645 [info ] [MainThread]: 
[0m17:07:31.415652 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m17:07:31.416697 [debug] [MainThread]: Command `dbt run` succeeded at 17:07:31.416697 after 23.10 seconds
[0m17:07:31.417696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9DFD5ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB2301A20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAB2311180>]}
[0m17:07:31.418233 [debug] [MainThread]: Flushing usage events
[0m17:07:31.981883 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:09:25.915545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB02C5EA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB22E2650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB22E3670>]}


============================== 17:09:25.927425 | ae74ef86-da97-4884-930e-cdc0098e53f2 ==============================
[0m17:09:25.927425 [info ] [MainThread]: Running with dbt=1.10.17
[0m17:09:25.930110 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Supriya\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --full-refresh', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Supriya\\Documents\\banking_pipeline - Copy\\dbt\\logs'}
[0m17:09:29.067269 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:09:29.067269 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:09:29.067269 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:09:30.743241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB4A3CA30>]}
[0m17:09:30.916151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB4A3E890>]}
[0m17:09:30.916151 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m17:09:31.919292 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:09:31.921815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5575240>]}
[0m17:09:31.977415 [debug] [MainThread]: checksum: a50d730a31f4c1af850aa8110f398f1e1657a0325ec5753dae2929b817189a3e, vars: {}, profile: , target: , version: 1.10.17
[0m17:09:32.707882 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:09:32.707882 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:09:32.728432 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:09:32.911925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5B70130>]}
[0m17:09:33.414221 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:09:33.418968 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:09:33.449412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5B5A5F0>]}
[0m17:09:33.455712 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 716 macros, 3 unit tests
[0m17:09:33.455712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5B59960>]}
[0m17:09:33.459638 [info ] [MainThread]: 
[0m17:09:33.462318 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:09:33.463486 [info ] [MainThread]: 
[0m17:09:33.463486 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:09:33.465511 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:09:33.500393 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:09:33.501915 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:09:33.558747 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:09:33.558747 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:09:33.562544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:34.102161 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b187-1264-93b4-8adbf64407ac) - Created
[0m17:09:34.439040 [debug] [ThreadPool]: SQL status: OK in 0.880 seconds
[0m17:09:34.439040 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b187-1264-93b4-8adbf64407ac, command-id=01f0ec2e-b19a-170a-8e44-7adad77eaf95) - Closing
[0m17:09:34.456531 [debug] [ThreadPool]: On list_workspace: Close
[0m17:09:34.458188 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b187-1264-93b4-8adbf64407ac) - Closing
[0m17:09:34.593495 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:09:34.595190 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:09:34.612033 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:09:34.614259 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:09:34.615941 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:35.273220 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b23a-1b7b-9e78-7fe54b55bcaf) - Created
[0m17:09:35.590490 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m17:09:35.594952 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b23a-1b7b-9e78-7fe54b55bcaf, command-id=01f0ec2e-b24c-186d-97c5-ab1826d6b2a0) - Closing
[0m17:09:35.597647 [debug] [ThreadPool]: On list_workspace: Close
[0m17:09:35.599314 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b23a-1b7b-9e78-7fe54b55bcaf) - Closing
[0m17:09:35.732428 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:09:35.732428 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:09:35.744347 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:09:35.746016 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:09:35.747222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:36.214786 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b2ca-1107-8cef-e05caf3c252f) - Created
[0m17:09:36.539937 [debug] [ThreadPool]: SQL status: OK in 0.790 seconds
[0m17:09:36.543937 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b2ca-1107-8cef-e05caf3c252f, command-id=01f0ec2e-b2de-1458-a5b1-afe5674679c8) - Closing
[0m17:09:36.545070 [debug] [ThreadPool]: On list_workspace: Close
[0m17:09:36.546212 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b2ca-1107-8cef-e05caf3c252f) - Closing
[0m17:09:36.691438 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m17:09:36.693951 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m17:09:36.731939 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m17:09:36.734211 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m17:09:36.734211 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:37.236882 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b366-1f63-af54-136695067fb8) - Created
[0m17:09:37.661180 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m17:09:37.668587 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b366-1f63-af54-136695067fb8, command-id=01f0ec2e-b378-1496-863c-a3771f39a4cf) - Closing
[0m17:09:37.670813 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m17:09:37.672511 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b366-1f63-af54-136695067fb8) - Closing
[0m17:09:37.794495 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m17:09:37.796288 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m17:09:37.803990 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m17:09:37.805094 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m17:09:37.806190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:38.286580 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b406-1f9b-a614-f3c405ac2f89) - Created
[0m17:09:38.719089 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m17:09:38.719089 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b406-1f9b-a614-f3c405ac2f89, command-id=01f0ec2e-b41e-103c-99b3-99ece2874ce1) - Closing
[0m17:09:38.719089 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m17:09:38.719089 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b406-1f9b-a614-f3c405ac2f89) - Closing
[0m17:09:38.877165 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m17:09:38.877165 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m17:09:38.888041 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m17:09:38.888041 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m17:09:38.890053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:39.384476 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b4af-1b84-aeac-57f6d324539a) - Created
[0m17:09:39.816548 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m17:09:39.826750 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-b4af-1b84-aeac-57f6d324539a, command-id=01f0ec2e-b4c2-1389-bef9-c9baa3898b59) - Closing
[0m17:09:39.826750 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m17:09:39.826750 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-b4af-1b84-aeac-57f6d324539a) - Closing
[0m17:09:39.958614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FAEE6BFA0>]}
[0m17:09:39.964004 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m17:09:39.964004 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m17:09:39.967400 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m17:09:39.967400 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m17:09:39.967400 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m17:09:39.984828 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m17:09:39.992737 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m17:09:40.022986 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:09:40.026864 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:09:40.028872 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5D36E60>]}
[0m17:09:40.054213 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m17:09:40.064507 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m17:09:40.064507 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m17:09:40.064507 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m17:09:40.064507 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:09:40.523633 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b55d-1966-be21-a122d6dfecd3) - Created
[0m17:09:41.197923 [debug] [Thread-7 (]: SQL status: OK in 1.130 seconds
[0m17:09:41.202050 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-b55d-1966-be21-a122d6dfecd3, command-id=01f0ec2e-b56e-16bf-802a-25d4cd84e95a) - Closing
[0m17:09:41.229583 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:09:41.236643 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m17:09:41.237742 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b55d-1966-be21-a122d6dfecd3) - Closing
[0m17:09:41.383045 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5CB84C0>]}
[0m17:09:41.385173 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.41s]
[0m17:09:41.387185 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m17:09:41.389954 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m17:09:41.390551 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m17:09:41.393424 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m17:09:41.393424 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m17:09:41.393424 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m17:09:41.407533 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m17:09:41.409023 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m17:09:41.413986 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:09:41.413986 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m17:09:41.413986 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m17:09:41.425029 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m17:09:41.428042 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m17:09:41.429042 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:09:41.868371 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b629-1c0c-9470-d8e80efb3c27) - Created
[0m17:09:42.558974 [debug] [Thread-7 (]: SQL status: OK in 1.130 seconds
[0m17:09:42.560984 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-b629-1c0c-9470-d8e80efb3c27, command-id=01f0ec2e-b63c-19d9-a38f-867fd881b2fd) - Closing
[0m17:09:42.560984 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:09:42.562994 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m17:09:42.564502 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b629-1c0c-9470-d8e80efb3c27) - Closing
[0m17:09:42.699961 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FAFEB5870>]}
[0m17:09:42.705711 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.31s]
[0m17:09:42.705711 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m17:09:42.709511 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m17:09:42.709511 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m17:09:42.709511 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m17:09:42.709511 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m17:09:42.709511 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m17:09:42.725977 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m17:09:42.730019 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m17:09:42.739327 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:09:42.742233 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m17:09:42.742233 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m17:09:42.742233 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m17:09:42.742233 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m17:09:42.742233 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:09:43.180980 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b6f2-165f-84aa-f2e1a73d7d30) - Created
[0m17:09:43.945622 [debug] [Thread-7 (]: SQL status: OK in 1.200 seconds
[0m17:09:43.947632 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-b6f2-165f-84aa-f2e1a73d7d30, command-id=01f0ec2e-b705-1385-b747-55d2f08410bc) - Closing
[0m17:09:43.948632 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:09:43.950632 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m17:09:43.951631 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b6f2-165f-84aa-f2e1a73d7d30) - Closing
[0m17:09:44.083093 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FAFEB5870>]}
[0m17:09:44.084093 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.37s]
[0m17:09:44.086094 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m17:09:44.088100 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m17:09:44.088665 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m17:09:44.090996 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m17:09:44.092167 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m17:09:44.092167 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m17:09:44.102688 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m17:09:44.105352 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m17:09:44.148817 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m17:09:44.211472 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m17:09:44.213980 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m17:09:44.214992 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m17:09:44.215998 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:09:44.696341 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b7da-1a8c-ae66-e0c859723df5) - Created
[0m17:09:47.529968 [debug] [Thread-7 (]: SQL status: OK in 3.310 seconds
[0m17:09:47.529968 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-b7da-1a8c-ae66-e0c859723df5, command-id=01f0ec2e-b7ea-18f6-b9f8-1d2946a245cb) - Closing
[0m17:09:47.529968 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:09:47.561049 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m17:09:47.562469 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-b7da-1a8c-ae66-e0c859723df5) - Closing
[0m17:09:47.700112 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ae74ef86-da97-4884-930e-cdc0098e53f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FC5D1E830>]}
[0m17:09:47.700676 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 3.61s]
[0m17:09:47.700676 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m17:09:47.706060 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:09:47.706658 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:09:47.709836 [info ] [MainThread]: 
[0m17:09:47.711002 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 14.25 seconds (14.25s).
[0m17:09:47.714724 [debug] [MainThread]: Command end result
[0m17:09:47.807334 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:09:47.811077 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:09:47.824053 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\run_results.json
[0m17:09:47.824053 [info ] [MainThread]: 
[0m17:09:47.824053 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:09:47.824053 [info ] [MainThread]: 
[0m17:09:47.831943 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m17:09:47.831943 [debug] [MainThread]: Command `dbt run` succeeded at 17:09:47.831943 after 22.15 seconds
[0m17:09:47.831943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB02C5EA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB0387A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014FB4A3E890>]}
[0m17:09:47.831943 [debug] [MainThread]: Flushing usage events
[0m17:09:48.324179 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:10:58.946639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB48455E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB4A472650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB4A473670>]}


============================== 17:10:58.951887 | 326288c5-7369-4f47-863e-9ca28cb5a015 ==============================
[0m17:10:58.951887 [info ] [MainThread]: Running with dbt=1.10.17
[0m17:10:58.951887 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Supriya\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --full-refresh', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Supriya\\Documents\\banking_pipeline - Copy\\dbt\\logs'}
[0m17:11:01.322055 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:11:01.323061 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:11:01.323061 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:11:02.620416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB465142B0>]}
[0m17:11:02.743649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB49BF1750>]}
[0m17:11:02.743649 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m17:11:03.330268 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:11:03.330268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5D7040A0>]}
[0m17:11:03.348162 [debug] [MainThread]: checksum: a50d730a31f4c1af850aa8110f398f1e1657a0325ec5753dae2929b817189a3e, vars: {}, profile: , target: , version: 1.10.17
[0m17:11:03.730906 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:11:03.730906 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://macros\generate_schema_name.sql
[0m17:11:03.739029 [info ] [MainThread]: Unable to do partial parsing because change detected to override macro. Starting full parse.
[0m17:11:05.874010 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:05.893405 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:05.899009 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:05.903228 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.162555 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.172836 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.783904 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.783904 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.783904 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.794366 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.798457 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.803425 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.803939 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.887892 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:06.929435 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:11:07.177800 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:11:07.198899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5E2FFDC0>]}
[0m17:11:07.435690 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:11:07.438444 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:11:07.454257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5D9A61A0>]}
[0m17:11:07.454257 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 716 macros, 3 unit tests
[0m17:11:07.458512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DBA9E70>]}
[0m17:11:07.458512 [info ] [MainThread]: 
[0m17:11:07.458512 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:11:07.458512 [info ] [MainThread]: 
[0m17:11:07.458512 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:11:07.458512 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:11:07.477700 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:11:07.479777 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:11:07.495231 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:11:07.496817 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:11:07.496817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:07.966743 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-e97b-1650-822f-12b7c45dd372) - Created
[0m17:11:08.374445 [debug] [ThreadPool]: SQL status: OK in 0.880 seconds
[0m17:11:08.374445 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-e97b-1650-822f-12b7c45dd372, command-id=01f0ec2e-e99b-13b4-9433-61c8ad48fa13) - Closing
[0m17:11:08.374445 [debug] [ThreadPool]: On list_workspace: Close
[0m17:11:08.374445 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-e97b-1650-822f-12b7c45dd372) - Closing
[0m17:11:08.506001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:11:08.506001 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:11:08.524769 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:11:08.524769 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:11:08.524769 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:08.936674 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ea0f-1de9-9932-8e631d05f29d) - Created
[0m17:11:09.266725 [debug] [ThreadPool]: SQL status: OK in 0.740 seconds
[0m17:11:09.277508 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-ea0f-1de9-9932-8e631d05f29d, command-id=01f0ec2e-ea21-14c4-8f44-3fe251e113f0) - Closing
[0m17:11:09.277508 [debug] [ThreadPool]: On list_workspace: Close
[0m17:11:09.277508 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ea0f-1de9-9932-8e631d05f29d) - Closing
[0m17:11:09.504752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:11:09.504752 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:11:09.514666 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:11:09.520455 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:11:09.520455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:10.008905 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-eab5-1480-b0ef-55281a0ee08f) - Created
[0m17:11:10.328559 [debug] [ThreadPool]: SQL status: OK in 0.810 seconds
[0m17:11:10.331695 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-eab5-1480-b0ef-55281a0ee08f, command-id=01f0ec2e-eac5-1126-b6e0-bf6711606cfa) - Closing
[0m17:11:10.336673 [debug] [ThreadPool]: On list_workspace: Close
[0m17:11:10.336673 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-eab5-1480-b0ef-55281a0ee08f) - Closing
[0m17:11:10.470756 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m17:11:10.470756 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m17:11:10.484635 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m17:11:10.484635 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m17:11:10.484635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:10.894083 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-eb3b-1c06-96cd-d49d4d0064aa) - Created
[0m17:11:11.370345 [debug] [ThreadPool]: SQL status: OK in 0.880 seconds
[0m17:11:11.371005 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-eb3b-1c06-96cd-d49d4d0064aa, command-id=01f0ec2e-eb50-1ae3-972a-631eb1817591) - Closing
[0m17:11:11.371005 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m17:11:11.371005 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-eb3b-1c06-96cd-d49d4d0064aa) - Closing
[0m17:11:11.563705 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m17:11:11.565833 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m17:11:11.571900 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m17:11:11.571900 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m17:11:11.573920 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:11.971819 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ebde-1c26-97ef-84a6dab66117) - Created
[0m17:11:12.481952 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m17:11:12.481952 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-ebde-1c26-97ef-84a6dab66117, command-id=01f0ec2e-ebfb-1d84-afeb-ef22a1accf5c) - Closing
[0m17:11:12.491461 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m17:11:12.491461 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ebde-1c26-97ef-84a6dab66117) - Closing
[0m17:11:12.632778 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m17:11:12.633120 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m17:11:12.643151 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m17:11:12.643151 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m17:11:12.643151 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:13.064884 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ec85-19ef-afd3-eccefef050d7) - Created
[0m17:11:13.657207 [debug] [ThreadPool]: SQL status: OK in 1.010 seconds
[0m17:11:13.661657 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2e-ec85-19ef-afd3-eccefef050d7, command-id=01f0ec2e-ec99-11d8-ba74-5b9c33dd512d) - Closing
[0m17:11:13.661657 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m17:11:13.661657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2e-ec85-19ef-afd3-eccefef050d7) - Closing
[0m17:11:13.795595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DFD8160>]}
[0m17:11:13.804207 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m17:11:13.804207 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m17:11:13.812024 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m17:11:13.812024 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m17:11:13.812024 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m17:11:13.827209 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m17:11:13.828669 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m17:11:13.846761 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:11:13.846761 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:11:13.856043 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5E2CBBB0>]}
[0m17:11:13.866506 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m17:11:13.880282 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m17:11:13.880282 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m17:11:13.880282 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m17:11:13.880282 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:11:14.297261 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-ed42-15bc-8d71-ef4656dc3201) - Created
[0m17:11:15.133187 [debug] [Thread-7 (]: SQL status: OK in 1.250 seconds
[0m17:11:15.135310 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-ed42-15bc-8d71-ef4656dc3201, command-id=01f0ec2e-ed63-1204-96da-a93220c74430) - Closing
[0m17:11:15.147430 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:11:15.157841 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m17:11:15.157841 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-ed42-15bc-8d71-ef4656dc3201) - Closing
[0m17:11:15.325100 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB46517DF0>]}
[0m17:11:15.325100 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.52s]
[0m17:11:15.325100 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m17:11:15.325100 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m17:11:15.325100 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m17:11:15.325100 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m17:11:15.325100 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m17:11:15.340121 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m17:11:15.350419 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m17:11:15.350419 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m17:11:15.350419 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:11:15.350419 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m17:11:15.361983 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m17:11:15.363991 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m17:11:15.364994 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m17:11:15.364994 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:11:15.811614 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-ee27-1fc5-8ebe-cbbb07436ea4) - Created
[0m17:11:16.553668 [debug] [Thread-7 (]: SQL status: OK in 1.190 seconds
[0m17:11:16.555178 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-ee27-1fc5-8ebe-cbbb07436ea4, command-id=01f0ec2e-ee3b-1466-99de-5136da235890) - Closing
[0m17:11:16.555178 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:11:16.557260 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m17:11:16.557260 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-ee27-1fc5-8ebe-cbbb07436ea4) - Closing
[0m17:11:16.687773 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DE7DF30>]}
[0m17:11:16.688772 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.36s]
[0m17:11:16.691285 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m17:11:16.692286 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m17:11:16.693875 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m17:11:16.694886 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m17:11:16.695884 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m17:11:16.695884 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m17:11:16.700661 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m17:11:16.705030 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m17:11:16.711622 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m17:11:16.711622 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m17:11:16.721415 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m17:11:16.721415 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m17:11:16.721415 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m17:11:16.721415 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:11:17.132369 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-eef0-1f8b-ae15-3e6fe68704b9) - Created
[0m17:11:17.888536 [debug] [Thread-7 (]: SQL status: OK in 1.170 seconds
[0m17:11:17.891546 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-eef0-1f8b-ae15-3e6fe68704b9, command-id=01f0ec2e-ef03-161d-85c3-66aa1e5bb2dd) - Closing
[0m17:11:17.893550 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:11:17.896273 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m17:11:17.897774 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-eef0-1f8b-ae15-3e6fe68704b9) - Closing
[0m17:11:18.034858 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DB89C00>]}
[0m17:11:18.035858 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.34s]
[0m17:11:18.037862 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m17:11:18.038859 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m17:11:18.040367 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m17:11:18.042941 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m17:11:18.044953 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m17:11:18.045953 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m17:11:18.059060 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m17:11:18.061590 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m17:11:18.101895 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m17:11:18.158758 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m17:11:18.160767 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m17:11:18.162185 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m17:11:18.163872 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:11:18.648890 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-efd9-164f-8b9f-2a69b87e47ae) - Created
[0m17:11:21.462351 [debug] [Thread-7 (]: SQL status: OK in 3.300 seconds
[0m17:11:21.472456 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec2e-efd9-164f-8b9f-2a69b87e47ae, command-id=01f0ec2e-efec-19ec-aee0-0b78a52ff76c) - Closing
[0m17:11:21.482678 [debug] [Thread-7 (]: Applying tags to relation None
[0m17:11:21.513628 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m17:11:21.514862 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec2e-efd9-164f-8b9f-2a69b87e47ae) - Closing
[0m17:11:21.708982 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '326288c5-7369-4f47-863e-9ca28cb5a015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DC6BAF0>]}
[0m17:11:21.710356 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 3.67s]
[0m17:11:21.712482 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m17:11:21.714483 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:11:21.714483 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:11:21.715522 [info ] [MainThread]: 
[0m17:11:21.716518 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 14.26 seconds (14.26s).
[0m17:11:21.718518 [debug] [MainThread]: Command end result
[0m17:11:21.785164 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:11:21.788781 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:11:21.803421 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\run_results.json
[0m17:11:21.804510 [info ] [MainThread]: 
[0m17:11:21.805509 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:11:21.806576 [info ] [MainThread]: 
[0m17:11:21.806576 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m17:11:21.808556 [debug] [MainThread]: Command `dbt run` succeeded at 17:11:21.808556 after 23.04 seconds
[0m17:11:21.809548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB48455E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB48526C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB5DB73E50>]}
[0m17:11:21.809548 [debug] [MainThread]: Flushing usage events
[0m17:11:22.303649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:15:40.219559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C3B85ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C4BD3B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C4BD0040>]}


============================== 17:15:40.219559 | 7b590102-d2a3-4f6c-ac8b-5029536f7d59 ==============================
[0m17:15:40.219559 [info ] [MainThread]: Running with dbt=1.10.17
[0m17:15:40.219559 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Supriya\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --full-refresh', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Supriya\\Documents\\banking_pipeline - Copy\\dbt\\logs'}
[0m17:15:42.590526 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:15:42.591236 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:15:42.591772 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:15:44.074792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D6B9D8A0>]}
[0m17:15:44.213190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C49FC100>]}
[0m17:15:44.213190 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m17:15:44.967738 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:15:44.969745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D7E64E80>]}
[0m17:15:44.986494 [debug] [MainThread]: checksum: a50d730a31f4c1af850aa8110f398f1e1657a0325ec5753dae2929b817189a3e, vars: {}, profile: , target: , version: 1.10.17
[0m17:15:45.364515 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m17:15:45.366064 [debug] [MainThread]: Partial parsing: added file: banking_pipeline://models\intermediate\int_customer_accounts.sql
[0m17:15:45.367110 [debug] [MainThread]: Partial parsing: deleted file: banking_pipeline://models\intermidiate\int_customer_accounts.sql
[0m17:15:45.727628 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:15:45.757486 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:15:46.049369 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:15:46.049369 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:15:46.337396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D8864F10>]}
[0m17:15:46.675701 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:15:46.677766 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:15:46.691440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C3D15900>]}
[0m17:15:46.691440 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 716 macros, 3 unit tests
[0m17:15:46.691440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C0C6A8C0>]}
[0m17:15:46.698547 [info ] [MainThread]: 
[0m17:15:46.698547 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:15:46.698547 [info ] [MainThread]: 
[0m17:15:46.698547 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:15:46.698547 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:15:46.713000 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:15:46.720369 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:15:46.730082 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:15:46.738511 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:15:46.738511 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:47.334695 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-8fff-1121-b6e0-a06ac519132f) - Created
[0m17:15:47.758670 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m17:15:47.763766 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-8fff-1121-b6e0-a06ac519132f, command-id=01f0ec2f-9015-13a1-9632-8b95f570fa8d) - Closing
[0m17:15:47.763766 [debug] [ThreadPool]: On list_workspace: Close
[0m17:15:47.763766 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-8fff-1121-b6e0-a06ac519132f) - Closing
[0m17:15:47.894063 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:15:47.896108 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:15:47.907445 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:15:47.907445 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:15:47.909459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:48.312550 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9093-1dca-9b01-4ee5f215b7ab) - Created
[0m17:15:48.673581 [debug] [ThreadPool]: SQL status: OK in 0.760 seconds
[0m17:15:48.678435 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-9093-1dca-9b01-4ee5f215b7ab, command-id=01f0ec2f-90ae-1cd7-8f92-dad49bdd7b02) - Closing
[0m17:15:48.678435 [debug] [ThreadPool]: On list_workspace: Close
[0m17:15:48.678435 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9093-1dca-9b01-4ee5f215b7ab) - Closing
[0m17:15:48.815416 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m17:15:48.815416 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:15:48.825462 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:15:48.825462 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m17:15:48.825462 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:49.253633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9126-1db7-acc2-46dce6be6aa8) - Created
[0m17:15:49.557546 [debug] [ThreadPool]: SQL status: OK in 0.730 seconds
[0m17:15:49.557546 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-9126-1db7-acc2-46dce6be6aa8, command-id=01f0ec2f-9136-1c99-95f6-206b58704392) - Closing
[0m17:15:49.557546 [debug] [ThreadPool]: On list_workspace: Close
[0m17:15:49.557546 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9126-1db7-acc2-46dce6be6aa8) - Closing
[0m17:15:49.715929 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_workspace_intermediate) - Creating connection
[0m17:15:49.716553 [debug] [ThreadPool]: Acquiring new databricks connection 'create_workspace_intermediate'
[0m17:15:49.717179 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "intermediate"
"
[0m17:15:49.722472 [debug] [ThreadPool]: Using databricks connection "create_workspace_intermediate"
[0m17:15:49.722472 [debug] [ThreadPool]: On create_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "create_workspace_intermediate"} */
create schema if not exists `workspace`.`intermediate`
  
[0m17:15:49.722472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:50.143883 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-91ad-1b57-b8d4-d2c1bde1d32d) - Created
[0m17:15:50.641657 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m17:15:50.641657 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-91ad-1b57-b8d4-d2c1bde1d32d, command-id=01f0ec2f-91bf-193b-8d26-14a4c877ce61) - Closing
[0m17:15:50.641657 [debug] [ThreadPool]: On create_workspace_intermediate: Close
[0m17:15:50.641657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-91ad-1b57-b8d4-d2c1bde1d32d) - Closing
[0m17:15:50.767994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m17:15:50.772344 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m17:15:50.778488 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m17:15:50.778488 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m17:15:50.778488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:51.270627 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9256-15dc-a7a7-ef5474ece4b9) - Created
[0m17:15:51.884898 [debug] [ThreadPool]: SQL status: OK in 1.110 seconds
[0m17:15:51.884898 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-9256-15dc-a7a7-ef5474ece4b9, command-id=01f0ec2f-9276-1bf6-bb62-d63d7f58bd53) - Closing
[0m17:15:51.894260 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m17:15:51.894260 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9256-15dc-a7a7-ef5474ece4b9) - Closing
[0m17:15:52.027526 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m17:15:52.027526 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m17:15:52.034948 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m17:15:52.034948 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m17:15:52.034948 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:52.468706 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-930f-1980-ac84-f574f0f0b21f) - Created
[0m17:15:52.904681 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m17:15:52.905684 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-930f-1980-ac84-f574f0f0b21f, command-id=01f0ec2f-9321-1e48-b46d-2fce7b2a2529) - Closing
[0m17:15:52.905684 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m17:15:52.913246 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-930f-1980-ac84-f574f0f0b21f) - Closing
[0m17:15:53.039970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m17:15:53.044190 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m17:15:53.047641 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m17:15:53.047641 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m17:15:53.048817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:53.457009 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-93a7-1110-b8e8-fc5ce6392d14) - Created
[0m17:15:53.960499 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m17:15:53.960499 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-93a7-1110-b8e8-fc5ce6392d14, command-id=01f0ec2f-93b7-14b9-8534-68886ccd15d6) - Closing
[0m17:15:53.960499 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m17:15:53.960499 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-93a7-1110-b8e8-fc5ce6392d14) - Closing
[0m17:15:54.089286 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m17:15:54.089286 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m17:15:54.089286 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m17:15:54.089286 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m17:15:54.089286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:54.506557 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9446-1f16-818e-100bbd0baf5d) - Created
[0m17:15:54.941420 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m17:15:54.945953 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec2f-9446-1f16-818e-100bbd0baf5d, command-id=01f0ec2f-945e-1703-8522-ceba6a656cd7) - Closing
[0m17:15:54.947145 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m17:15:54.948321 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec2f-9446-1f16-818e-100bbd0baf5d) - Closing
[0m17:15:55.062534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D8864190>]}
[0m17:15:55.138859 [debug] [Thread-9 (]: Began running node model.banking_pipeline.stg_accounts
[0m17:15:55.141269 [info ] [Thread-9 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m17:15:55.142900 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m17:15:55.142900 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m17:15:55.142900 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m17:15:55.180765 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m17:15:55.186037 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.stg_accounts
[0m17:15:55.250033 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:15:55.256948 [warn ] [Thread-9 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m17:15:55.259199 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D8424F10>]}
[0m17:15:55.302290 [debug] [Thread-9 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m17:15:55.332374 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m17:15:55.335351 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m17:15:55.336359 [debug] [Thread-9 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m17:15:55.337360 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:15:55.798434 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-950a-19bc-8977-fd3d3ba49b1f) - Created
[0m17:15:56.481656 [debug] [Thread-9 (]: SQL status: OK in 1.140 seconds
[0m17:15:56.486131 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2f-950a-19bc-8977-fd3d3ba49b1f, command-id=01f0ec2f-951b-1a34-a12c-a744d095fbac) - Closing
[0m17:15:56.513614 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:15:56.517774 [debug] [Thread-9 (]: On model.banking_pipeline.stg_accounts: Close
[0m17:15:56.518398 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-950a-19bc-8977-fd3d3ba49b1f) - Closing
[0m17:15:56.731190 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C27A49D0>]}
[0m17:15:56.733565 [info ] [Thread-9 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.58s]
[0m17:15:56.734747 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.stg_accounts
[0m17:15:56.735934 [debug] [Thread-9 (]: Began running node model.banking_pipeline.stg_customers
[0m17:15:56.735934 [info ] [Thread-9 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m17:15:56.737982 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m17:15:56.737982 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m17:15:56.738946 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.stg_customers
[0m17:15:56.752541 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m17:15:56.754489 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.stg_customers
[0m17:15:56.760488 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:15:56.762341 [debug] [Thread-9 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m17:15:56.762942 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m17:15:56.765192 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m17:15:56.766198 [debug] [Thread-9 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m17:15:56.766198 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:15:57.248921 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-95e9-1189-bfc6-459c6213eafc) - Created
[0m17:15:57.924799 [debug] [Thread-9 (]: SQL status: OK in 1.160 seconds
[0m17:15:57.926810 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2f-95e9-1189-bfc6-459c6213eafc, command-id=01f0ec2f-95fa-1618-bb50-814be34b62d5) - Closing
[0m17:15:57.928821 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:15:57.930832 [debug] [Thread-9 (]: On model.banking_pipeline.stg_customers: Close
[0m17:15:57.932339 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-95e9-1189-bfc6-459c6213eafc) - Closing
[0m17:15:58.062199 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D8444F70>]}
[0m17:15:58.064914 [info ] [Thread-9 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.32s]
[0m17:15:58.064914 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.stg_customers
[0m17:15:58.064914 [debug] [Thread-9 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m17:15:58.064914 [info ] [Thread-9 (]: 3 of 4 START sql view model intermediate.int_customer_accounts ................. [RUN]
[0m17:15:58.073947 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m17:15:58.075508 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m17:15:58.075508 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m17:15:58.096106 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m17:15:58.101102 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m17:15:58.159331 [debug] [Thread-9 (]: MATERIALIZING VIEW
[0m17:15:58.165333 [debug] [Thread-9 (]: Creating view `workspace`.`intermediate`.`int_customer_accounts`
[0m17:15:58.168840 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m17:15:58.174816 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m17:15:58.175832 [debug] [Thread-9 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`intermediate`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m17:15:58.177831 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:15:58.605803 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-96b7-1a0a-8adc-26b437a23545) - Created
[0m17:15:59.235041 [debug] [Thread-9 (]: SQL status: OK in 1.060 seconds
[0m17:15:59.235041 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2f-96b7-1a0a-8adc-26b437a23545, command-id=01f0ec2f-96c9-1172-8a5f-5b925a2b0c18) - Closing
[0m17:15:59.235041 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:15:59.235041 [debug] [Thread-9 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m17:15:59.235041 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-96b7-1a0a-8adc-26b437a23545) - Closing
[0m17:15:59.395010 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D867C1C0>]}
[0m17:15:59.395010 [info ] [Thread-9 (]: 3 of 4 OK created sql view model intermediate.int_customer_accounts ............ [[32mOK[0m in 1.32s]
[0m17:15:59.395010 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m17:15:59.404912 [debug] [Thread-9 (]: Began running node model.banking_pipeline.account_interest_summary
[0m17:15:59.407746 [info ] [Thread-9 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m17:15:59.410406 [debug] [Thread-9 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m17:15:59.411099 [debug] [Thread-9 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m17:15:59.411099 [debug] [Thread-9 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m17:15:59.426530 [debug] [Thread-9 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m17:15:59.426530 [debug] [Thread-9 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m17:15:59.469082 [debug] [Thread-9 (]: MATERIALIZING TABLE
[0m17:15:59.556448 [debug] [Thread-9 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m17:15:59.559355 [debug] [Thread-9 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m17:15:59.560519 [debug] [Thread-9 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.17", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`intermediate`.`int_customer_accounts`
  
[0m17:15:59.561613 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:16:00.068072 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-9797-1ca7-835e-c7783d3482dd) - Created
[0m17:16:02.972718 [debug] [Thread-9 (]: SQL status: OK in 3.410 seconds
[0m17:16:02.972718 [debug] [Thread-9 (]: Databricks adapter: Cursor(session-id=01f0ec2f-9797-1ca7-835e-c7783d3482dd, command-id=01f0ec2f-97b0-1c6a-820e-821832ef02de) - Closing
[0m17:16:02.972718 [debug] [Thread-9 (]: Applying tags to relation None
[0m17:16:02.990356 [debug] [Thread-9 (]: On model.banking_pipeline.account_interest_summary: Close
[0m17:16:02.990356 [debug] [Thread-9 (]: Databricks adapter: Connection(session-id=01f0ec2f-9797-1ca7-835e-c7783d3482dd) - Closing
[0m17:16:03.177246 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b590102-d2a3-4f6c-ac8b-5029536f7d59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229D8636560>]}
[0m17:16:03.178285 [info ] [Thread-9 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 3.77s]
[0m17:16:03.179250 [debug] [Thread-9 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m17:16:03.181284 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m17:16:03.181284 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:16:03.181284 [info ] [MainThread]: 
[0m17:16:03.182836 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 16.48 seconds (16.48s).
[0m17:16:03.184882 [debug] [MainThread]: Command end result
[0m17:16:03.240230 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\manifest.json
[0m17:16:03.245118 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\semantic_manifest.json
[0m17:16:03.258834 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Supriya\Documents\banking_pipeline - Copy\dbt\target\run_results.json
[0m17:16:03.258834 [info ] [MainThread]: 
[0m17:16:03.259880 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:16:03.260833 [info ] [MainThread]: 
[0m17:16:03.261836 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m17:16:03.262882 [debug] [MainThread]: Command `dbt run` succeeded at 17:16:03.262882 after 23.23 seconds
[0m17:16:03.263832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C3B85ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C3D15900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C17DBC10>]}
[0m17:16:03.263832 [debug] [MainThread]: Flushing usage events
[0m17:16:03.818643 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:18:53.172791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae660a6a410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65f8ac2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65f8ac1c0>]}
[0m06:18:53.175824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e853d5e4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e852bac280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e852bac220>]}


============================== 06:18:53.197510 | ad9db48e-a534-44ef-8e81-b5a9de237842 ==============================
[0m06:18:53.197510 [info ] [MainThread]: Running with dbt=1.10.18


============================== 06:18:53.197707 | 32ea0e1d-ad6c-42c1-9d0b-c81927f9366a ==============================
[0m06:18:53.197707 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:18:53.200158 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'log_format': 'default', 'printer_width': '80', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'static_parser': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'cache_selected_only': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'partial_parse': 'True', 'empty': 'None', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None'}
[0m06:18:53.200776 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'printer_width': '80', 'indirect_selection': 'eager', 'log_format': 'default', 'write_json': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'profiles_dir': '/opt/dagster/app/dbt', 'target_path': 'None', 'static_parser': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'no_print': 'None', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False'}
[0m06:18:53.312248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b366de230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b354f8280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b354f8220>]}


============================== 06:18:53.319664 | 63578bf5-6f95-43fe-aeee-c3daee9ddd27 ==============================
[0m06:18:53.319664 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:18:53.323317 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'static_parser': 'True', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'warn_error': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'quiet': 'False', 'fail_fast': 'False', 'introspect': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'target_path': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'write_json': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/opt/dagster/app/dbt/logs', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'printer_width': '80'}
[0m06:18:55.630930 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:18:55.634530 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:18:55.633879 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:18:55.637485 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:18:55.638378 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:18:55.638405 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:18:55.641502 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:18:55.642828 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:18:55.643989 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:18:59.510473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e855891030>]}
[0m06:18:59.510600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b35862740>]}
[0m06:18:59.510659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65fad1420>]}
[0m06:18:59.767835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65d703160>]}
[0m06:18:59.770576 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:18:59.778384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e852d2d510>]}
[0m06:18:59.780785 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:18:59.784687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b35d6dbd0>]}
[0m06:18:59.788206 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:19:00.164306 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:19:00.165034 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:19:00.166300 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:19:00.579379 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m06:19:00.581871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6600e60e0>]}
[0m06:19:00.606100 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m06:19:00.608992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e8533e5960>]}
[0m06:19:00.612733 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m06:19:00.615375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b35ed3a90>]}
[0m06:19:05.357054 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.388956 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.398913 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.400024 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.406030 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.452879 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.459454 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.475134 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.506455 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.569302 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.595010 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.603486 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.971676 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:05.979381 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:06.082843 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:06.091278 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:06.104138 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:06.115155 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.035637 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.041883 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.048120 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.054067 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.061285 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.067627 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.079838 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.215377 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.222019 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.231326 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.237958 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.244796 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.250962 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.256843 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.321529 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.328149 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.333644 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.340680 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.346325 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.352135 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.358686 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.360920 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.443588 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.465541 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.470632 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.537716 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.550254 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m06:19:07.824848 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:19:07.890502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e2dcd00>]}
[0m06:19:07.986510 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:19:08.000789 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:19:08.026491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6385a4d60>]}
[0m06:19:08.048770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e81f9ed4b0>]}
[0m06:19:08.318813 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:19:08.331258 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:19:08.380342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e1f2860>]}
[0m06:19:08.382971 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:19:08.385135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e1f1480>]}
[0m06:19:08.390877 [info ] [MainThread]: 
[0m06:19:08.393248 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:19:08.395255 [info ] [MainThread]: 
[0m06:19:08.398015 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:19:08.399353 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:19:08.400089 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:19:08.406415 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:19:08.410148 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:19:08.419264 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:19:08.421821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:19:08.424598 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:19:08.444863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6385badd0>]}
[0m06:19:08.447294 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:19:08.447160 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:19:08.449373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6385a60e0>]}
[0m06:19:08.449443 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:19:08.451542 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:08.454154 [info ] [MainThread]: 
[0m06:19:08.456624 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:19:08.459227 [info ] [MainThread]: 
[0m06:19:08.461894 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:19:08.464152 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:19:08.469325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e81f9ea080>]}
[0m06:19:08.472393 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:19:08.475275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e81f9ef0a0>]}
[0m06:19:08.481104 [info ] [MainThread]: 
[0m06:19:08.483358 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:19:08.484356 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:19:08.485484 [info ] [MainThread]: 
[0m06:19:08.486532 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:19:08.488233 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:19:08.491182 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:19:08.508642 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:19:08.510088 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:19:08.511069 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:19:08.514222 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:19:08.514853 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:08.546078 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:19:08.550897 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:19:08.552863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:09.550151 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14d-118e-905f-5bd5b8b35fa7) - Created
[0m06:19:09.552086 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14c-1110-b4b4-c72da17d36c1) - Created
[0m06:19:09.552352 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14e-1f83-87de-0c8d2998d92e) - Created
[0m06:19:23.957806 [debug] [ThreadPool]: SQL status: OK in 15.440 seconds
[0m06:19:23.958402 [debug] [ThreadPool]: SQL status: OK in 15.510 seconds
[0m06:19:24.001014 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-f14d-118e-905f-5bd5b8b35fa7, command-id=01f0ec59-f182-16c4-89ff-1d45ac0fa83c) - Closing
[0m06:19:24.001007 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-f14c-1110-b4b4-c72da17d36c1, command-id=01f0ec59-f183-1952-a22b-a34de6c70b25) - Closing
[0m06:19:24.403214 [debug] [ThreadPool]: On list_workspace: Close
[0m06:19:24.403238 [debug] [ThreadPool]: On list_workspace: Close
[0m06:19:24.406608 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14c-1110-b4b4-c72da17d36c1) - Closing
[0m06:19:24.406603 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14d-118e-905f-5bd5b8b35fa7) - Closing
[0m06:19:24.645224 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:19:24.655133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:19:24.673916 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:19:24.676061 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:19:24.740527 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:19:24.746437 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:19:24.749283 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:24.760713 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:19:24.764261 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:19:24.767243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:25.874455 [debug] [ThreadPool]: SQL status: OK in 17.320 seconds
[0m06:19:25.904779 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-f14e-1f83-87de-0c8d2998d92e, command-id=01f0ec59-f180-1a94-917d-bd7d4dfdadaf) - Closing
[0m06:19:26.067642 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fb42-115e-8b78-ddd931fd6f5a) - Created
[0m06:19:26.085713 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fb41-1101-939e-e92dd91eb30d) - Created
[0m06:19:26.148269 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:19:26.155572 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-f14e-1f83-87de-0c8d2998d92e) - Closing
[0m06:19:26.299793 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:19:26.308311 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:19:26.332240 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:19:26.338856 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:19:26.346772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:27.002753 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fbd1-1a3c-876c-4ab6999d425f) - Created
[0m06:19:27.476027 [debug] [ThreadPool]: SQL status: OK in 2.730 seconds
[0m06:19:27.492777 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fb41-1101-939e-e92dd91eb30d, command-id=01f0ec59-fb59-1709-85b6-d3a8bf3563f6) - Closing
[0m06:19:27.503084 [debug] [ThreadPool]: On list_workspace: Close
[0m06:19:27.509790 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fb41-1101-939e-e92dd91eb30d) - Closing
[0m06:19:27.662194 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:19:27.672636 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:19:27.715449 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:19:27.722452 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:19:27.728808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:28.314753 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fc99-1638-a283-8705a49e30ef) - Created
[0m06:19:28.864310 [debug] [ThreadPool]: SQL status: OK in 4.100 seconds
[0m06:19:28.891888 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fb42-115e-8b78-ddd931fd6f5a, command-id=01f0ec59-fb58-16d2-9f5d-6da8ba687c50) - Closing
[0m06:19:28.906758 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:19:28.914501 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fb42-115e-8b78-ddd931fd6f5a) - Closing
[0m06:19:29.046960 [debug] [ThreadPool]: SQL status: OK in 2.700 seconds
[0m06:19:29.069799 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fbd1-1a3c-876c-4ab6999d425f, command-id=01f0ec59-fbe7-15e5-9b89-00336ae350cd) - Closing
[0m06:19:29.070228 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:19:29.073721 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:19:29.075103 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:19:29.084860 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fbd1-1a3c-876c-4ab6999d425f) - Closing
[0m06:19:29.092293 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:19:29.098848 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:19:29.102478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:29.220214 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:19:29.223181 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:19:29.232412 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:19:29.234389 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:19:29.236642 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:29.593498 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fd56-1ef3-b126-11115d95f3ad) - Created
[0m06:19:29.710889 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fd6e-1716-9dfe-cec47f989b44) - Created
[0m06:19:29.935927 [debug] [ThreadPool]: SQL status: OK in 2.210 seconds
[0m06:19:29.952704 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fc99-1638-a283-8705a49e30ef, command-id=01f0ec59-fcad-10c1-9f6e-b54bdc6d88c1) - Closing
[0m06:19:29.960867 [debug] [ThreadPool]: On list_workspace: Close
[0m06:19:29.968937 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fc99-1638-a283-8705a49e30ef) - Closing
[0m06:19:30.136346 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:19:30.142869 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:19:30.201769 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:19:30.208559 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:19:30.216454 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:30.805734 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fe15-1863-acbc-4bfd8f7a5f2a) - Created
[0m06:19:31.081885 [debug] [ThreadPool]: SQL status: OK in 1.840 seconds
[0m06:19:31.085009 [debug] [ThreadPool]: SQL status: OK in 1.980 seconds
[0m06:19:31.104503 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fd6e-1716-9dfe-cec47f989b44, command-id=01f0ec59-fd81-1fab-b2dc-edc115c8a19c) - Closing
[0m06:19:31.107880 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fd56-1ef3-b126-11115d95f3ad, command-id=01f0ec59-fd71-1d1b-aa93-3ba0263c04f6) - Closing
[0m06:19:31.113412 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:19:31.118398 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:19:31.121760 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fd6e-1716-9dfe-cec47f989b44) - Closing
[0m06:19:31.125227 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fd56-1ef3-b126-11115d95f3ad) - Closing
[0m06:19:31.290705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad9db48e-a534-44ef-8e81-b5a9de237842', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e850b51ed0>]}
[0m06:19:31.322276 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:19:31.338266 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:19:31.364463 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:19:31.374400 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:19:31.375808 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m06:19:31.379622 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:19:31.380977 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m06:19:31.383113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:31.384219 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m06:19:31.387957 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:19:31.481885 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:19:31.540136 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:19:31.590850 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:19:31.630471 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:19:31.632987 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m06:19:31.635316 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:31.953044 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-febc-1ece-9dbe-a4e13858633e) - Created
[0m06:19:32.412149 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec59-fef9-136c-8f5a-6b1bf548dd7f) - Created
[0m06:19:32.447741 [debug] [ThreadPool]: SQL status: OK in 2.230 seconds
[0m06:19:32.476172 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-fe15-1863-acbc-4bfd8f7a5f2a, command-id=01f0ec59-fe29-17e5-949e-98ed7e845600) - Closing
[0m06:19:32.487128 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:19:32.495136 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-fe15-1863-acbc-4bfd8f7a5f2a) - Closing
[0m06:19:32.691383 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:19:32.699053 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:19:32.715435 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:19:32.719547 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:19:32.724430 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:33.255758 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-ff8c-122a-975d-e874a85776ea) - Created
[0m06:19:33.291867 [debug] [ThreadPool]: SQL status: OK in 1.910 seconds
[0m06:19:33.307105 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-febc-1ece-9dbe-a4e13858633e, command-id=01f0ec59-fed9-10c2-a2d8-a7409a725461) - Closing
[0m06:19:33.315626 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:19:33.322868 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-febc-1ece-9dbe-a4e13858633e) - Closing
[0m06:19:33.490214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b38031ba0>]}
[0m06:19:33.524564 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m06:19:33.546618 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m06:19:33.572318 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m06:19:33.596065 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m06:19:33.610173 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m06:19:33.625393 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m06:19:33.674049 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:19:33.682839 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e0d2020>]}
[0m06:19:33.921624 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:19:33.924629 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e0bb070>]}
[0m06:19:34.001085 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:19:34.003145 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:19:34.004855 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:19:34.949706 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5a-0061-11d7-b1fd-937d58798a3b) - Created
[0m06:19:34.970316 [debug] [ThreadPool]: SQL status: OK in 2.250 seconds
[0m06:19:34.991337 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec59-ff8c-122a-975d-e874a85776ea, command-id=01f0ec59-ff9f-11cd-85ec-71d67af1de03) - Closing
[0m06:19:35.004742 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:19:35.011953 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec59-ff8c-122a-975d-e874a85776ea) - Closing
[0m06:19:35.166142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:19:35.176727 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:19:35.200320 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:19:35.207374 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:19:35.214684 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:35.811494 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5a-010d-19de-a86e-bcbb349828c2) - Created
[0m06:19:37.113018 [debug] [ThreadPool]: SQL status: OK in 1.900 seconds
[0m06:19:37.131592 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5a-010d-19de-a86e-bcbb349828c2, command-id=01f0ec5a-0127-1110-894c-973c0da092bb) - Closing
[0m06:19:37.140638 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:19:37.148244 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5a-010d-19de-a86e-bcbb349828c2) - Closing
[0m06:19:37.300776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae638644340>]}
[0m06:19:37.328883 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m06:19:37.336581 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m06:19:37.345754 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m06:19:37.352364 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m06:19:37.359819 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m06:19:37.485117 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m06:19:37.550705 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m06:19:37.668681 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:19:37.685810 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:19:37.693905 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae638383a90>]}
[0m06:19:37.826657 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m06:19:37.912433 [debug] [Thread-4 (]: SQL status: OK in 6.280 seconds
[0m06:19:37.915707 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m06:19:37.926367 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec59-fef9-136c-8f5a-6b1bf548dd7f, command-id=01f0ec59-ff1f-11bb-8f2e-3dc6b66320fa) - Closing
[0m06:19:37.938630 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m06:19:37.941510 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m06:19:37.942360 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec59-fef9-136c-8f5a-6b1bf548dd7f) - Closing
[0m06:19:37.944368 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m06:19:37.947177 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:19:38.079654 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 6.70s]
[0m06:19:38.083861 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:19:38.087531 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:19:38.092306 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m06:19:38.096686 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m06:19:38.099755 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m06:19:38.103066 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:19:38.118473 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:19:38.152042 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:19:38.164520 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:19:38.195570 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:19:38.198966 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m06:19:38.202040 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:38.870229 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-02e2-18ea-88be-d10f1f89fe0f) - Created
[0m06:19:39.125416 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0306-12ba-a5f2-408b25999ee1) - Created
[0m06:19:39.435356 [debug] [Thread-5 (]: SQL status: OK in 5.430 seconds
[0m06:19:39.441755 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0061-11d7-b1fd-937d58798a3b, command-id=01f0ec5a-00a2-11b1-90a3-c611d09232e2) - Closing
[0m06:19:39.499540 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:19:39.502964 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m06:19:40.483221 [debug] [Thread-4 (]: SQL status: OK in 2.280 seconds
[0m06:19:40.499850 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0306-12ba-a5f2-408b25999ee1, command-id=01f0ec5a-031f-19f1-b26f-c9fef7c0ead6) - Closing
[0m06:19:40.513302 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m06:19:40.524323 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0306-12ba-a5f2-408b25999ee1) - Closing
[0m06:19:40.553421 [debug] [Thread-7 (]: SQL status: OK in 2.610 seconds
[0m06:19:40.564003 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5a-02e2-18ea-88be-d10f1f89fe0f, command-id=01f0ec5a-02f8-1d86-b5a5-bff8126bea01) - Closing
[0m06:19:40.650632 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:19:40.681382 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m06:19:40.681887 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 2.58s]
[0m06:19:40.693710 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-02e2-18ea-88be-d10f1f89fe0f) - Closing
[0m06:19:40.695185 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:19:40.703845 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:19:40.720162 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m06:19:40.731657 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m06:19:40.741401 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m06:19:40.749887 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:19:40.803620 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:19:40.832932 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:19:40.853907 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6627ad870>]}
[0m06:19:40.854586 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:19:40.858711 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 3.50s]
[0m06:19:40.862764 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m06:19:40.865982 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m06:19:40.869296 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m06:19:40.873610 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m06:19:40.876776 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m06:19:40.879043 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:19:40.879583 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m06:19:40.882426 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m06:19:40.885489 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:40.895685 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m06:19:40.920268 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m06:19:40.935868 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:19:40.942969 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m06:19:40.947118 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m06:19:40.985231 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m06:19:40.987891 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m06:19:40.990870 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:19:41.438740 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-046b-1bde-be7d-2927e5a59769) - Created
[0m06:19:41.496451 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0475-19b2-b99f-24ddac7e9784) - Created
[0m06:19:42.754489 [debug] [Thread-4 (]: SQL status: OK in 1.870 seconds
[0m06:19:42.770875 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-046b-1bde-be7d-2927e5a59769, command-id=01f0ec5a-0481-1f0e-bd62-5b5b2259e77e) - Closing
[0m06:19:42.780165 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m06:19:42.786439 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-046b-1bde-be7d-2927e5a59769) - Closing
[0m06:19:42.951310 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 2.22s]
[0m06:19:42.958268 [debug] [Thread-7 (]: SQL status: OK in 1.970 seconds
[0m06:19:42.961049 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:19:42.969766 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0475-19b2-b99f-24ddac7e9784, command-id=01f0ec5a-0489-19cd-ac64-a54ee57a1889) - Closing
[0m06:19:42.969682 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:19:42.978095 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:19:42.977617 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m06:19:42.987810 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m06:19:42.990015 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m06:19:42.999124 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m06:19:43.000550 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0475-19b2-b99f-24ddac7e9784) - Closing
[0m06:19:43.008147 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:19:43.125140 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:19:43.157369 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:19:43.174530 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae638377310>]}
[0m06:19:43.175948 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:19:43.179250 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 2.30s]
[0m06:19:43.183914 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m06:19:43.188601 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m06:19:43.192482 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m06:19:43.196614 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m06:19:43.198250 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:19:43.199970 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m06:19:43.201252 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m06:19:43.202771 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m06:19:43.203714 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:43.226150 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m06:19:43.246742 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m06:19:43.260444 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:19:43.265321 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m06:19:43.269243 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m06:19:43.295988 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m06:19:43.300184 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m06:19:43.310528 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:19:43.688646 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-05c5-117e-b56e-fd4933506c1b) - Created
[0m06:19:44.826480 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0670-15f0-a9c4-946490e3323f) - Created
[0m06:19:44.844934 [debug] [Thread-4 (]: SQL status: OK in 1.640 seconds
[0m06:19:44.850638 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-05c5-117e-b56e-fd4933506c1b, command-id=01f0ec5a-05d8-138a-b968-6dd78e2115a0) - Closing
[0m06:19:44.855624 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m06:19:44.859624 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-05c5-117e-b56e-fd4933506c1b) - Closing
[0m06:19:45.001800 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 2.01s]
[0m06:19:45.010541 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:19:45.017871 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:19:45.026009 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m06:19:45.034764 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m06:19:45.041225 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m06:19:45.048818 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:19:45.096930 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:19:45.172893 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:19:45.221099 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:19:45.284592 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:19:45.291555 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m06:19:45.299161 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:45.853157 [debug] [Thread-5 (]: SQL status: OK in 6.350 seconds
[0m06:19:45.861798 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0061-11d7-b1fd-937d58798a3b, command-id=01f0ec5a-0358-1b22-9340-9553c727ede9) - Closing
[0m06:19:45.868602 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0710-1285-a5b5-80e0a82e070f) - Created
[0m06:19:46.069176 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m06:19:46.287272 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m06:19:46.294519 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5a-0061-11d7-b1fd-937d58798a3b) - Closing
[0m06:19:46.453682 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b3843d990>]}
[0m06:19:46.462356 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 12.88s]
[0m06:19:46.471595 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m06:19:46.479721 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m06:19:46.487435 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m06:19:46.495740 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m06:19:46.502632 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m06:19:46.510128 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m06:19:46.517870 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m06:19:46.588990 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:19:46.593181 [debug] [Thread-7 (]: SQL status: OK in 3.280 seconds
[0m06:19:46.597876 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:19:46.603724 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0670-15f0-a9c4-946490e3323f, command-id=01f0ec5a-0687-1678-a6a0-ef6e8abd2800) - Closing
[0m06:19:46.604187 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:19:46.612943 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:19:46.624424 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m06:19:46.633371 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0670-15f0-a9c4-946490e3323f) - Closing
[0m06:19:46.799723 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae638350fd0>]}
[0m06:19:46.813633 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 3.60s]
[0m06:19:46.818291 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m06:19:46.824227 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m06:19:46.827724 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m06:19:46.831806 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m06:19:46.834923 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m06:19:46.837806 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m06:19:46.849068 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m06:19:46.876034 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m06:19:46.931610 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m06:19:46.964607 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:19:46.968270 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae638473760>]}
[0m06:19:47.040529 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m06:19:47.066341 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m06:19:47.069527 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m06:19:47.072514 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:19:47.200405 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5a-07d4-10bc-8837-369c08b31523) - Created
[0m06:19:47.596177 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0818-16b8-b969-d2d50f9f9157) - Created
[0m06:19:47.617415 [debug] [Thread-4 (]: SQL status: OK in 2.320 seconds
[0m06:19:47.633853 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0710-1285-a5b5-80e0a82e070f, command-id=01f0ec5a-0724-1837-a551-829da55b49eb) - Closing
[0m06:19:47.644021 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m06:19:47.651569 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0710-1285-a5b5-80e0a82e070f) - Closing
[0m06:19:47.803034 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.77s]
[0m06:19:47.820404 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:19:47.833280 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:19:47.841901 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m06:19:47.854072 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m06:19:47.867558 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m06:19:47.879026 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:19:47.915315 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:19:47.963539 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:19:47.976990 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:19:48.019540 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:19:48.022320 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m06:19:48.025614 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:48.509379 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-08a2-18eb-be14-8ad99289dba6) - Created
[0m06:19:49.485692 [debug] [Thread-4 (]: SQL status: OK in 1.460 seconds
[0m06:19:49.496572 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-08a2-18eb-be14-8ad99289dba6, command-id=01f0ec5a-08b6-1900-8ea2-9c3faff9dfe4) - Closing
[0m06:19:49.502022 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m06:19:49.507808 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-08a2-18eb-be14-8ad99289dba6) - Closing
[0m06:19:49.618536 [debug] [Thread-5 (]: SQL status: OK in 3.010 seconds
[0m06:19:49.629737 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5a-07d4-10bc-8837-369c08b31523, command-id=01f0ec5a-07f1-1967-a53d-d941c2d5ae44) - Closing
[0m06:19:49.656060 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:19:49.657474 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.80s]
[0m06:19:49.665943 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m06:19:49.669228 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:19:49.678321 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:19:49.687755 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m06:19:49.697560 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m06:19:49.703812 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m06:19:49.711659 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:19:49.758397 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:19:49.832145 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:19:49.855473 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:19:49.931781 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:19:49.939281 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m06:19:49.947528 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:50.517108 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-09d6-1534-b55a-056939bffd03) - Created
[0m06:19:51.746156 [debug] [Thread-4 (]: SQL status: OK in 1.800 seconds
[0m06:19:51.761219 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-09d6-1534-b55a-056939bffd03, command-id=01f0ec5a-09f1-1272-bd1f-1bb82e72448d) - Closing
[0m06:19:51.769211 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m06:19:51.775627 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-09d6-1534-b55a-056939bffd03) - Closing
[0m06:19:51.946648 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 2.25s]
[0m06:19:51.955053 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:19:51.962514 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:19:51.969497 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m06:19:51.979149 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m06:19:51.986905 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m06:19:51.995550 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:19:52.064284 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:19:52.114152 [debug] [Thread-7 (]: SQL status: OK in 5.040 seconds
[0m06:19:52.125770 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0818-16b8-b969-d2d50f9f9157, command-id=01f0ec5a-082d-1f87-bb09-f7c1ee40d027) - Closing
[0m06:19:52.129002 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:19:52.153432 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:19:52.154385 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:19:52.216649 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:19:52.225346 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:19:52.232682 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:52.297571 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m06:19:52.304706 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5a-0818-16b8-b969-d2d50f9f9157) - Closing
[0m06:19:52.446073 [debug] [Thread-5 (]: SQL status: OK in 2.770 seconds
[0m06:19:52.450409 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5a-07d4-10bc-8837-369c08b31523, command-id=01f0ec5a-0967-176d-9313-722384a9158b) - Closing
[0m06:19:52.457345 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m06:19:52.465737 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ea0e1d-ad6c-42c1-9d0b-c81927f9366a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae6383a8d60>]}
[0m06:19:52.474698 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 5.63s]
[0m06:19:52.481512 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m06:19:52.488967 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:19:52.495305 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:19:52.497481 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m06:19:52.500057 [info ] [MainThread]: 
[0m06:19:52.500999 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5a-07d4-10bc-8837-369c08b31523) - Closing
[0m06:19:52.505116 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 44.04 seconds (44.04s).
[0m06:19:52.512389 [debug] [MainThread]: Command end result
[0m06:19:52.640078 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63578bf5-6f95-43fe-aeee-c3daee9ddd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0f5bfc10>]}
[0m06:19:52.646294 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 6.14s]
[0m06:19:52.649274 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m06:19:52.655185 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:19:52.657768 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:19:52.662674 [info ] [MainThread]: 
[0m06:19:52.665383 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 44.26 seconds (44.26s).
[0m06:19:52.668706 [debug] [MainThread]: Command end result
[0m06:19:52.692545 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:19:52.705319 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:19:52.728185 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:19:52.730567 [info ] [MainThread]: 
[0m06:19:52.732899 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:19:52.735264 [info ] [MainThread]: 
[0m06:19:52.737533 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m06:19:52.744158 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 59.878525, "process_in_blocks": "69224", "process_kernel_time": 1.738281, "process_mem_max_rss": "258132", "process_out_blocks": "544", "process_user_time": 20.393696}
[0m06:19:52.746516 [debug] [MainThread]: Command `dbt run` succeeded at 06:19:52.746290 after 59.88 seconds
[0m06:19:52.749037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae660a6a410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65fad0d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ae65d702560>]}
[0m06:19:52.751407 [debug] [MainThread]: Flushing usage events
[0m06:19:52.795467 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0b31-1e4f-a028-d4377535fdc4) - Created
[0m06:19:52.857823 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:19:52.892298 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:19:52.970710 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:19:52.977794 [info ] [MainThread]: 
[0m06:19:52.985972 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:19:52.994382 [info ] [MainThread]: 
[0m06:19:53.002523 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m06:19:53.013961 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 60.018032, "process_in_blocks": "59256", "process_kernel_time": 1.626955, "process_mem_max_rss": "254564", "process_out_blocks": "336", "process_user_time": 19.041405}
[0m06:19:53.021019 [debug] [MainThread]: Command `dbt seed` succeeded at 06:19:53.020426 after 60.03 seconds
[0m06:19:53.028083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b366de230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b0e24ca90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752b35862740>]}
[0m06:19:53.035017 [debug] [MainThread]: Flushing usage events
[0m06:19:53.348660 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:19:53.503689 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:19:53.898315 [debug] [Thread-4 (]: SQL status: OK in 1.670 seconds
[0m06:19:53.917669 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0b31-1e4f-a028-d4377535fdc4, command-id=01f0ec5a-0b45-1781-b1bb-c484525a57af) - Closing
[0m06:19:53.925626 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m06:19:53.931992 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0b31-1e4f-a028-d4377535fdc4) - Closing
[0m06:19:54.076079 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 2.10s]
[0m06:19:54.084234 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:19:54.090388 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:19:54.098333 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m06:19:54.105800 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m06:19:54.112853 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m06:19:54.116207 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:19:54.150351 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:19:54.185633 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:19:54.210386 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:19:54.239308 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:19:54.243683 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:19:54.247888 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:54.788432 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0c61-1b18-86b8-f96098049755) - Created
[0m06:19:56.147464 [debug] [Thread-4 (]: SQL status: OK in 1.900 seconds
[0m06:19:56.157301 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0c61-1b18-86b8-f96098049755, command-id=01f0ec5a-0c75-12a2-a058-f88267daeaee) - Closing
[0m06:19:56.164333 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m06:19:56.168366 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0c61-1b18-86b8-f96098049755) - Closing
[0m06:19:56.315816 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.21s]
[0m06:19:56.321325 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:19:56.326197 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:19:56.331870 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m06:19:56.337634 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m06:19:56.342171 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m06:19:56.347512 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:19:56.352615 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:19:56.573374 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:56.577166 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:19:56.580286 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:19:57.089063 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8) - Created
[0m06:19:57.604132 [debug] [Thread-4 (]: SQL status: OK in 1.020 seconds
[0m06:19:57.620013 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8, command-id=01f0ec5a-0dd7-1219-99cc-3b42b99f529a) - Closing
[0m06:19:57.775374 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:57.858079 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:58.034633 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:58.040003 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:19:58.705871 [debug] [Thread-4 (]: SQL status: OK in 0.660 seconds
[0m06:19:58.718744 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8, command-id=01f0ec5a-0e65-1da5-97fb-07a324352e4f) - Closing
[0m06:19:58.741389 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:58.750261 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m06:19:59.147179 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m06:19:59.164067 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8, command-id=01f0ec5a-0ed1-1513-b567-f96bb1c4f2c5) - Closing
[0m06:19:59.218182 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:59.319026 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:59.321232 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:19:59.854944 [debug] [Thread-4 (]: SQL status: OK in 0.530 seconds
[0m06:19:59.880842 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8, command-id=01f0ec5a-0f29-17a3-9531-4335f743e96a) - Closing
[0m06:19:59.931779 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:19:59.965155 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:19:59.972110 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:20:00.454586 [debug] [Thread-4 (]: SQL status: OK in 0.480 seconds
[0m06:20:00.467576 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8, command-id=01f0ec5a-0f8c-12dd-9cd7-4642656cf81f) - Closing
[0m06:20:00.510789 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m06:20:00.520035 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-0dc0-1364-a8e6-e55d129391c8) - Closing
[0m06:20:00.689174 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 4.35s]
[0m06:20:00.699807 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:20:00.707383 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:20:00.714319 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m06:20:00.723136 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m06:20:00.731465 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m06:20:00.737997 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:20:00.745533 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:20:00.845363 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:00.854237 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:20:00.861595 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:20:01.465240 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-105b-18d6-954f-d6889b206567) - Created
[0m06:20:01.998338 [debug] [Thread-4 (]: SQL status: OK in 1.140 seconds
[0m06:20:02.016004 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-105b-18d6-954f-d6889b206567, command-id=01f0ec5a-1070-1fcb-a013-7442fbdccf39) - Closing
[0m06:20:02.053440 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:02.112628 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:02.161223 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:02.169273 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:20:02.555097 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m06:20:02.567137 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-105b-18d6-954f-d6889b206567, command-id=01f0ec5a-10df-11bf-8ebc-79f117cbdba2) - Closing
[0m06:20:02.588207 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:02.596939 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m06:20:02.914588 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m06:20:02.928482 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-105b-18d6-954f-d6889b206567, command-id=01f0ec5a-111c-1f6f-88e0-bcae7a094927) - Closing
[0m06:20:02.948051 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:03.033604 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:03.040565 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:20:03.438311 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m06:20:03.458218 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-105b-18d6-954f-d6889b206567, command-id=01f0ec5a-1160-1530-985e-39e5f0874c6b) - Closing
[0m06:20:03.484761 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:20:03.493246 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:20:03.502046 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:20:03.837033 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m06:20:03.850871 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-105b-18d6-954f-d6889b206567, command-id=01f0ec5a-11a7-100b-8070-5cc443729530) - Closing
[0m06:20:03.874065 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m06:20:03.881973 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-105b-18d6-954f-d6889b206567) - Closing
[0m06:20:04.037866 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 3.31s]
[0m06:20:04.048452 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:20:04.057334 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:20:04.070485 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m06:20:04.076083 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m06:20:04.080254 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m06:20:04.083939 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:20:04.087317 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:20:04.163769 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:04.170429 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:20:04.175326 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:20:04.641097 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09) - Created
[0m06:20:05.067669 [debug] [Thread-4 (]: SQL status: OK in 0.890 seconds
[0m06:20:05.094234 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-1254-1cb6-a3bb-3d89d5cbb017) - Closing
[0m06:20:05.139853 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:05.455260 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:05.530472 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:05.534465 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m06:20:05.821666 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m06:20:05.845805 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-12db-1415-9ab3-a71fd1dfa48e) - Closing
[0m06:20:05.859156 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:05.867685 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:20:06.242678 [debug] [Thread-4 (]: SQL status: OK in 0.370 seconds
[0m06:20:06.255290 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-1312-1183-bd45-d8e15ca46b0e) - Closing
[0m06:20:06.278322 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:06.285991 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m06:20:06.607546 [debug] [Thread-4 (]: SQL status: OK in 0.320 seconds
[0m06:20:06.616491 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-134e-1eee-8f2b-4f52c1d0c2d4) - Closing
[0m06:20:06.622007 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:06.658560 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:06.662729 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:20:07.066553 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m06:20:07.086059 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-1388-11e3-b400-545da7a9ffe0) - Closing
[0m06:20:07.117628 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m06:20:07.127199 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:20:07.135996 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m06:20:07.402738 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m06:20:07.416258 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09, command-id=01f0ec5a-13d0-13ec-b14a-67f1c63eefbb) - Closing
[0m06:20:07.443918 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m06:20:07.451042 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5a-1240-13fb-bc5c-d262cbabcf09) - Closing
[0m06:20:07.588601 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 3.51s]
[0m06:20:07.599101 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:20:07.614503 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:20:07.620909 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:20:07.630326 [info ] [MainThread]: 
[0m06:20:07.636808 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 59.14 seconds (59.14s).
[0m06:20:07.659209 [debug] [MainThread]: Command end result
[0m06:20:08.119145 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:20:08.157698 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:20:08.244761 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:20:08.250677 [info ] [MainThread]: 
[0m06:20:08.257514 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:20:08.264488 [info ] [MainThread]: 
[0m06:20:08.271237 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m06:20:08.279826 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m06:20:08.286580 [info ] [MainThread]: 
[0m06:20:08.293401 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m06:20:08.299934 [info ] [MainThread]: 
[0m06:20:08.306828 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m06:20:08.316718 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 75.4654, "process_in_blocks": "64432", "process_kernel_time": 2.036143, "process_mem_max_rss": "262036", "process_out_blocks": "656", "process_user_time": 23.345087}
[0m06:20:08.324152 [debug] [MainThread]: Command `dbt test` failed at 06:20:08.323279 after 75.47 seconds
[0m06:20:08.331991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e853d5e4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e81e6c35b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75e81e6c2f80>]}
[0m06:20:08.338654 [debug] [MainThread]: Flushing usage events
[0m06:20:09.133143 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:36:35.624315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb02264e470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb0214742b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb021474250>]}


============================== 06:36:35.646559 | 85fc43e6-f551-4e6c-a491-8cd92e059aeb ==============================
[0m06:36:35.646559 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:36:35.632736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988d6a24a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988c4c8280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988c4c8220>]}
[0m06:36:35.650844 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'use_colors': 'True', 'target_path': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'quiet': 'False', 'log_cache_events': 'False', 'empty': 'None', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'warn_error': 'None', 'log_format': 'default', 'partial_parse': 'True', 'printer_width': '80', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'fail_fast': 'False', 'debug': 'False'}


============================== 06:36:35.654785 | fba298b6-5387-4093-98ae-9fb6da91e0cb ==============================
[0m06:36:35.654785 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:36:35.657584 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'empty': 'None', 'warn_error': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'quiet': 'False', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'printer_width': '80', 'version_check': 'True', 'debug': 'False', 'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'indirect_selection': 'eager', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False'}
[0m06:36:35.679461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90658de2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90646ac250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90646ac1f0>]}


============================== 06:36:35.689625 | f7850da2-0650-4011-87b0-6d7f14d930b8 ==============================
[0m06:36:35.689625 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:36:35.692832 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'write_json': 'True', 'warn_error': 'None', 'printer_width': '80', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'version_check': 'True', 'introspect': 'True', 'partial_parse': 'True', 'log_format': 'default', 'no_print': 'None', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'fail_fast': 'False', 'target_path': 'None'}
[0m06:36:37.422510 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:36:37.422509 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:36:37.423205 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:36:37.424729 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:36:37.424786 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:36:37.425252 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:36:37.426644 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:36:37.426639 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:36:37.427118 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:36:39.188698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988c66fb50>]}
[0m06:36:39.218335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e906565f760>]}
[0m06:36:39.220230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb02184f520>]}
[0m06:36:39.364896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779866f72f50>]}
[0m06:36:39.371294 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:36:39.397054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90657dcb20>]}
[0m06:36:39.397859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb0217cab60>]}
[0m06:36:39.400420 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:36:39.401104 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:36:39.714896 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:36:39.721478 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:36:39.725939 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:36:40.404069 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:36:40.405691 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:36:40.408124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:36:40.410228 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:36:40.424369 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:36:40.426115 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:36:40.427107 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:36:40.431022 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:36:40.443889 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:36:40.569079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9064dccb80>]}
[0m06:36:40.579231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb021ba4b80>]}
[0m06:36:40.595964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988cc1ccd0>]}
[0m06:36:40.919071 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:36:40.926086 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:36:40.926770 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:36:40.935070 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:36:40.938128 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:36:40.947693 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:36:40.959982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb021ba77f0>]}
[0m06:36:40.960647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903e79f1c0>]}
[0m06:36:40.961925 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:36:40.962509 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:36:40.964315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb021b96b00>]}
[0m06:36:40.964988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903e79c130>]}
[0m06:36:40.968590 [info ] [MainThread]: 
[0m06:36:40.969573 [info ] [MainThread]: 
[0m06:36:40.970270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:36:40.971082 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:36:40.971614 [info ] [MainThread]: 
[0m06:36:40.972391 [info ] [MainThread]: 
[0m06:36:40.973360 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:36:40.974098 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:36:40.974713 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:36:40.975480 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:36:40.989851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988cc1d7e0>]}
[0m06:36:40.990553 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:36:40.991872 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:36:40.992261 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:36:40.992271 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:36:40.993580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779866fd2140>]}
[0m06:36:40.993896 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:36:40.999875 [info ] [MainThread]: 
[0m06:36:41.001577 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:36:41.003018 [info ] [MainThread]: 
[0m06:36:41.004783 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:36:41.006353 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:36:41.015179 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:36:41.017009 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:36:41.018151 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:36:41.018608 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:36:41.019970 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:36:41.021217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:36:41.022430 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:36:41.024451 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:36:41.051421 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:36:41.053579 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:36:41.055556 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:36:41.951695 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-649e-1738-87c5-1dbf68bf8908) - Created
[0m06:36:41.952485 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6499-10f3-8e49-a2b9ba007967) - Created
[0m06:36:41.961865 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6499-123a-91e8-c40c9e6c3340) - Created
[0m06:36:59.632719 [debug] [ThreadPool]: SQL status: OK in 18.610 seconds
[0m06:36:59.633398 [debug] [ThreadPool]: SQL status: OK in 18.610 seconds
[0m06:36:59.661533 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-6499-123a-91e8-c40c9e6c3340, command-id=01f0ec5c-64cc-1960-bf7f-0852dbc31a9e) - Closing
[0m06:36:59.661771 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-649e-1738-87c5-1dbf68bf8908, command-id=01f0ec5c-64c9-1a39-89ef-8136d164ecab) - Closing
[0m06:37:00.061312 [debug] [ThreadPool]: On list_workspace: Close
[0m06:37:00.063535 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6499-123a-91e8-c40c9e6c3340) - Closing
[0m06:37:00.068159 [debug] [ThreadPool]: On list_workspace: Close
[0m06:37:00.070521 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-649e-1738-87c5-1dbf68bf8908) - Closing
[0m06:37:00.193802 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:37:00.195246 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:37:00.199766 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:37:00.201853 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:37:00.203524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:00.205659 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:37:00.207143 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:37:00.221594 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:37:00.223026 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:37:00.224403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:00.705540 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6fdb-13a0-a9b1-169644c6b7e3) - Created
[0m06:37:00.711966 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6fdc-125e-b351-9a1bf41b6cae) - Created
[0m06:37:01.755518 [debug] [ThreadPool]: SQL status: OK in 1.550 seconds
[0m06:37:01.761537 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-6fdc-125e-b351-9a1bf41b6cae, command-id=01f0ec5c-6ff4-11b0-a97b-2a153933fbc1) - Closing
[0m06:37:01.764272 [debug] [ThreadPool]: On list_workspace: Close
[0m06:37:01.766730 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6fdc-125e-b351-9a1bf41b6cae) - Closing
[0m06:37:01.787258 [debug] [ThreadPool]: SQL status: OK in 20.730 seconds
[0m06:37:01.793120 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-6499-10f3-8e49-a2b9ba007967, command-id=01f0ec5c-64c9-1ffd-9575-3a96027dbb25) - Closing
[0m06:37:01.898122 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:37:01.907821 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:37:01.946239 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:37:01.951095 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:37:01.955613 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:01.992354 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:37:01.994325 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6499-10f3-8e49-a2b9ba007967) - Closing
[0m06:37:02.123861 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:37:02.126462 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:37:02.135055 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:37:02.137848 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:37:02.140388 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:02.388910 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-70e2-13db-892b-357210de8b63) - Created
[0m06:37:02.539158 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-70f9-189e-ba9f-11b68a333044) - Created
[0m06:37:02.641097 [debug] [ThreadPool]: SQL status: OK in 2.420 seconds
[0m06:37:02.644924 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-6fdb-13a0-a9b1-169644c6b7e3, command-id=01f0ec5c-6ff4-1f68-80e6-80b640a1a8b8) - Closing
[0m06:37:02.646566 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:37:02.647723 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-6fdb-13a0-a9b1-169644c6b7e3) - Closing
[0m06:37:02.760410 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:37:02.762422 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:37:02.775424 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:37:02.777182 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:37:02.779023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:03.162727 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7159-1ec4-96c4-28bb5002f1c3) - Created
[0m06:37:04.498808 [debug] [ThreadPool]: SQL status: OK in 2.540 seconds
[0m06:37:04.502629 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-70e2-13db-892b-357210de8b63, command-id=01f0ec5c-70f3-18f7-9698-30e5f0b5a8fb) - Closing
[0m06:37:04.504549 [debug] [ThreadPool]: On list_workspace: Close
[0m06:37:04.505930 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-70e2-13db-892b-357210de8b63) - Closing
[0m06:37:04.596146 [debug] [ThreadPool]: SQL status: OK in 2.460 seconds
[0m06:37:04.602787 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-70f9-189e-ba9f-11b68a333044, command-id=01f0ec5c-710a-1d5a-b143-879739b371e6) - Closing
[0m06:37:04.605704 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:37:04.607685 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-70f9-189e-ba9f-11b68a333044) - Closing
[0m06:37:04.642141 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:37:04.643793 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:37:04.655335 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:37:04.657193 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:37:04.658656 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:04.743783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:37:04.746537 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:37:04.757504 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:37:04.759525 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:37:04.761071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:05.040998 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7277-18ae-a668-bec77b4feffd) - Created
[0m06:37:05.130315 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7285-18ab-982c-49a3acac352c) - Created
[0m06:37:05.258658 [debug] [ThreadPool]: SQL status: OK in 2.480 seconds
[0m06:37:05.264784 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-7159-1ec4-96c4-28bb5002f1c3, command-id=01f0ec5c-7169-127f-8f3a-d31c9784b26b) - Closing
[0m06:37:05.267604 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:37:05.269792 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7159-1ec4-96c4-28bb5002f1c3) - Closing
[0m06:37:05.403193 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:37:05.405146 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:37:05.411162 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:37:05.412681 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:37:05.414078 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:05.793252 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-72ea-1e89-b881-229104a4f4d7) - Created
[0m06:37:06.740233 [debug] [ThreadPool]: SQL status: OK in 1.330 seconds
[0m06:37:06.749569 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-72ea-1e89-b881-229104a4f4d7, command-id=01f0ec5c-72fa-1bbf-9481-c3b5a6127f23) - Closing
[0m06:37:06.753647 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:37:06.756479 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-72ea-1e89-b881-229104a4f4d7) - Closing
[0m06:37:06.820652 [debug] [ThreadPool]: SQL status: OK in 2.060 seconds
[0m06:37:06.826520 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-7285-18ab-982c-49a3acac352c, command-id=01f0ec5c-7295-1e8c-81d2-57c1f123429b) - Closing
[0m06:37:06.829567 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:37:06.832057 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7285-18ab-982c-49a3acac352c) - Closing
[0m06:37:06.889275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb01f456560>]}
[0m06:37:06.901758 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m06:37:06.905198 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m06:37:06.908350 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m06:37:06.910632 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m06:37:06.913015 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m06:37:06.915455 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m06:37:06.930882 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:37:06.933192 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baffa411c30>]}
[0m06:37:06.961379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fba298b6-5387-4093-98ae-9fb6da91e0cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988a4b5ed0>]}
[0m06:37:06.966038 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:37:06.967899 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m06:37:06.970031 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m06:37:06.971530 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m06:37:06.972840 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:37:06.979566 [debug] [ThreadPool]: SQL status: OK in 2.320 seconds
[0m06:37:06.984773 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-7277-18ae-a668-bec77b4feffd, command-id=01f0ec5c-7288-176b-b6f7-a7cbfce4f4aa) - Closing
[0m06:37:06.987047 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:37:06.988764 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-7277-18ae-a668-bec77b4feffd) - Closing
[0m06:37:06.993447 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:37:06.995789 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baffa430730>]}
[0m06:37:06.998822 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:37:07.012124 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:37:07.029721 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:37:07.031601 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:37:07.033459 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:37:07.042509 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:37:07.057000 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:37:07.058727 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m06:37:07.060521 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:07.131847 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:37:07.134316 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:37:07.143076 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:37:07.144788 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:37:07.146329 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:07.408194 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5c-73e1-13c6-9442-1451ce390372) - Created
[0m06:37:07.433548 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-73e5-13ba-8198-25fc608716e3) - Created
[0m06:37:07.518154 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-73f2-1a9b-a2a9-4ab32e9e6dec) - Created
[0m06:37:08.614266 [debug] [ThreadPool]: SQL status: OK in 1.470 seconds
[0m06:37:08.618383 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-73f2-1a9b-a2a9-4ab32e9e6dec, command-id=01f0ec5c-7402-1c5d-a912-c75a6bb469d1) - Closing
[0m06:37:08.620401 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:37:08.622459 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-73f2-1a9b-a2a9-4ab32e9e6dec) - Closing
[0m06:37:08.750984 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:37:08.753247 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:37:08.758035 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:37:08.759758 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:37:08.761450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:37:09.161390 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-74ec-1301-9cb2-78431c186b46) - Created
[0m06:37:11.156267 [debug] [ThreadPool]: SQL status: OK in 2.390 seconds
[0m06:37:11.163664 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5c-74ec-1301-9cb2-78431c186b46, command-id=01f0ec5c-74ff-1992-8e72-e9baf10df0dc) - Closing
[0m06:37:11.167742 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:37:11.169895 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5c-74ec-1301-9cb2-78431c186b46) - Closing
[0m06:37:11.317245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90673fd210>]}
[0m06:37:11.322682 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m06:37:11.324610 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m06:37:11.326566 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m06:37:11.328040 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m06:37:11.329578 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m06:37:11.346264 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m06:37:11.357762 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m06:37:11.379499 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:37:11.384618 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:37:11.386264 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903def23b0>]}
[0m06:37:11.401523 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m06:37:11.412072 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m06:37:11.427569 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m06:37:11.429093 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m06:37:11.430569 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:37:11.945798 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7683-1a22-8757-23ef2085e70c) - Created
[0m06:37:13.428481 [debug] [Thread-4 (]: SQL status: OK in 6.370 seconds
[0m06:37:13.435362 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-73e5-13ba-8198-25fc608716e3, command-id=01f0ec5c-73f5-185c-a2ab-29877b791dad) - Closing
[0m06:37:13.607497 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m06:37:13.610043 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-73e5-13ba-8198-25fc608716e3) - Closing
[0m06:37:13.751472 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 6.78s]
[0m06:37:13.755341 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:37:13.758134 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:37:13.760654 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m06:37:13.763306 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m06:37:13.765391 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m06:37:13.767654 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:37:13.775125 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:37:13.788400 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:37:13.792959 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:37:13.807582 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:37:13.808888 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m06:37:13.810048 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:13.929051 [debug] [Thread-7 (]: SQL status: OK in 2.500 seconds
[0m06:37:13.931474 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7683-1a22-8757-23ef2085e70c, command-id=01f0ec5c-76a5-19c5-a35c-b2a341eed59f) - Closing
[0m06:37:13.944874 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:37:13.948684 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m06:37:13.950856 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7683-1a22-8757-23ef2085e70c) - Closing
[0m06:37:14.091331 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903deb8580>]}
[0m06:37:14.093942 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.76s]
[0m06:37:14.096164 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m06:37:14.098491 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m06:37:14.101241 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m06:37:14.103763 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m06:37:14.105535 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m06:37:14.107311 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m06:37:14.113854 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m06:37:14.128404 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m06:37:14.133098 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:37:14.136042 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m06:37:14.138197 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m06:37:14.151541 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m06:37:14.153100 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m06:37:14.154400 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:37:14.182449 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-77eb-1400-829e-3af0faa8d257) - Created
[0m06:37:14.541478 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7822-1603-bc24-f58452d19c57) - Created
[0m06:37:15.170186 [debug] [Thread-5 (]: SQL status: OK in 8.140 seconds
[0m06:37:15.173997 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5c-73e1-13c6-9442-1451ce390372, command-id=01f0ec5c-73f1-1485-8871-93a687e8fa68) - Closing
[0m06:37:15.374276 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:37:15.376182 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m06:37:15.481617 [debug] [Thread-4 (]: SQL status: OK in 1.670 seconds
[0m06:37:15.488564 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-77eb-1400-829e-3af0faa8d257, command-id=01f0ec5c-77fd-1847-aec9-ea793ccd84e0) - Closing
[0m06:37:15.491488 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m06:37:15.493614 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-77eb-1400-829e-3af0faa8d257) - Closing
[0m06:37:15.625240 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.86s]
[0m06:37:15.628830 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:37:15.631838 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:37:15.634792 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m06:37:15.637569 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m06:37:15.639474 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m06:37:15.642175 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:37:15.655158 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:37:15.667434 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:37:15.676766 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:37:15.690014 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:37:15.691258 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m06:37:15.692310 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:16.093382 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-790e-152f-a031-5a78a74d50c2) - Created
[0m06:37:16.384369 [debug] [Thread-7 (]: SQL status: OK in 2.230 seconds
[0m06:37:16.386965 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7822-1603-bc24-f58452d19c57, command-id=01f0ec5c-7832-11be-aa4c-1e4b40ddb508) - Closing
[0m06:37:16.389164 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:37:16.391677 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m06:37:16.393361 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7822-1603-bc24-f58452d19c57) - Closing
[0m06:37:16.545903 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903f0dc250>]}
[0m06:37:16.550946 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 2.44s]
[0m06:37:16.554762 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m06:37:16.558922 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m06:37:16.562528 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m06:37:16.565668 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m06:37:16.568746 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m06:37:16.570737 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m06:37:16.590502 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m06:37:16.603886 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m06:37:16.608293 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:37:16.610755 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m06:37:16.613131 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m06:37:16.623624 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m06:37:16.625628 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m06:37:16.627292 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:37:17.114780 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-79a4-12dd-a7fa-4c89921e2c9b) - Created
[0m06:37:17.580933 [debug] [Thread-4 (]: SQL status: OK in 1.890 seconds
[0m06:37:17.586434 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-790e-152f-a031-5a78a74d50c2, command-id=01f0ec5c-7921-188b-bba8-916020076536) - Closing
[0m06:37:17.588825 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m06:37:17.590455 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-790e-152f-a031-5a78a74d50c2) - Closing
[0m06:37:17.719963 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 2.08s]
[0m06:37:17.724289 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:37:17.727117 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:37:17.729950 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m06:37:17.733354 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m06:37:17.735808 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m06:37:17.737591 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:37:17.748658 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:37:17.763249 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:37:17.769449 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:37:17.780616 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:37:17.781832 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m06:37:17.783550 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:18.295215 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7a5d-16d6-847f-d11d62769493) - Created
[0m06:37:18.549576 [debug] [Thread-7 (]: SQL status: OK in 1.920 seconds
[0m06:37:18.565176 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5c-79a4-12dd-a7fa-4c89921e2c9b, command-id=01f0ec5c-79bc-1b41-af54-a2c841b883b4) - Closing
[0m06:37:18.573813 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:37:18.580109 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m06:37:18.584434 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-79a4-12dd-a7fa-4c89921e2c9b) - Closing
[0m06:37:18.728665 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903f0dc250>]}
[0m06:37:18.731221 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.16s]
[0m06:37:18.733778 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m06:37:18.736672 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m06:37:18.738537 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m06:37:18.740495 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m06:37:18.742070 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m06:37:18.743513 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m06:37:18.756194 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m06:37:18.770853 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m06:37:18.814033 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m06:37:18.830206 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:37:18.831818 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903df37c70>]}
[0m06:37:18.863560 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m06:37:18.875213 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m06:37:18.876430 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m06:37:18.877455 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:37:19.257410 [debug] [Thread-4 (]: SQL status: OK in 1.470 seconds
[0m06:37:19.261165 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7a5d-16d6-847f-d11d62769493, command-id=01f0ec5c-7a70-1ffe-8eef-72d63a1fb5b6) - Closing
[0m06:37:19.263288 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m06:37:19.264888 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7a5d-16d6-847f-d11d62769493) - Closing
[0m06:37:19.323158 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7afb-174a-8c8f-3aba8447d4ff) - Created
[0m06:37:19.400453 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.67s]
[0m06:37:19.402804 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:37:19.404677 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:37:19.406283 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m06:37:19.408042 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m06:37:19.409417 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m06:37:19.410810 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:37:19.420230 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:37:19.431260 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:37:19.436997 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:37:19.446953 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:37:19.447958 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m06:37:19.448858 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:19.872084 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7b4c-19c6-8729-1fb5ac34b2bf) - Created
[0m06:37:23.020157 [debug] [Thread-5 (]: SQL status: OK in 7.640 seconds
[0m06:37:23.022076 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5c-73e1-13c6-9442-1451ce390372, command-id=01f0ec5c-78b1-10dc-b18d-c28d243dc2c0) - Closing
[0m06:37:23.222119 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m06:37:23.248234 [debug] [Thread-4 (]: SQL status: OK in 3.800 seconds
[0m06:37:23.252699 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7b4c-19c6-8729-1fb5ac34b2bf, command-id=01f0ec5c-7b61-10d7-b84a-e12ace473295) - Closing
[0m06:37:23.254772 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m06:37:23.256385 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7b4c-19c6-8729-1fb5ac34b2bf) - Closing
[0m06:37:23.260781 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m06:37:23.262482 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5c-73e1-13c6-9442-1451ce390372) - Closing
[0m06:37:23.390224 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 3.98s]
[0m06:37:23.392460 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:37:23.394503 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:37:23.395626 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb021b31900>]}
[0m06:37:23.396299 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m06:37:23.397491 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 16.49s]
[0m06:37:23.398384 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m06:37:23.399504 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m06:37:23.400255 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m06:37:23.401469 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m06:37:23.402039 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:37:23.403183 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m06:37:23.405015 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m06:37:23.406773 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m06:37:23.408239 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m06:37:23.409656 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m06:37:23.411777 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:37:23.424093 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:37:23.425378 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:37:23.425664 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:37:23.426641 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:37:23.430052 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:37:23.444823 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:37:23.446156 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m06:37:23.447249 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:23.832369 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7dab-169d-bc93-2a6338112ed5) - Created
[0m06:37:23.845481 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5c-7dac-1e52-8917-a5e7ce6a1df7) - Created
[0m06:37:24.760744 [debug] [Thread-4 (]: SQL status: OK in 1.310 seconds
[0m06:37:24.769981 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7dab-169d-bc93-2a6338112ed5, command-id=01f0ec5c-7dbb-175f-aa90-ed3f7cf29cc3) - Closing
[0m06:37:24.773888 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m06:37:24.776897 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7dab-169d-bc93-2a6338112ed5) - Closing
[0m06:37:24.916993 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.52s]
[0m06:37:24.920416 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:37:24.922921 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:37:24.925245 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m06:37:24.928244 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m06:37:24.930564 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m06:37:24.932855 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:37:24.946767 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:37:24.963175 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:37:24.968258 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:37:24.980097 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:37:24.981153 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m06:37:24.982378 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:25.356879 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7e92-1c20-af4b-1760b3db969d) - Created
[0m06:37:26.662769 [debug] [Thread-5 (]: SQL status: OK in 3.240 seconds
[0m06:37:26.671707 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7dac-1e52-8917-a5e7ce6a1df7, command-id=01f0ec5c-7dbd-1bbf-b455-9ebe3f477bea) - Closing
[0m06:37:26.679365 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:37:26.681618 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m06:37:27.290982 [debug] [Thread-4 (]: SQL status: OK in 2.310 seconds
[0m06:37:27.295532 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7e92-1c20-af4b-1760b3db969d, command-id=01f0ec5c-7ea4-131d-bbab-13f6f1d68ca1) - Closing
[0m06:37:27.297671 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m06:37:27.301335 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-7e92-1c20-af4b-1760b3db969d) - Closing
[0m06:37:27.431610 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 2.50s]
[0m06:37:27.440167 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:37:27.446990 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:37:27.459766 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m06:37:27.476084 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m06:37:27.482209 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m06:37:27.488562 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:37:27.508548 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:37:27.520895 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:37:27.526695 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:37:27.540892 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:37:27.542126 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:37:27.543285 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:27.753482 [debug] [Thread-7 (]: SQL status: OK in 8.880 seconds
[0m06:37:27.755560 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7afb-174a-8c8f-3aba8447d4ff, command-id=01f0ec5c-7b0c-11a0-bcb6-e2006c73f96a) - Closing
[0m06:37:27.924539 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:37:27.935924 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-801c-1edf-aed1-c7e62b01915b) - Created
[0m06:37:27.945189 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m06:37:27.946697 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5c-7afb-174a-8c8f-3aba8447d4ff) - Closing
[0m06:37:28.062205 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7850da2-0650-4011-87b0-6d7f14d930b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e903ef7c190>]}
[0m06:37:28.064508 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 9.32s]
[0m06:37:28.066538 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m06:37:28.070226 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:37:28.072049 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:37:28.073801 [info ] [MainThread]: 
[0m06:37:28.075418 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 47.10 seconds (47.10s).
[0m06:37:28.077823 [debug] [MainThread]: Command end result
[0m06:37:28.161493 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:37:28.170019 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:37:28.183005 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:37:28.184588 [info ] [MainThread]: 
[0m06:37:28.186463 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:37:28.187831 [info ] [MainThread]: 
[0m06:37:28.189171 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m06:37:28.191285 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 52.802193, "process_in_blocks": "0", "process_kernel_time": 1.005059, "process_mem_max_rss": "244216", "process_out_blocks": "336", "process_user_time": 9.351071}
[0m06:37:28.192635 [debug] [MainThread]: Command `dbt run` succeeded at 06:37:28.192531 after 52.80 seconds
[0m06:37:28.194019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90658de2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90673a1030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e90657dd0c0>]}
[0m06:37:28.195361 [debug] [MainThread]: Flushing usage events
[0m06:37:28.701899 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:37:29.219004 [debug] [Thread-4 (]: SQL status: OK in 1.680 seconds
[0m06:37:29.223463 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-801c-1edf-aed1-c7e62b01915b, command-id=01f0ec5c-802d-1787-809e-d167be9f1b72) - Closing
[0m06:37:29.226095 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m06:37:29.228188 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-801c-1edf-aed1-c7e62b01915b) - Closing
[0m06:37:29.362711 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.89s]
[0m06:37:29.365119 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:37:29.367230 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:37:29.370378 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m06:37:29.372962 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m06:37:29.374786 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m06:37:29.377221 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:37:29.395737 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:37:29.411013 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:37:29.417330 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:37:29.431949 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:37:29.433926 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:37:29.436704 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:29.857977 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8141-1e83-b208-3c0d8914a6f5) - Created
[0m06:37:29.928981 [debug] [Thread-5 (]: SQL status: OK in 3.250 seconds
[0m06:37:29.931207 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5c-7dac-1e52-8917-a5e7ce6a1df7, command-id=01f0ec5c-7f6e-1519-80d6-2997386642d8) - Closing
[0m06:37:29.933741 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m06:37:29.950893 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m06:37:29.953401 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5c-7dac-1e52-8917-a5e7ce6a1df7) - Closing
[0m06:37:30.085138 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85fc43e6-f551-4e6c-a491-8cd92e059aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb000158610>]}
[0m06:37:30.087272 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 6.68s]
[0m06:37:30.089027 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m06:37:30.091907 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:37:30.093486 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:37:30.095275 [info ] [MainThread]: 
[0m06:37:30.096741 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 49.12 seconds (49.12s).
[0m06:37:30.098996 [debug] [MainThread]: Command end result
[0m06:37:30.188925 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:37:30.196557 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:37:30.210786 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:37:30.212144 [info ] [MainThread]: 
[0m06:37:30.213576 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:37:30.214882 [info ] [MainThread]: 
[0m06:37:30.216232 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m06:37:30.218945 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 54.87279, "process_in_blocks": "0", "process_kernel_time": 0.99798, "process_mem_max_rss": "243192", "process_out_blocks": "616", "process_user_time": 9.054255}
[0m06:37:30.220781 [debug] [MainThread]: Command `dbt seed` succeeded at 06:37:30.220653 after 54.88 seconds
[0m06:37:30.222096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb02264e470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb0217cab60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bb02184f7f0>]}
[0m06:37:30.223365 [debug] [MainThread]: Flushing usage events
[0m06:37:30.611208 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:37:30.941734 [debug] [Thread-4 (]: SQL status: OK in 1.510 seconds
[0m06:37:30.946395 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8141-1e83-b208-3c0d8914a6f5, command-id=01f0ec5c-8153-1928-9cc1-9a170adceeb9) - Closing
[0m06:37:30.948981 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m06:37:30.951433 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8141-1e83-b208-3c0d8914a6f5) - Closing
[0m06:37:31.083603 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.71s]
[0m06:37:31.086240 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:37:31.088301 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:37:31.090016 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m06:37:31.092173 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m06:37:31.093717 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m06:37:31.098042 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:37:31.104341 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:37:31.176113 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:31.178245 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:37:31.179933 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:31.634508 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95) - Created
[0m06:37:32.015145 [debug] [Thread-4 (]: SQL status: OK in 0.840 seconds
[0m06:37:32.020750 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95, command-id=01f0ec5c-8262-11e5-835e-adf44712fe53) - Closing
[0m06:37:32.053175 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:32.076027 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:32.132419 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:32.134991 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:37:32.627594 [debug] [Thread-4 (]: SQL status: OK in 0.490 seconds
[0m06:37:32.630743 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95, command-id=01f0ec5c-82af-1ee1-8ad0-5c87d91114c5) - Closing
[0m06:37:32.637149 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:32.639215 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m06:37:33.019516 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m06:37:33.025601 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95, command-id=01f0ec5c-82ff-1da1-ab82-af661c0676e0) - Closing
[0m06:37:33.041508 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:33.060522 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:33.062025 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:37:33.563572 [debug] [Thread-4 (]: SQL status: OK in 0.500 seconds
[0m06:37:33.569196 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95, command-id=01f0ec5c-833c-10be-87d6-95fa14620cbe) - Closing
[0m06:37:33.578523 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:37:33.584027 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:37:33.585891 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:37:33.893396 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m06:37:33.900214 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95, command-id=01f0ec5c-838b-1fbd-b383-b04797000e6c) - Closing
[0m06:37:33.917315 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m06:37:33.921373 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8252-13ea-befd-fa74c495ce95) - Closing
[0m06:37:34.057263 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.96s]
[0m06:37:34.061462 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:37:34.065104 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:37:34.068778 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m06:37:34.073611 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m06:37:34.077304 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m06:37:34.080810 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:37:34.084440 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:37:34.107230 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:34.109163 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:37:34.110925 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:34.499470 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89) - Created
[0m06:37:34.830866 [debug] [Thread-4 (]: SQL status: OK in 0.720 seconds
[0m06:37:34.840655 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89, command-id=01f0ec5c-8417-132d-bd31-ca642285ea60) - Closing
[0m06:37:34.849074 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:34.877594 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:34.900160 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:34.901963 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:37:35.445542 [debug] [Thread-4 (]: SQL status: OK in 0.540 seconds
[0m06:37:35.448333 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89, command-id=01f0ec5c-8457-112a-a9a3-98afcb3ae7e0) - Closing
[0m06:37:35.453436 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:35.455291 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m06:37:36.022193 [debug] [Thread-4 (]: SQL status: OK in 0.570 seconds
[0m06:37:36.025411 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89, command-id=01f0ec5c-84a9-10d2-ab88-31c4ad79de01) - Closing
[0m06:37:36.029313 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:36.046022 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:36.049786 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:37:36.598396 [debug] [Thread-4 (]: SQL status: OK in 0.550 seconds
[0m06:37:36.604690 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89, command-id=01f0ec5c-8503-1cd4-82b4-59968990b89c) - Closing
[0m06:37:36.612805 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:37:36.615484 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:37:36.617553 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:37:37.025816 [debug] [Thread-4 (]: SQL status: OK in 0.410 seconds
[0m06:37:37.029063 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89, command-id=01f0ec5c-855b-10fe-b905-8d1a1ce2cd4a) - Closing
[0m06:37:37.033565 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m06:37:37.035558 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-8406-1277-8ece-b0497fd0cb89) - Closing
[0m06:37:37.166630 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 3.09s]
[0m06:37:37.169457 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:37:37.171786 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:37:37.173649 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m06:37:37.175902 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m06:37:37.177808 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m06:37:37.179608 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:37:37.181362 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:37:37.197139 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:37.198997 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:37:37.200717 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:37:37.608828 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8) - Created
[0m06:37:37.975704 [debug] [Thread-4 (]: SQL status: OK in 0.770 seconds
[0m06:37:37.979470 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-85f2-12a7-b07c-44c3459679ff) - Closing
[0m06:37:37.984597 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:38.010951 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:38.035156 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:38.037114 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m06:37:38.395905 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m06:37:38.400139 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-8633-1880-b6ca-8bd1dae6f341) - Closing
[0m06:37:38.403004 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:38.404716 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:37:38.730995 [debug] [Thread-4 (]: SQL status: OK in 0.320 seconds
[0m06:37:38.735157 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-866b-131b-a784-d9903190c35b) - Closing
[0m06:37:38.740877 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:38.742862 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m06:37:39.328476 [debug] [Thread-4 (]: SQL status: OK in 0.580 seconds
[0m06:37:39.336063 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-869f-151f-9e46-740c3e2247ce) - Closing
[0m06:37:39.340303 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:39.359211 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:39.360531 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:37:39.691530 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m06:37:39.696185 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-86fd-17e8-87da-7bf9b1223f7b) - Closing
[0m06:37:39.703188 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m06:37:39.705562 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:37:39.707445 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m06:37:39.984278 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m06:37:39.988273 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8, command-id=01f0ec5c-8731-1c72-bb5a-777bf82e62dd) - Closing
[0m06:37:39.994481 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m06:37:39.996496 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5c-85e1-11be-a6f1-4b9fe709c2f8) - Closing
[0m06:37:40.121795 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.95s]
[0m06:37:40.124027 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:37:40.127287 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:37:40.128911 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:37:40.130399 [info ] [MainThread]: 
[0m06:37:40.131571 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 59.13 seconds (59.13s).
[0m06:37:40.136845 [debug] [MainThread]: Command end result
[0m06:37:40.347284 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:37:40.356454 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:37:40.373012 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:37:40.374621 [info ] [MainThread]: 
[0m06:37:40.376368 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:37:40.377871 [info ] [MainThread]: 
[0m06:37:40.379492 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m06:37:40.381144 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m06:37:40.382815 [info ] [MainThread]: 
[0m06:37:40.384886 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m06:37:40.386499 [info ] [MainThread]: 
[0m06:37:40.388116 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m06:37:40.390430 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 65.04548, "process_in_blocks": "0", "process_kernel_time": 1.219508, "process_mem_max_rss": "252792", "process_out_blocks": "624", "process_user_time": 10.146472}
[0m06:37:40.392167 [debug] [MainThread]: Command `dbt test` failed at 06:37:40.392008 after 65.05 seconds
[0m06:37:40.393783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988d6a24a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988d9af700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77988cbe2200>]}
[0m06:37:40.395363 [debug] [MainThread]: Flushing usage events
[0m06:37:40.764839 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:44:39.854651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855dc163b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855ca582b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855ca58250>]}


============================== 06:44:39.869922 | 6a1e0289-0e3b-476d-a64e-ab882c27974d ==============================
[0m06:44:39.869922 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:44:39.860731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad107e440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bacfea8310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bacfea82b0>]}
[0m06:44:39.872152 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/dagster/app/dbt/logs', 'use_colors': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'empty': 'None', 'static_parser': 'True', 'printer_width': '80', 'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'debug': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'fail_fast': 'False'}
[0m06:44:39.857257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3924252260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3922ff8280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3922ff8220>]}


============================== 06:44:39.873653 | 695ae45d-d380-4830-873d-f92b42d22e78 ==============================
[0m06:44:39.873653 [info ] [MainThread]: Running with dbt=1.10.18


============================== 06:44:39.875082 | 6de78063-985d-46c7-8c3e-462bebbad7c6 ==============================
[0m06:44:39.875082 [info ] [MainThread]: Running with dbt=1.10.18
[0m06:44:39.875772 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'warn_error': 'None', 'write_json': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'target_path': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'introspect': 'True', 'log_format': 'default', 'quiet': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'debug': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'version_check': 'True'}
[0m06:44:39.877686 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'use_colors': 'True', 'debug': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'warn_error': 'None', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'profiles_dir': '/opt/dagster/app/dbt', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'target_path': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'empty': 'None', 'version_check': 'True', 'introspect': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs'}
[0m06:44:41.584121 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:44:41.584057 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:44:41.584563 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:44:41.586378 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:44:41.586335 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:44:41.586921 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:44:41.587973 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:44:41.588134 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:44:41.588389 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:44:43.310887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7285373fae90>]}
[0m06:44:43.315534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bacfefc0a0>]}
[0m06:44:43.332184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a39231a7a00>]}
[0m06:44:43.501917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855d2bac20>]}
[0m06:44:43.503967 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:44:43.504246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bace0e15a0>]}
[0m06:44:43.506409 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:44:43.528839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a38fdd16e00>]}
[0m06:44:43.531562 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m06:44:43.830329 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:44:43.878461 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:44:43.889866 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m06:44:44.470514 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:44:44.472144 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:44:44.489347 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:44:44.503293 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:44:44.505164 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:44:44.527447 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:44:44.528244 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:44:44.529329 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:44:44.547811 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m06:44:44.630535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855d15cbe0>]}
[0m06:44:44.668434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa9f78b50>]}
[0m06:44:44.692257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3923740af0>]}
[0m06:44:44.959442 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:44:44.968353 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:44:45.012557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x728536b44610>]}
[0m06:44:45.014476 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:44:45.014579 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:44:45.016541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x728536b45c60>]}
[0m06:44:45.021577 [info ] [MainThread]: 
[0m06:44:45.023338 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:44:45.023972 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:44:45.024842 [info ] [MainThread]: 
[0m06:44:45.026768 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:44:45.028280 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:44:45.043090 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:44:45.044961 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:44:45.045243 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:44:45.046371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa9f5fe50>]}
[0m06:44:45.048418 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:44:45.051136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa9f9d210>]}
[0m06:44:45.055695 [info ] [MainThread]: 
[0m06:44:45.056169 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:44:45.057516 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:44:45.059131 [info ] [MainThread]: 
[0m06:44:45.061316 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:44:45.063422 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:44:45.075497 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:44:45.077903 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:44:45.079911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:45.087019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3923743820>]}
[0m06:44:45.088682 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m06:44:45.090061 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:44:45.090255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a392372ae60>]}
[0m06:44:45.091541 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:44:45.093996 [info ] [MainThread]: 
[0m06:44:45.095418 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:44:45.096682 [info ] [MainThread]: 
[0m06:44:45.098565 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:44:45.101397 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:44:45.115624 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:44:45.118153 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:44:45.120961 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:44:45.122625 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:44:45.123958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:45.146127 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:44:45.147929 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:44:45.149787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:45.723032 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1b11-a172-90af57cde229) - Created
[0m06:44:45.723139 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1c2f-bf5e-e452d0cba3a9) - Created
[0m06:44:45.723443 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1ead-83b5-596485b2387f) - Created
[0m06:44:46.304611 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m06:44:46.310946 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-850b-1ead-83b5-596485b2387f, command-id=01f0ec5d-8523-1719-b52a-beddf0b276ea) - Closing
[0m06:44:46.313213 [debug] [ThreadPool]: On list_workspace: Close
[0m06:44:46.315053 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1ead-83b5-596485b2387f) - Closing
[0m06:44:46.354767 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m06:44:46.360666 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-850b-1c2f-bf5e-e452d0cba3a9, command-id=01f0ec5d-8523-1554-af1b-b9ff00f5abbe) - Closing
[0m06:44:46.362838 [debug] [ThreadPool]: On list_workspace: Close
[0m06:44:46.364571 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1c2f-bf5e-e452d0cba3a9) - Closing
[0m06:44:46.427329 [debug] [ThreadPool]: SQL status: OK in 1.350 seconds
[0m06:44:46.435173 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-850b-1b11-a172-90af57cde229, command-id=01f0ec5d-8523-1f7d-a130-abfd7e12aa49) - Closing
[0m06:44:46.437493 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:44:46.439144 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-850b-1b11-a172-90af57cde229) - Closing
[0m06:44:46.445157 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:44:46.446948 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:44:46.452908 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:44:46.454812 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:44:46.456371 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:46.504058 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:44:46.506060 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:44:46.522041 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:44:46.524194 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:44:46.526065 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:46.590954 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:44:46.592903 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:44:46.598913 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:44:46.601371 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:44:46.603358 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:46.861866 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85bc-15c8-bb96-d04276c5f52c) - Created
[0m06:44:46.932314 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85c7-178b-bf61-51af09e25204) - Created
[0m06:44:47.012913 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85d3-19bf-8c24-cf6c8c8cb077) - Created
[0m06:44:47.220772 [debug] [ThreadPool]: SQL status: OK in 0.760 seconds
[0m06:44:47.226125 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-85bc-15c8-bb96-d04276c5f52c, command-id=01f0ec5d-85d0-10ac-b7d2-15b67f12f1c5) - Closing
[0m06:44:47.228469 [debug] [ThreadPool]: On list_workspace: Close
[0m06:44:47.230687 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85bc-15c8-bb96-d04276c5f52c) - Closing
[0m06:44:47.360258 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m06:44:47.362142 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:44:47.374709 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:44:47.376701 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m06:44:47.378518 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:47.574244 [debug] [ThreadPool]: SQL status: OK in 1.050 seconds
[0m06:44:47.584406 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-85c7-178b-bf61-51af09e25204, command-id=01f0ec5d-85db-1ce5-9f7b-c509a1ed55e3) - Closing
[0m06:44:47.587912 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:44:47.590661 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85c7-178b-bf61-51af09e25204) - Closing
[0m06:44:47.614427 [debug] [ThreadPool]: SQL status: OK in 1.010 seconds
[0m06:44:47.623453 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-85d3-19bf-8c24-cf6c8c8cb077, command-id=01f0ec5d-85e4-1028-a68f-1ff779357375) - Closing
[0m06:44:47.626585 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:44:47.629400 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-85d3-19bf-8c24-cf6c8c8cb077) - Closing
[0m06:44:47.724428 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:44:47.726352 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:44:47.737918 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:44:47.739467 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:44:47.740759 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:47.757509 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:44:47.759667 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:44:47.768850 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:44:47.771094 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:44:47.773345 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8646-1eea-9b18-41b1df2f4fa5) - Created
[0m06:44:47.774524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:48.089208 [debug] [ThreadPool]: SQL status: OK in 0.710 seconds
[0m06:44:48.100606 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-8646-1eea-9b18-41b1df2f4fa5, command-id=01f0ec5d-8658-1726-bc09-8c992e66ce0e) - Closing
[0m06:44:48.105050 [debug] [ThreadPool]: On list_workspace: Close
[0m06:44:48.107891 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8646-1eea-9b18-41b1df2f4fa5) - Closing
[0m06:44:48.151901 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8681-1b35-87ff-8c0143b85ca3) - Created
[0m06:44:48.203476 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8689-1610-bb6b-8ea01070f738) - Created
[0m06:44:48.260095 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m06:44:48.265514 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m06:44:48.318760 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m06:44:48.322867 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m06:44:48.325899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:48.763711 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m06:44:48.774273 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-86de-1126-afe4-4c1f6b2ac476) - Created
[0m06:44:48.777692 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-8681-1b35-87ff-8c0143b85ca3, command-id=01f0ec5d-8691-1f3f-b40b-c4df4011c281) - Closing
[0m06:44:48.781957 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:44:48.785514 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8681-1b35-87ff-8c0143b85ca3) - Closing
[0m06:44:48.798973 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m06:44:48.806693 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-8689-1610-bb6b-8ea01070f738, command-id=01f0ec5d-869a-17c8-bcc0-e9e574486a9b) - Closing
[0m06:44:48.810414 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:44:48.813578 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-8689-1610-bb6b-8ea01070f738) - Closing
[0m06:44:48.953513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a1e0289-0e3b-476d-a64e-ab882c27974d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855aa05e40>]}
[0m06:44:48.950367 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:44:48.963330 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:44:48.981429 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:44:48.986452 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:44:48.990081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:48.992982 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:44:48.995285 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m06:44:48.997896 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m06:44:49.000027 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m06:44:49.002717 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:44:49.030232 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:44:49.044434 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:44:49.069533 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:44:49.082361 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m06:44:49.083858 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m06:44:49.085896 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:49.372335 [debug] [ThreadPool]: SQL status: OK in 1.050 seconds
[0m06:44:49.373712 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-873c-121f-bf4c-dd9a58f48bd0) - Created
[0m06:44:49.377910 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-86de-1126-afe4-4c1f6b2ac476, command-id=01f0ec5d-86f1-1994-970c-7dc1f9482bb0) - Closing
[0m06:44:49.380263 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m06:44:49.382108 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-86de-1126-afe4-4c1f6b2ac476) - Closing
[0m06:44:49.460503 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8747-1d2c-bde2-59f06e8c3b3f) - Created
[0m06:44:49.526357 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m06:44:49.530280 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m06:44:49.536903 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m06:44:49.539007 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m06:44:49.540895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:49.807862 [debug] [ThreadPool]: SQL status: OK in 0.820 seconds
[0m06:44:49.812833 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-873c-121f-bf4c-dd9a58f48bd0, command-id=01f0ec5d-874c-126e-a805-d1f5d7a6af00) - Closing
[0m06:44:49.815354 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:44:49.817469 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-873c-121f-bf4c-dd9a58f48bd0) - Closing
[0m06:44:49.904916 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-878c-1317-81e4-79b4bffdc944) - Created
[0m06:44:49.944091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3925b32800>]}
[0m06:44:49.956177 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m06:44:49.960160 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m06:44:49.964860 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m06:44:49.969692 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m06:44:49.974843 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m06:44:49.978674 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m06:44:50.005497 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:44:50.007617 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a38f7f5a020>]}
[0m06:44:50.118651 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:44:50.122115 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a38f7f53d90>]}
[0m06:44:50.175301 [debug] [Thread-4 (]: SQL status: OK in 1.090 seconds
[0m06:44:50.182194 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8747-1d2c-bde2-59f06e8c3b3f, command-id=01f0ec5d-875a-143a-bc66-fc8f2e89af28) - Closing
[0m06:44:50.192504 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m06:44:50.193769 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:44:50.194398 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8747-1d2c-bde2-59f06e8c3b3f) - Closing
[0m06:44:50.195371 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:44:50.196825 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:44:50.340401 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.34s]
[0m06:44:50.345555 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m06:44:50.348919 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:44:50.352394 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m06:44:50.355428 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m06:44:50.357522 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m06:44:50.359964 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:44:50.369602 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:44:50.382537 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:44:50.387099 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:44:50.395704 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m06:44:50.396653 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m06:44:50.397504 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:50.510651 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m06:44:50.514404 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-878c-1317-81e4-79b4bffdc944, command-id=01f0ec5d-879d-1f34-ac7b-8ea32c429923) - Closing
[0m06:44:50.516300 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m06:44:50.517968 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-878c-1317-81e4-79b4bffdc944) - Closing
[0m06:44:50.621812 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5d-87f8-1d2b-bf74-52388c24b368) - Created
[0m06:44:50.641302 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m06:44:50.648862 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m06:44:50.667013 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m06:44:50.671949 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m06:44:50.675568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:44:50.777259 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8810-1eb5-ad80-823435fb7f49) - Created
[0m06:44:51.077499 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-883f-1a65-a0e7-0676fd7dfb02) - Created
[0m06:44:51.580037 [debug] [Thread-4 (]: SQL status: OK in 1.180 seconds
[0m06:44:51.583945 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8810-1eb5-ad80-823435fb7f49, command-id=01f0ec5d-8824-1e79-bf06-341513265e0f) - Closing
[0m06:44:51.586434 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m06:44:51.588075 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8810-1eb5-ad80-823435fb7f49) - Closing
[0m06:44:51.615386 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m06:44:51.619070 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5d-883f-1a65-a0e7-0676fd7dfb02, command-id=01f0ec5d-8850-17aa-806d-b98734a28c86) - Closing
[0m06:44:51.620945 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m06:44:51.622333 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5d-883f-1a65-a0e7-0676fd7dfb02) - Closing
[0m06:44:51.729412 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.37s]
[0m06:44:51.733407 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m06:44:51.736720 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:44:51.739487 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m06:44:51.742009 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m06:44:51.743878 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m06:44:51.745602 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:44:51.752989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad29ab940>]}
[0m06:44:51.757851 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:44:51.758114 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m06:44:51.760001 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m06:44:51.761786 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m06:44:51.763019 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m06:44:51.764150 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m06:44:51.771676 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:44:51.779641 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m06:44:51.780052 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:44:51.793411 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m06:44:51.793637 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m06:44:51.794671 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m06:44:51.795708 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:51.834538 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:44:51.839268 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:44:51.841400 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa9f7bc70>]}
[0m06:44:51.878792 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m06:44:51.894864 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m06:44:51.911568 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m06:44:51.914290 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m06:44:51.916354 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:44:52.193966 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-88e9-11ec-9d61-596f47583825) - Created
[0m06:44:52.349442 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8901-1f02-acff-0e25171795bb) - Created
[0m06:44:52.594773 [debug] [Thread-5 (]: SQL status: OK in 2.400 seconds
[0m06:44:52.606428 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5d-87f8-1d2b-bf74-52388c24b368, command-id=01f0ec5d-880b-1205-9d9b-b5fb6d43475c) - Closing
[0m06:44:52.644060 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m06:44:52.645644 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m06:44:53.147118 [debug] [Thread-4 (]: SQL status: OK in 1.350 seconds
[0m06:44:53.151573 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-88e9-11ec-9d61-596f47583825, command-id=01f0ec5d-88fa-1f80-9c59-2ecf8698b3d6) - Closing
[0m06:44:53.154046 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m06:44:53.155749 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-88e9-11ec-9d61-596f47583825) - Closing
[0m06:44:53.296509 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.55s]
[0m06:44:53.298765 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m06:44:53.300595 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:44:53.303037 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m06:44:53.305608 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m06:44:53.307466 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m06:44:53.309233 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:44:53.319573 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:44:53.331477 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:44:53.336409 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:44:53.347719 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m06:44:53.349221 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m06:44:53.351039 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:53.390967 [debug] [Thread-7 (]: SQL status: OK in 1.470 seconds
[0m06:44:53.393034 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8901-1f02-acff-0e25171795bb, command-id=01f0ec5d-8912-17df-b852-af892c1183f7) - Closing
[0m06:44:53.408734 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:44:53.412830 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m06:44:53.414584 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8901-1f02-acff-0e25171795bb) - Closing
[0m06:44:53.557663 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa9ef59c0>]}
[0m06:44:53.559823 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.79s]
[0m06:44:53.561776 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m06:44:53.563438 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m06:44:53.565244 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m06:44:53.567244 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m06:44:53.568940 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m06:44:53.570925 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m06:44:53.576788 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m06:44:53.591271 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m06:44:53.595510 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:44:53.597667 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m06:44:53.599422 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m06:44:53.612649 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m06:44:53.613874 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m06:44:53.615198 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:44:53.748601 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-89d6-1681-b6d4-01a389108d66) - Created
[0m06:44:54.024112 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8a01-170b-98c6-2d766fe8afc7) - Created
[0m06:44:54.702885 [debug] [Thread-5 (]: SQL status: OK in 2.060 seconds
[0m06:44:54.707736 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5d-87f8-1d2b-bf74-52388c24b368, command-id=01f0ec5d-8940-1d46-a653-d7255ee8fbfc) - Closing
[0m06:44:54.730100 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m06:44:54.766044 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m06:44:54.767899 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5d-87f8-1d2b-bf74-52388c24b368) - Closing
[0m06:44:54.904038 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a39237435e0>]}
[0m06:44:54.906735 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 4.94s]
[0m06:44:54.909313 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m06:44:54.911497 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m06:44:54.913823 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m06:44:54.916206 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m06:44:54.919050 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m06:44:54.921871 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m06:44:54.923891 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m06:44:54.940657 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:44:54.942152 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m06:44:54.943537 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m06:44:55.137367 [debug] [Thread-4 (]: SQL status: OK in 1.790 seconds
[0m06:44:55.140816 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-89d6-1681-b6d4-01a389108d66, command-id=01f0ec5d-89e8-1e82-8ef1-ad34d128084e) - Closing
[0m06:44:55.142631 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m06:44:55.143957 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-89d6-1681-b6d4-01a389108d66) - Closing
[0m06:44:55.219402 [debug] [Thread-7 (]: SQL status: OK in 1.600 seconds
[0m06:44:55.221969 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8a01-170b-98c6-2d766fe8afc7, command-id=01f0ec5d-8a11-181b-a97e-6bfd7e6cfe6c) - Closing
[0m06:44:55.223941 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:44:55.226371 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m06:44:55.227921 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8a01-170b-98c6-2d766fe8afc7) - Closing
[0m06:44:55.278389 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.97s]
[0m06:44:55.280255 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m06:44:55.281864 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:44:55.283320 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m06:44:55.285151 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m06:44:55.287198 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m06:44:55.288744 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:44:55.296716 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:44:55.309912 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:44:55.310905 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5d-8ac5-1436-98f7-8d561a2b6ef2) - Created
[0m06:44:55.315468 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:44:55.327998 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m06:44:55.329175 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m06:44:55.330189 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:55.369219 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad2588460>]}
[0m06:44:55.371779 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.80s]
[0m06:44:55.374280 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m06:44:55.377186 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m06:44:55.379198 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m06:44:55.381254 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m06:44:55.382733 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m06:44:55.384196 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m06:44:55.396535 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m06:44:55.409325 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m06:44:55.413217 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m06:44:55.415136 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m06:44:55.417088 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m06:44:55.431917 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m06:44:55.433340 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m06:44:55.434483 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:44:55.722363 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8b04-1479-99f0-bd6a5be6e06d) - Created
[0m06:44:55.817365 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8b12-1a0d-881a-524a7b696019) - Created
[0m06:44:56.589824 [debug] [Thread-4 (]: SQL status: OK in 1.260 seconds
[0m06:44:56.596962 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8b04-1479-99f0-bd6a5be6e06d, command-id=01f0ec5d-8b15-139f-b9cc-90da8c7934f5) - Closing
[0m06:44:56.600912 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m06:44:56.604567 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8b04-1479-99f0-bd6a5be6e06d) - Closing
[0m06:44:56.744677 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.46s]
[0m06:44:56.751098 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m06:44:56.756963 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:44:56.762146 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m06:44:56.767878 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m06:44:56.773355 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m06:44:56.778497 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:44:56.809922 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:44:56.827857 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:44:56.831931 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:44:56.846012 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m06:44:56.847223 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m06:44:56.848225 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:57.223449 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8be8-1e70-87cf-53233f53f72b) - Created
[0m06:44:57.287514 [debug] [Thread-7 (]: SQL status: OK in 1.850 seconds
[0m06:44:57.290118 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8b12-1a0d-881a-524a7b696019, command-id=01f0ec5d-8b25-1348-ad61-87707ffa7378) - Closing
[0m06:44:57.292196 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:44:57.294822 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m06:44:57.295819 [debug] [Thread-5 (]: SQL status: OK in 2.350 seconds
[0m06:44:57.296455 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8b12-1a0d-881a-524a7b696019) - Closing
[0m06:44:57.298380 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8ac5-1436-98f7-8d561a2b6ef2, command-id=01f0ec5d-8ad5-1b26-94f7-a45c08870409) - Closing
[0m06:44:57.303082 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m06:44:57.305156 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m06:44:57.423372 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baaa877fa0>]}
[0m06:44:57.425872 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.04s]
[0m06:44:57.428072 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m06:44:57.430563 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m06:44:57.432583 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m06:44:57.434743 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m06:44:57.436961 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m06:44:57.439145 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m06:44:57.447723 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m06:44:57.461611 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m06:44:57.484225 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m06:44:57.499240 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m06:44:57.501041 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76baa85d0910>]}
[0m06:44:57.531339 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m06:44:57.544759 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m06:44:57.545976 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m06:44:57.546952 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m06:44:57.863277 [debug] [Thread-4 (]: SQL status: OK in 1.020 seconds
[0m06:44:57.866923 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8be8-1e70-87cf-53233f53f72b, command-id=01f0ec5d-8bfa-1ef6-ac82-0d907d69e1fd) - Closing
[0m06:44:57.868686 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m06:44:57.870612 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8be8-1e70-87cf-53233f53f72b) - Closing
[0m06:44:57.950245 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8c58-1c15-90a8-cc24646b12b1) - Created
[0m06:44:57.992870 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.23s]
[0m06:44:57.994735 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m06:44:57.996106 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:44:57.997566 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m06:44:57.999172 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m06:44:58.000396 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m06:44:58.001710 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:44:58.011636 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:44:58.027581 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:44:58.032210 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:44:58.051820 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m06:44:58.053808 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m06:44:58.055731 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:58.436964 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8ca1-1679-b090-c1613c961c25) - Created
[0m06:44:59.456490 [debug] [Thread-4 (]: SQL status: OK in 1.400 seconds
[0m06:44:59.465696 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8ca1-1679-b090-c1613c961c25, command-id=01f0ec5d-8cb3-1265-857b-36cf7ae5ca12) - Closing
[0m06:44:59.474997 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m06:44:59.478700 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8ca1-1679-b090-c1613c961c25) - Closing
[0m06:44:59.522366 [debug] [Thread-5 (]: SQL status: OK in 2.220 seconds
[0m06:44:59.525081 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8ac5-1436-98f7-8d561a2b6ef2, command-id=01f0ec5d-8c06-1add-a2da-56f89af5ced9) - Closing
[0m06:44:59.528125 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m06:44:59.551432 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m06:44:59.554175 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5d-8ac5-1436-98f7-8d561a2b6ef2) - Closing
[0m06:44:59.627884 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.63s]
[0m06:44:59.630544 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m06:44:59.632821 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:44:59.635097 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m06:44:59.638264 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m06:44:59.640757 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m06:44:59.643006 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:44:59.660443 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:44:59.675352 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:44:59.680825 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:44:59.688878 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6de78063-985d-46c7-8c3e-462bebbad7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a38fdab5600>]}
[0m06:44:59.690794 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 4.77s]
[0m06:44:59.692357 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m06:44:59.695112 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:44:59.695112 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m06:44:59.696541 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:44:59.696657 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:44:59.698015 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:44:59.698150 [info ] [MainThread]: 
[0m06:44:59.699559 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 14.60 seconds (14.60s).
[0m06:44:59.701492 [debug] [MainThread]: Command end result
[0m06:44:59.802301 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:44:59.811246 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:44:59.825029 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:44:59.826519 [info ] [MainThread]: 
[0m06:44:59.827822 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:44:59.828994 [info ] [MainThread]: 
[0m06:44:59.830233 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m06:44:59.832245 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 20.17455, "process_in_blocks": "0", "process_kernel_time": 0.876408, "process_mem_max_rss": "245192", "process_out_blocks": "312", "process_user_time": 9.479688}
[0m06:44:59.833633 [debug] [MainThread]: Command `dbt seed` succeeded at 06:44:59.833515 after 20.18 seconds
[0m06:44:59.834802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3924252260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3924121180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a38fdd16e00>]}
[0m06:44:59.836131 [debug] [MainThread]: Flushing usage events
[0m06:45:00.096018 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8d9e-1832-ae95-a6cb873c9372) - Created
[0m06:45:00.266307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:45:00.906799 [debug] [Thread-4 (]: SQL status: OK in 1.210 seconds
[0m06:45:00.910511 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8d9e-1832-ae95-a6cb873c9372, command-id=01f0ec5d-8db0-146c-aaf8-1a87428b5519) - Closing
[0m06:45:00.912249 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m06:45:00.913754 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8d9e-1832-ae95-a6cb873c9372) - Closing
[0m06:45:01.033488 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.40s]
[0m06:45:01.035785 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m06:45:01.038858 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:45:01.040890 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m06:45:01.043469 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m06:45:01.046294 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m06:45:01.048496 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:45:01.062542 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:45:01.079727 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:45:01.086945 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:45:01.102602 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m06:45:01.104677 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m06:45:01.106572 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:45:01.520897 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8e78-1dee-8de8-76a407f39c73) - Created
[0m06:45:02.210318 [debug] [Thread-7 (]: SQL status: OK in 4.660 seconds
[0m06:45:02.214897 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8c58-1c15-90a8-cc24646b12b1, command-id=01f0ec5d-8c6a-1051-a214-0d36dbf728b1) - Closing
[0m06:45:02.223600 [debug] [Thread-7 (]: Applying tags to relation None
[0m06:45:02.245535 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m06:45:02.247218 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5d-8c58-1c15-90a8-cc24646b12b1) - Closing
[0m06:45:02.382205 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '695ae45d-d380-4830-873d-f92b42d22e78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad1c68040>]}
[0m06:45:02.386865 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 4.95s]
[0m06:45:02.390855 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m06:45:02.395487 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:45:02.397262 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:45:02.399223 [info ] [MainThread]: 
[0m06:45:02.400773 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 17.34 seconds (17.34s).
[0m06:45:02.403647 [debug] [MainThread]: Command end result
[0m06:45:02.494304 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:45:02.503080 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:45:02.517561 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:45:02.518921 [info ] [MainThread]: 
[0m06:45:02.520570 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:45:02.522509 [info ] [MainThread]: 
[0m06:45:02.523868 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m06:45:02.525749 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 22.877302, "process_in_blocks": "0", "process_kernel_time": 0.924605, "process_mem_max_rss": "247640", "process_out_blocks": "472", "process_user_time": 9.738378}
[0m06:45:02.527042 [debug] [MainThread]: Command `dbt run` succeeded at 06:45:02.526939 after 22.88 seconds
[0m06:45:02.528339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad107e440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad0fe33d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76bad2ba50c0>]}
[0m06:45:02.529722 [debug] [MainThread]: Flushing usage events
[0m06:45:02.569571 [debug] [Thread-4 (]: SQL status: OK in 1.460 seconds
[0m06:45:02.576560 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8e78-1dee-8de8-76a407f39c73, command-id=01f0ec5d-8e8b-11ee-98e2-f8346825a5aa) - Closing
[0m06:45:02.579743 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m06:45:02.581746 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8e78-1dee-8de8-76a407f39c73) - Closing
[0m06:45:02.707150 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.66s]
[0m06:45:02.709627 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m06:45:02.711693 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:45:02.713890 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m06:45:02.716323 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m06:45:02.718650 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m06:45:02.721000 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:45:02.723039 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:45:02.791302 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:02.793397 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:45:02.795066 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:45:02.970974 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:45:03.192983 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f) - Created
[0m06:45:03.518761 [debug] [Thread-4 (]: SQL status: OK in 0.720 seconds
[0m06:45:03.524247 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f, command-id=01f0ec5d-8f89-164e-acf0-1b71ca65563b) - Closing
[0m06:45:03.547958 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:03.575650 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:03.646237 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:03.648408 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:45:03.920597 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m06:45:03.923887 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f, command-id=01f0ec5d-8fce-1951-b072-b7456f52372e) - Closing
[0m06:45:03.929935 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:03.931851 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m06:45:04.195780 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m06:45:04.200359 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f, command-id=01f0ec5d-8ff9-1522-8ed4-ed3341d15d7e) - Closing
[0m06:45:04.217231 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:04.236013 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:04.238968 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:45:04.535004 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m06:45:04.539792 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f, command-id=01f0ec5d-9028-13dc-980c-2d52351a151c) - Closing
[0m06:45:04.548306 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:45:04.554156 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m06:45:04.556511 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m06:45:04.794680 [debug] [Thread-4 (]: SQL status: OK in 0.240 seconds
[0m06:45:04.797319 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f, command-id=01f0ec5d-9058-19f9-a3a3-c2b1775b57a5) - Closing
[0m06:45:04.803994 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m06:45:04.806535 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-8f78-169c-98e5-4ebad6aedf5f) - Closing
[0m06:45:04.927942 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.21s]
[0m06:45:04.929798 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m06:45:04.931141 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:45:04.932579 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m06:45:04.934303 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m06:45:04.935826 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m06:45:04.937433 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:45:04.939566 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:45:04.953694 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:04.955634 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:45:04.957269 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:45:05.355025 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2) - Created
[0m06:45:05.712087 [debug] [Thread-4 (]: SQL status: OK in 0.750 seconds
[0m06:45:05.716594 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2, command-id=01f0ec5d-90d2-13fc-973c-05bbe2a76433) - Closing
[0m06:45:05.723506 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:05.753157 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:05.777521 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:05.779164 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:45:06.105670 [debug] [Thread-4 (]: SQL status: OK in 0.320 seconds
[0m06:45:06.109380 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2, command-id=01f0ec5d-9113-15f4-9da4-82d15f9f7480) - Closing
[0m06:45:06.114507 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:06.116138 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m06:45:06.374948 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m06:45:06.378265 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2, command-id=01f0ec5d-9147-1e7d-bc6e-cec8f9d23b0d) - Closing
[0m06:45:06.381798 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:06.397892 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:06.399159 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:45:06.708823 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m06:45:06.713184 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2, command-id=01f0ec5d-9172-18b1-b566-05f0e206e374) - Closing
[0m06:45:06.719236 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:45:06.721830 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m06:45:06.723869 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m06:45:07.140730 [debug] [Thread-4 (]: SQL status: OK in 0.420 seconds
[0m06:45:07.143122 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2, command-id=01f0ec5d-91a3-14a9-9293-9543651033a2) - Closing
[0m06:45:07.146851 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m06:45:07.148808 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-90c0-12cd-8de5-79316f5ccbd2) - Closing
[0m06:45:07.283499 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.35s]
[0m06:45:07.285341 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m06:45:07.286770 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:45:07.289138 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m06:45:07.291180 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m06:45:07.292589 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m06:45:07.293969 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:45:07.295339 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:45:07.308918 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:07.310719 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m06:45:07.312169 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m06:45:07.699294 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc) - Created
[0m06:45:08.053680 [debug] [Thread-4 (]: SQL status: OK in 0.740 seconds
[0m06:45:08.056917 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-923a-14bd-be11-e96457a22422) - Closing
[0m06:45:08.061095 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.083626 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.109537 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.111377 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m06:45:08.363070 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m06:45:08.369257 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-9279-1b97-b3ca-650c2a688d99) - Closing
[0m06:45:08.373492 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.376150 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m06:45:08.648748 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m06:45:08.662932 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-92a0-1652-b2b3-c89eba02482a) - Closing
[0m06:45:08.677336 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.680618 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m06:45:08.923963 [debug] [Thread-4 (]: SQL status: OK in 0.240 seconds
[0m06:45:08.930398 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-92ce-1ceb-87e3-11ee9bc6504c) - Closing
[0m06:45:08.937854 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.960356 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:08.961847 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m06:45:09.362695 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m06:45:09.372904 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-92f9-15d2-9a15-f26c00d28de4) - Closing
[0m06:45:09.383806 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m06:45:09.387649 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m06:45:09.389886 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m06:45:09.629252 [debug] [Thread-4 (]: SQL status: OK in 0.240 seconds
[0m06:45:09.635150 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc, command-id=01f0ec5d-933c-1e57-babb-c2da82720fb6) - Closing
[0m06:45:09.646132 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m06:45:09.649669 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5d-9227-1ada-9820-6a8c483087bc) - Closing
[0m06:45:09.791259 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.50s]
[0m06:45:09.793474 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m06:45:09.796400 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m06:45:09.797956 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:45:09.799636 [info ] [MainThread]: 
[0m06:45:09.801338 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 24.77 seconds (24.77s).
[0m06:45:09.805463 [debug] [MainThread]: Command end result
[0m06:45:10.020047 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m06:45:10.027206 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m06:45:10.043513 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m06:45:10.044900 [info ] [MainThread]: 
[0m06:45:10.046375 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:45:10.048061 [info ] [MainThread]: 
[0m06:45:10.049616 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m06:45:10.051280 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m06:45:10.053312 [info ] [MainThread]: 
[0m06:45:10.054765 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m06:45:10.056102 [info ] [MainThread]: 
[0m06:45:10.057514 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m06:45:10.059533 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 30.424093, "process_in_blocks": "0", "process_kernel_time": 1.116639, "process_mem_max_rss": "255684", "process_out_blocks": "712", "process_user_time": 10.381942}
[0m06:45:10.060949 [debug] [MainThread]: Command `dbt test` failed at 06:45:10.060818 after 30.43 seconds
[0m06:45:10.062431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72855dc163b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7284c3e1fc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7284c3e1ecb0>]}
[0m06:45:10.063800 [debug] [MainThread]: Flushing usage events
[0m06:45:10.438376 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:01:07.999136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a2079ea4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a20683c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a20683c250>]}
[0m07:01:07.998239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023faba410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023e8fc2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023e8fc250>]}


============================== 07:01:08.028097 | 33d5ca03-f76a-422f-b310-3bc7f9485951 ==============================
[0m07:01:08.028097 [info ] [MainThread]: Running with dbt=1.10.18


============================== 07:01:08.031236 | 63c89347-86af-482e-be62-2c905c31fb94 ==============================
[0m07:01:08.031236 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:01:08.031542 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'profiles_dir': '/opt/dagster/app/dbt', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'static_parser': 'True', 'debug': 'False', 'fail_fast': 'False', 'printer_width': '80', 'write_json': 'True', 'no_print': 'None', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'introspect': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_format': 'default', 'empty': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True'}
[0m07:01:08.034547 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'printer_width': '80', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'empty': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'introspect': 'True', 'target_path': 'None', 'write_json': 'True', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'debug': 'False', 'log_cache_events': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'use_experimental_parser': 'False', 'log_format': 'default'}
[0m07:01:08.006316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c55963e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c43b02b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c43b0250>]}


============================== 07:01:08.040517 | ab044aa2-132c-4b8b-979c-baa906164e41 ==============================
[0m07:01:08.040517 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:01:08.045085 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/opt/dagster/app/dbt', 'no_print': 'None', 'warn_error': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'use_colors': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'version_check': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'fail_fast': 'False', 'log_format': 'default', 'quiet': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'empty': 'None', 'write_json': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'introspect': 'True'}
[0m07:01:09.773616 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:01:09.773671 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:01:09.774032 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:01:09.775602 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:01:09.775515 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:01:09.776787 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:01:09.777203 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:01:09.777155 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:01:09.778486 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:01:12.147033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a209521000>]}
[0m07:01:12.147004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7802415e10f0>]}
[0m07:01:12.147025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c4558a60>]}
[0m07:01:12.374704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a2069bc100>]}
[0m07:01:12.381924 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:01:12.397049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259eed01c0>]}
[0m07:01:12.397857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78024141c8b0>]}
[0m07:01:12.401160 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:01:12.401243 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:01:12.805400 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:01:12.841206 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:01:12.847554 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:01:13.787250 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m07:01:13.787575 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m07:01:13.787676 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m07:01:13.789802 [debug] [MainThread]: Partial parsing: added file: banking_pipeline://macros/export_to_local_csv.sql
[0m07:01:13.789733 [debug] [MainThread]: Partial parsing: added file: banking_pipeline://macros/export_to_local_csv.sql
[0m07:01:13.791005 [debug] [MainThread]: Partial parsing: added file: banking_pipeline://macros/export_to_local_csv.sql
[0m07:01:13.791753 [debug] [MainThread]: Partial parsing: deleted file: banking_pipeline://macros/export_to_csv.sql
[0m07:01:13.791809 [debug] [MainThread]: Partial parsing: deleted file: banking_pipeline://macros/export_to_csv.sql
[0m07:01:13.792579 [debug] [MainThread]: Partial parsing: deleted file: banking_pipeline://macros/export_to_csv.sql
[0m07:01:14.074993 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:01:14.085656 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:01:14.100836 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:01:14.112699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a1e48f6500>]}
[0m07:01:14.123835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e4d62f0>]}
[0m07:01:14.132935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c9b2320>]}
[0m07:01:14.483545 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:01:14.485453 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:01:14.496747 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:01:14.497000 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:01:14.497856 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:01:14.510721 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:01:14.549619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e4f6800>]}
[0m07:01:14.550645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c9fccd0>]}
[0m07:01:14.552426 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:01:14.553520 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:01:14.554953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e4f4af0>]}
[0m07:01:14.555894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c9fc160>]}
[0m07:01:14.562617 [info ] [MainThread]: 
[0m07:01:14.562801 [info ] [MainThread]: 
[0m07:01:14.565434 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:01:14.566786 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:01:14.567075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a206ecf520>]}
[0m07:01:14.567959 [info ] [MainThread]: 
[0m07:01:14.569208 [info ] [MainThread]: 
[0m07:01:14.569685 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:01:14.571140 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:01:14.571841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a206ece380>]}
[0m07:01:14.572049 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:01:14.574570 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:01:14.574053 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:01:14.579946 [info ] [MainThread]: 
[0m07:01:14.582093 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:01:14.583954 [info ] [MainThread]: 
[0m07:01:14.586455 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:01:14.588264 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:01:14.597677 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:01:14.600179 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:01:14.603096 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:01:14.606323 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:01:14.620911 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:01:14.624299 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:01:14.640842 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:01:14.645862 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:01:14.647296 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:01:14.647639 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:14.649531 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:01:14.651723 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:14.664818 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:01:14.667372 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:01:14.669414 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:15.517058 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f4-14b2-9da9-9d3926b9511c) - Created
[0m07:01:15.538483 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f7-1e70-b3b8-cd057e27efb0) - Created
[0m07:01:15.543010 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f9-10c3-8657-99d6a0523980) - Created
[0m07:01:34.158137 [debug] [ThreadPool]: SQL status: OK in 19.510 seconds
[0m07:01:34.158198 [debug] [ThreadPool]: SQL status: OK in 19.510 seconds
[0m07:01:34.181604 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-d2f4-14b2-9da9-9d3926b9511c, command-id=01f0ec5f-d31a-18df-861c-e74f62cc5e9f) - Closing
[0m07:01:34.182531 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-d2f7-1e70-b3b8-cd057e27efb0, command-id=01f0ec5f-d31f-127a-9bfd-eb3eca4049e5) - Closing
[0m07:01:34.552521 [debug] [ThreadPool]: On list_workspace: Close
[0m07:01:34.558629 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f7-1e70-b3b8-cd057e27efb0) - Closing
[0m07:01:34.561299 [debug] [ThreadPool]: On list_workspace: Close
[0m07:01:34.568165 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f4-14b2-9da9-9d3926b9511c) - Closing
[0m07:01:34.710160 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:01:34.713608 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:01:34.717397 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:01:34.718003 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:01:34.729714 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:01:34.734860 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:01:34.739894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:34.755164 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:01:34.757033 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:01:34.758730 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:35.192205 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-debd-1445-a349-9c8650055ed8) - Created
[0m07:01:35.217258 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-debf-1a22-9945-1724172f76be) - Created
[0m07:01:36.305256 [debug] [ThreadPool]: SQL status: OK in 21.640 seconds
[0m07:01:36.326028 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-d2f9-10c3-8657-99d6a0523980, command-id=01f0ec5f-d31f-10cf-abf3-3060f18affd6) - Closing
[0m07:01:36.526049 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:01:36.529246 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-d2f9-10c3-8657-99d6a0523980) - Closing
[0m07:01:36.656028 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:01:36.658748 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:01:36.671204 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:01:36.673498 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:01:36.675659 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:37.106915 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-dfe3-1248-b8b6-17da2306922b) - Created
[0m07:01:37.565876 [debug] [ThreadPool]: SQL status: OK in 2.830 seconds
[0m07:01:37.572621 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-debf-1a22-9945-1724172f76be, command-id=01f0ec5f-ded6-1dcb-8d06-eab4857ee69e) - Closing
[0m07:01:37.575623 [debug] [ThreadPool]: On list_workspace: Close
[0m07:01:37.578079 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-debf-1a22-9945-1724172f76be) - Closing
[0m07:01:37.723877 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:01:37.727288 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:01:37.737810 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:01:37.741650 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:01:37.744421 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:38.138711 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e082-1121-b635-29e87c7f7190) - Created
[0m07:01:38.184131 [debug] [ThreadPool]: SQL status: OK in 3.430 seconds
[0m07:01:38.193679 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-debd-1445-a349-9c8650055ed8, command-id=01f0ec5f-ded1-16f4-a5c1-512bc5a153c0) - Closing
[0m07:01:38.196404 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:01:38.199004 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-debd-1445-a349-9c8650055ed8) - Closing
[0m07:01:38.346136 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:01:38.352968 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:01:38.371100 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:01:38.374271 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:01:38.376483 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:38.745017 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e0de-1da8-94db-f55e8f50d49e) - Created
[0m07:01:38.827415 [debug] [ThreadPool]: SQL status: OK in 2.150 seconds
[0m07:01:38.831249 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-dfe3-1248-b8b6-17da2306922b, command-id=01f0ec5f-dff4-10b4-a41c-0208eb1e3e81) - Closing
[0m07:01:38.833057 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:01:38.834649 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-dfe3-1248-b8b6-17da2306922b) - Closing
[0m07:01:38.965823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:01:38.967844 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:01:38.975064 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:01:38.976770 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:01:38.978206 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:39.349563 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e13a-18cd-b47c-44bad9f8c423) - Created
[0m07:01:40.097426 [debug] [ThreadPool]: SQL status: OK in 2.350 seconds
[0m07:01:40.101750 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e082-1121-b635-29e87c7f7190, command-id=01f0ec5f-e092-1acd-bd01-16f06a1d58d2) - Closing
[0m07:01:40.103721 [debug] [ThreadPool]: On list_workspace: Close
[0m07:01:40.105786 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e082-1121-b635-29e87c7f7190) - Closing
[0m07:01:40.245961 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:01:40.247757 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:01:40.270319 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:01:40.271946 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:01:40.274186 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:40.656087 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e201-1817-899d-8c4cf1a7a087) - Created
[0m07:01:40.855333 [debug] [ThreadPool]: SQL status: OK in 2.480 seconds
[0m07:01:40.867587 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e0de-1da8-94db-f55e8f50d49e, command-id=01f0ec5f-e0ef-14c2-92ff-5c684c9863a8) - Closing
[0m07:01:40.872575 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:01:40.876907 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e0de-1da8-94db-f55e8f50d49e) - Closing
[0m07:01:40.998633 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:01:41.004035 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:01:41.033996 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:01:41.038470 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:01:41.043181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:41.497460 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e27f-15e3-9257-a1dae6c6277a) - Created
[0m07:01:41.690312 [debug] [ThreadPool]: SQL status: OK in 2.710 seconds
[0m07:01:41.699261 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e13a-18cd-b47c-44bad9f8c423, command-id=01f0ec5f-e14a-1d8e-b4c1-f96c5fff79e0) - Closing
[0m07:01:41.703132 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:01:41.706880 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e13a-18cd-b47c-44bad9f8c423) - Closing
[0m07:01:41.856785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '33d5ca03-f76a-422f-b310-3bc7f9485951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a20957d210>]}
[0m07:01:41.893445 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:01:41.896260 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m07:01:41.899242 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m07:01:41.901599 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m07:01:41.904157 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:01:41.939962 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:01:41.953326 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:01:41.974664 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:01:41.988009 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:01:41.989552 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m07:01:41.991096 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:42.171493 [debug] [ThreadPool]: SQL status: OK in 1.900 seconds
[0m07:01:42.176323 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e201-1817-899d-8c4cf1a7a087, command-id=01f0ec5f-e212-1c7c-a6a9-33ae80cb86cc) - Closing
[0m07:01:42.178441 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:01:42.180001 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e201-1817-899d-8c4cf1a7a087) - Closing
[0m07:01:42.312993 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:01:42.315023 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:01:42.321244 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:01:42.323035 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:01:42.324923 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:42.351030 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e304-18fe-bc9b-5fad4b9e81e4) - Created
[0m07:01:42.584933 [debug] [ThreadPool]: SQL status: OK in 1.540 seconds
[0m07:01:42.588777 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e27f-15e3-9257-a1dae6c6277a, command-id=01f0ec5f-e292-1e5f-ab40-ef9a7435ceb5) - Closing
[0m07:01:42.591235 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:01:42.593031 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e27f-15e3-9257-a1dae6c6277a) - Closing
[0m07:01:42.700922 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e338-10b6-b36a-7e584e72e078) - Created
[0m07:01:42.756886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c61a1660>]}
[0m07:01:42.762644 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m07:01:42.764547 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m07:01:42.766192 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m07:01:42.767401 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m07:01:42.768621 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m07:01:42.769917 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m07:01:42.781324 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:01:42.783082 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e425a80>]}
[0m07:01:42.834010 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:01:42.835801 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e425c90>]}
[0m07:01:42.863201 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:01:42.864938 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:01:42.866434 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:01:43.295883 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5f-e394-1873-a38b-5280413b4309) - Created
[0m07:01:43.925205 [debug] [ThreadPool]: SQL status: OK in 1.600 seconds
[0m07:01:43.930895 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e338-10b6-b36a-7e584e72e078, command-id=01f0ec5f-e34a-1fdf-ba6e-8fee08df9aed) - Closing
[0m07:01:43.933342 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:01:43.935143 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e338-10b6-b36a-7e584e72e078) - Closing
[0m07:01:44.061536 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:01:44.064577 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:01:44.071752 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:01:44.074542 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:01:44.077053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:01:44.443923 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e444-181b-b41c-96c216f03293) - Created
[0m07:01:46.235930 [debug] [ThreadPool]: SQL status: OK in 2.160 seconds
[0m07:01:46.245542 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec5f-e444-181b-b41c-96c216f03293, command-id=01f0ec5f-e455-170a-967e-e217666ed96a) - Closing
[0m07:01:46.248560 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:01:46.250667 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec5f-e444-181b-b41c-96c216f03293) - Closing
[0m07:01:46.377190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7802413f2800>]}
[0m07:01:46.384789 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m07:01:46.387448 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m07:01:46.395153 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m07:01:46.397668 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m07:01:46.399944 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m07:01:46.416867 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m07:01:46.431938 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m07:01:46.453675 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:01:46.457465 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:01:46.459595 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c9cbd60>]}
[0m07:01:46.476241 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m07:01:46.486014 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m07:01:46.500546 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m07:01:46.501823 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m07:01:46.502870 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:01:46.896655 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e5ba-1aa6-80f0-69a2a5d6b020) - Created
[0m07:01:48.900204 [debug] [Thread-4 (]: SQL status: OK in 6.910 seconds
[0m07:01:48.906664 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e304-18fe-bc9b-5fad4b9e81e4, command-id=01f0ec5f-e314-1a72-9190-dd7709907c41) - Closing
[0m07:01:49.147646 [debug] [Thread-7 (]: SQL status: OK in 2.640 seconds
[0m07:01:49.152259 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e5ba-1aa6-80f0-69a2a5d6b020, command-id=01f0ec5f-e5ca-1bbf-ac08-816ea202362f) - Closing
[0m07:01:49.171303 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m07:01:49.174407 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e304-18fe-bc9b-5fad4b9e81e4) - Closing
[0m07:01:49.182515 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:01:49.186832 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m07:01:49.188563 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e5ba-1aa6-80f0-69a2a5d6b020) - Closing
[0m07:01:49.301860 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 7.40s]
[0m07:01:49.309473 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:01:49.314522 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:01:49.318340 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m07:01:49.322251 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m07:01:49.323728 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78024180d870>]}
[0m07:01:49.325412 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m07:01:49.328219 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:01:49.328477 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.93s]
[0m07:01:49.331084 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m07:01:49.333257 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m07:01:49.335740 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m07:01:49.337916 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m07:01:49.339748 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m07:01:49.341758 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m07:01:49.343066 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:01:49.349279 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m07:01:49.356564 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:01:49.358846 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m07:01:49.361818 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:01:49.363255 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:01:49.365407 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m07:01:49.367224 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m07:01:49.373527 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:01:49.375515 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m07:01:49.375871 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m07:01:49.376963 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:49.377482 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m07:01:49.378972 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:01:49.784405 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e770-133b-83a9-2907cdb1ac90) - Created
[0m07:01:49.790047 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e771-1bb8-b555-d340737f0cba) - Created
[0m07:01:52.021582 [debug] [Thread-4 (]: SQL status: OK in 2.640 seconds
[0m07:01:52.031751 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e771-1bb8-b555-d340737f0cba, command-id=01f0ec5f-e787-13ea-bd4f-4cf4dad03311) - Closing
[0m07:01:52.035581 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m07:01:52.038222 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e771-1bb8-b555-d340737f0cba) - Closing
[0m07:01:52.191475 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 2.87s]
[0m07:01:52.209111 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:01:52.219172 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:01:52.227306 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m07:01:52.233331 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m07:01:52.238325 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m07:01:52.243730 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:01:52.255476 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:01:52.269543 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:01:52.273639 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:01:52.288402 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:01:52.289587 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m07:01:52.290727 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:52.496117 [debug] [Thread-7 (]: SQL status: OK in 3.120 seconds
[0m07:01:52.498262 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e770-133b-83a9-2907cdb1ac90, command-id=01f0ec5f-e785-1ca2-a0d8-2f82c1aa4ec8) - Closing
[0m07:01:52.499836 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:01:52.501829 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m07:01:52.503033 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e770-133b-83a9-2907cdb1ac90) - Closing
[0m07:01:52.572634 [debug] [Thread-5 (]: SQL status: OK in 9.710 seconds
[0m07:01:52.579163 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e394-1873-a38b-5280413b4309, command-id=01f0ec5f-e3a4-1d49-86e4-a589708d8965) - Closing
[0m07:01:52.631078 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c90d0f0>]}
[0m07:01:52.633045 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 3.29s]
[0m07:01:52.634519 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m07:01:52.636448 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m07:01:52.638137 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m07:01:52.640117 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m07:01:52.642100 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m07:01:52.644144 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m07:01:52.650260 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m07:01:52.665873 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m07:01:52.677104 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:01:52.679734 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m07:01:52.681597 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m07:01:52.695879 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m07:01:52.697899 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e92f-11d0-8300-c00e6ddb9a7e) - Created
[0m07:01:52.697981 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m07:01:52.699611 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:01:52.825306 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:01:52.827632 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m07:01:53.083473 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e969-1f0f-8bfa-acd67f8f5007) - Created
[0m07:01:54.219386 [debug] [Thread-4 (]: SQL status: OK in 1.930 seconds
[0m07:01:54.229161 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e92f-11d0-8300-c00e6ddb9a7e, command-id=01f0ec5f-e93f-1d16-a567-32813ae7cf01) - Closing
[0m07:01:54.234171 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m07:01:54.237646 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-e92f-11d0-8300-c00e6ddb9a7e) - Closing
[0m07:01:54.378252 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 2.14s]
[0m07:01:54.382703 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:01:54.385247 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:01:54.387770 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m07:01:54.390192 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m07:01:54.392486 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m07:01:54.395100 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:01:54.404762 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:01:54.419515 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:01:54.429565 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:01:54.441856 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:01:54.444077 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m07:01:54.445769 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:54.703268 [debug] [Thread-7 (]: SQL status: OK in 2.000 seconds
[0m07:01:54.706235 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e969-1f0f-8bfa-acd67f8f5007, command-id=01f0ec5f-e97a-1605-8533-3af0318a1bc6) - Closing
[0m07:01:54.708783 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:01:54.711941 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m07:01:54.713981 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-e969-1f0f-8bfa-acd67f8f5007) - Closing
[0m07:01:54.812030 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ea71-173c-9235-9509d3e20f84) - Created
[0m07:01:54.851181 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780214702c20>]}
[0m07:01:54.853518 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.21s]
[0m07:01:54.855654 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m07:01:54.858351 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m07:01:54.860805 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m07:01:54.862992 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m07:01:54.864519 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m07:01:54.866039 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m07:01:54.874241 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m07:01:54.887069 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m07:01:54.908697 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m07:01:54.922370 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:01:54.924271 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78021c90cd00>]}
[0m07:01:54.959093 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m07:01:54.971896 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m07:01:54.973068 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m07:01:54.974049 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:01:55.375777 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-eac7-12da-89b4-b97f337fc2f5) - Created
[0m07:01:55.683064 [debug] [Thread-4 (]: SQL status: OK in 1.240 seconds
[0m07:01:55.690518 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-ea71-173c-9235-9509d3e20f84, command-id=01f0ec5f-ea82-1343-b0d5-04d287f39655) - Closing
[0m07:01:55.695413 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m07:01:55.698900 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ea71-173c-9235-9509d3e20f84) - Closing
[0m07:01:55.839512 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.45s]
[0m07:01:55.846829 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:01:55.852336 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:01:55.856814 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m07:01:55.863050 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m07:01:55.867768 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m07:01:55.872089 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:01:55.888259 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:01:55.904364 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:01:55.909292 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:01:55.922630 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:01:55.924019 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m07:01:55.925153 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:56.318994 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-eb56-1c48-ae21-3405f50d7bae) - Created
[0m07:01:58.229385 [debug] [Thread-4 (]: SQL status: OK in 2.300 seconds
[0m07:01:58.232858 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-eb56-1c48-ae21-3405f50d7bae, command-id=01f0ec5f-eb67-1f84-bb7f-25bf1e8ef5db) - Closing
[0m07:01:58.234669 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m07:01:58.236019 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-eb56-1c48-ae21-3405f50d7bae) - Closing
[0m07:01:58.392225 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.53s]
[0m07:01:58.394827 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:01:58.396429 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:01:58.398007 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m07:01:58.399630 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m07:01:58.400935 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m07:01:58.402181 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:01:58.410963 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:01:58.422767 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:01:58.428959 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:01:58.443178 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:01:58.445395 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m07:01:58.446968 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:01:59.261934 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ed16-1845-84a6-2ed202d978ef) - Created
[0m07:02:00.176845 [debug] [Thread-5 (]: SQL status: OK in 7.350 seconds
[0m07:02:00.178895 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5f-e394-1873-a38b-5280413b4309, command-id=01f0ec5f-e953-12e3-8785-7e0e480a9d0c) - Closing
[0m07:02:00.588131 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m07:02:00.640403 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m07:02:00.642527 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5f-e394-1873-a38b-5280413b4309) - Closing
[0m07:02:00.771731 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e414af0>]}
[0m07:02:00.773134 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 18.00s]
[0m07:02:00.774396 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m07:02:00.775380 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m07:02:00.776656 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m07:02:00.778813 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m07:02:00.779901 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m07:02:00.780945 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m07:02:00.781971 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m07:02:00.798011 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:02:00.799214 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:02:00.800222 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:02:00.886104 [debug] [Thread-4 (]: SQL status: OK in 2.440 seconds
[0m07:02:00.904425 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-ed16-1845-84a6-2ed202d978ef, command-id=01f0ec5f-ed2a-103b-b042-8bd5ab65954d) - Closing
[0m07:02:00.909464 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m07:02:00.913883 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ed16-1845-84a6-2ed202d978ef) - Closing
[0m07:02:01.055431 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 2.65s]
[0m07:02:01.066254 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:02:01.070701 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:02:01.075130 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m07:02:01.082207 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m07:02:01.084825 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m07:02:01.087393 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:02:01.104087 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:02:01.119598 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:02:01.124490 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:02:01.139971 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:02:01.141242 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m07:02:01.142457 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:01.196921 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5f-ee3e-1780-954d-6eb300164ca7) - Created
[0m07:02:01.554268 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ee74-19fc-be22-2fe5407b2445) - Created
[0m07:02:03.217530 [debug] [Thread-4 (]: SQL status: OK in 2.070 seconds
[0m07:02:03.221992 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-ee74-19fc-be22-2fe5407b2445, command-id=01f0ec5f-ee87-14b5-ba19-70f1441dc94b) - Closing
[0m07:02:03.224339 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m07:02:03.226108 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-ee74-19fc-be22-2fe5407b2445) - Closing
[0m07:02:03.359534 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 2.28s]
[0m07:02:03.362297 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:02:03.364269 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:02:03.365882 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m07:02:03.367787 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m07:02:03.369270 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m07:02:03.370807 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:02:03.382496 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:02:03.397714 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:02:03.402661 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:02:03.416277 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:02:03.417818 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:02:03.419031 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:03.830477 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-efd2-15b1-bf83-8d38880e45a9) - Created
[0m07:02:04.056971 [debug] [Thread-7 (]: SQL status: OK in 9.080 seconds
[0m07:02:04.061462 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec5f-eac7-12da-89b4-b97f337fc2f5, command-id=01f0ec5f-ead9-1da8-ab83-457054c4b1c0) - Closing
[0m07:02:04.272342 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:02:04.292505 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m07:02:04.294338 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec5f-eac7-12da-89b4-b97f337fc2f5) - Closing
[0m07:02:04.454074 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63c89347-86af-482e-be62-2c905c31fb94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023ef8c940>]}
[0m07:02:04.456011 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 9.59s]
[0m07:02:04.457669 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m07:02:04.460517 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:02:04.462536 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:02:04.463140 [debug] [Thread-5 (]: SQL status: OK in 3.660 seconds
[0m07:02:04.464565 [info ] [MainThread]: 
[0m07:02:04.465410 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5f-ee3e-1780-954d-6eb300164ca7, command-id=01f0ec5f-ee50-117f-b57c-dfa5027199ff) - Closing
[0m07:02:04.465886 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 49.89 seconds (49.89s).
[0m07:02:04.468020 [debug] [MainThread]: Command end result
[0m07:02:04.469087 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:02:04.470635 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m07:02:04.567216 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:02:04.576336 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:02:04.593266 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:02:04.594979 [info ] [MainThread]: 
[0m07:02:04.597174 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:02:04.598893 [info ] [MainThread]: 
[0m07:02:04.600543 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m07:02:04.605472 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 56.920033, "process_in_blocks": "74144", "process_kernel_time": 1.297423, "process_mem_max_rss": "246740", "process_out_blocks": "648", "process_user_time": 10.33493}
[0m07:02:04.607129 [debug] [MainThread]: Command `dbt run` succeeded at 07:02:04.606980 after 56.92 seconds
[0m07:02:04.610332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023faba410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023eb20d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78023fa227a0>]}
[0m07:02:04.614594 [debug] [MainThread]: Flushing usage events
[0m07:02:04.870112 [debug] [Thread-4 (]: SQL status: OK in 1.450 seconds
[0m07:02:04.874336 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-efd2-15b1-bf83-8d38880e45a9, command-id=01f0ec5f-efe2-11f9-8b21-fffdaa95111a) - Closing
[0m07:02:04.876647 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m07:02:04.878813 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-efd2-15b1-bf83-8d38880e45a9) - Closing
[0m07:02:05.009501 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.64s]
[0m07:02:05.012223 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:02:05.014627 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:02:05.016080 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m07:02:05.017948 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m07:02:05.019480 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m07:02:05.020756 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:02:05.031703 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:02:05.045138 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:02:05.050417 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:02:05.062607 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:02:05.064555 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:02:05.066042 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:05.171817 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:02:05.439858 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f0c6-1d90-9313-0928945449fe) - Created
[0m07:02:07.007092 [debug] [Thread-4 (]: SQL status: OK in 1.940 seconds
[0m07:02:07.010693 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f0c6-1d90-9313-0928945449fe, command-id=01f0ec5f-f0d7-1923-9501-47237bd1691e) - Closing
[0m07:02:07.013209 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m07:02:07.015011 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f0c6-1d90-9313-0928945449fe) - Closing
[0m07:02:07.149604 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.13s]
[0m07:02:07.151958 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:02:07.153845 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:02:07.155513 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m07:02:07.157389 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m07:02:07.159043 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m07:02:07.160731 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:02:07.162733 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:02:07.225309 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:07.227180 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:02:07.228791 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:07.600108 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2) - Created
[0m07:02:08.020033 [debug] [Thread-5 (]: SQL status: OK in 3.550 seconds
[0m07:02:08.022246 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec5f-ee3e-1780-954d-6eb300164ca7, command-id=01f0ec5f-f043-14aa-8d04-15acd2250336) - Closing
[0m07:02:08.025128 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m07:02:08.038390 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m07:02:08.039596 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec5f-ee3e-1780-954d-6eb300164ca7) - Closing
[0m07:02:08.171892 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab044aa2-132c-4b8b-979c-baa906164e41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78259e52d270>]}
[0m07:02:08.174817 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 7.39s]
[0m07:02:08.177211 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m07:02:08.181129 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:02:08.182555 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:02:08.184251 [info ] [MainThread]: 
[0m07:02:08.185871 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 53.61 seconds (53.61s).
[0m07:02:08.188293 [debug] [MainThread]: Command end result
[0m07:02:08.282764 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:02:08.291085 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:02:08.304259 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:02:08.305802 [info ] [MainThread]: 
[0m07:02:08.307394 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:02:08.308755 [info ] [MainThread]: 
[0m07:02:08.310059 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m07:02:08.311973 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 60.631416, "process_in_blocks": "68984", "process_kernel_time": 1.299467, "process_mem_max_rss": "244276", "process_out_blocks": "640", "process_user_time": 10.269063}
[0m07:02:08.313329 [debug] [MainThread]: Command `dbt seed` succeeded at 07:02:08.313202 after 60.63 seconds
[0m07:02:08.314510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c55963e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c6ed9ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825c45584f0>]}
[0m07:02:08.315734 [debug] [MainThread]: Flushing usage events
[0m07:02:08.530437 [debug] [Thread-4 (]: SQL status: OK in 1.300 seconds
[0m07:02:08.534850 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2, command-id=01f0ec5f-f222-147d-bc29-a3961ada503e) - Closing
[0m07:02:08.566582 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:08.593388 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:08.653242 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:08.655243 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:02:08.709552 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:02:09.195581 [debug] [Thread-4 (]: SQL status: OK in 0.540 seconds
[0m07:02:09.198409 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2, command-id=01f0ec5f-f2c4-1bb2-a432-927d76d8e22b) - Closing
[0m07:02:09.202786 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:09.204666 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m07:02:09.469486 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m07:02:09.472774 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2, command-id=01f0ec5f-f316-1a6a-8fbe-83449bf52ec3) - Closing
[0m07:02:09.488133 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:09.507759 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:09.510162 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:02:10.099562 [debug] [Thread-4 (]: SQL status: OK in 0.590 seconds
[0m07:02:10.105989 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2, command-id=01f0ec5f-f346-1199-924b-6a1a36515838) - Closing
[0m07:02:10.117193 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:02:10.125264 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:02:10.127450 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:02:10.839071 [debug] [Thread-4 (]: SQL status: OK in 0.710 seconds
[0m07:02:10.843588 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2, command-id=01f0ec5f-f3a3-1911-a15b-913411526943) - Closing
[0m07:02:10.855131 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m07:02:10.857839 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f212-10d8-b528-bcf0a91a93b2) - Closing
[0m07:02:11.001051 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 3.84s]
[0m07:02:11.008984 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:02:11.016181 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:02:11.021850 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m07:02:11.029204 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m07:02:11.033109 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m07:02:11.036004 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:02:11.038461 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:02:11.055607 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:11.057661 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:02:11.059701 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:11.543382 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70) - Created
[0m07:02:11.963906 [debug] [Thread-4 (]: SQL status: OK in 0.900 seconds
[0m07:02:11.966980 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70, command-id=01f0ec5f-f47c-1173-88a8-61f8eba34cc3) - Closing
[0m07:02:11.972699 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:11.998382 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:12.020369 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:12.022024 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:02:12.380866 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m07:02:12.384792 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70, command-id=01f0ec5f-f4c4-1fee-b60e-5fdf2cc66d0a) - Closing
[0m07:02:12.390811 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:12.393375 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m07:02:12.652802 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m07:02:12.665541 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70, command-id=01f0ec5f-f4fd-19fb-a174-67d32ad1ca7f) - Closing
[0m07:02:12.676039 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:12.700512 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:12.702133 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:02:13.095972 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m07:02:13.100279 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70, command-id=01f0ec5f-f52d-147a-bae0-dbea3ee4868b) - Closing
[0m07:02:13.105206 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:02:13.107030 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:02:13.108983 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:02:13.383773 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m07:02:13.385835 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70, command-id=01f0ec5f-f56b-1e83-b290-16cdf4defb5b) - Closing
[0m07:02:13.388989 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m07:02:13.390438 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f469-1123-a4f4-38efe2c5da70) - Closing
[0m07:02:13.522523 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.49s]
[0m07:02:13.526257 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:02:13.528764 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:02:13.530751 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m07:02:13.533103 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m07:02:13.534851 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m07:02:13.536559 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:02:13.538210 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:02:13.552039 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:13.553875 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:02:13.555361 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:02:13.923917 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3) - Created
[0m07:02:14.500844 [debug] [Thread-4 (]: SQL status: OK in 0.950 seconds
[0m07:02:14.504496 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f5e6-19d5-8c9c-0f56a90dd7c6) - Closing
[0m07:02:14.513294 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:14.543647 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:14.573332 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:14.575819 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m07:02:14.878142 [debug] [Thread-4 (]: SQL status: OK in 0.300 seconds
[0m07:02:14.888050 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f649-1df4-9c6f-c9120198959b) - Closing
[0m07:02:14.893896 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:14.896014 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:02:15.328200 [debug] [Thread-4 (]: SQL status: OK in 0.430 seconds
[0m07:02:15.335668 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f67c-10e9-a33f-733a98483fbd) - Closing
[0m07:02:15.348724 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:15.353149 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m07:02:15.654526 [debug] [Thread-4 (]: SQL status: OK in 0.300 seconds
[0m07:02:15.662759 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f6c1-1373-a683-a590ab62c0c1) - Closing
[0m07:02:15.667074 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:15.690983 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:15.693274 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:02:16.265795 [debug] [Thread-4 (]: SQL status: OK in 0.570 seconds
[0m07:02:16.269230 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f6f5-1204-a1aa-caf76a2eddc2) - Closing
[0m07:02:16.273771 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m07:02:16.276467 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:02:16.278430 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m07:02:16.551176 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m07:02:16.553378 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3, command-id=01f0ec5f-f74d-14e9-a846-4b2e78ed9350) - Closing
[0m07:02:16.557576 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m07:02:16.559551 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec5f-f5d6-10b0-8207-6732085eaff3) - Closing
[0m07:02:16.744307 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 3.21s]
[0m07:02:16.746811 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:02:16.750351 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:02:16.751822 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:02:16.753466 [info ] [MainThread]: 
[0m07:02:16.754875 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 1 minutes and 2.17 seconds (62.17s).
[0m07:02:16.759736 [debug] [MainThread]: Command end result
[0m07:02:16.973744 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:02:16.983651 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:02:17.001572 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:02:17.003092 [info ] [MainThread]: 
[0m07:02:17.004487 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m07:02:17.005759 [info ] [MainThread]: 
[0m07:02:17.007100 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m07:02:17.008797 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m07:02:17.010617 [info ] [MainThread]: 
[0m07:02:17.011959 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m07:02:17.013225 [info ] [MainThread]: 
[0m07:02:17.014374 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m07:02:17.016184 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 69.33623, "process_in_blocks": "72656", "process_kernel_time": 1.342068, "process_mem_max_rss": "252288", "process_out_blocks": "632", "process_user_time": 11.487129}
[0m07:02:17.017553 [debug] [MainThread]: Command `dbt test` failed at 07:02:17.017435 after 69.34 seconds
[0m07:02:17.018756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a2079ea4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a2097484c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a2096dcbb0>]}
[0m07:02:17.019961 [debug] [MainThread]: Flushing usage events
[0m07:02:17.397985 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:12:44.157955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9c3e22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9b3c82b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9b3c8250>]}


============================== 07:12:44.198298 | 63cbfb42-5639-4073-ae95-b92fd240a740 ==============================
[0m07:12:44.198298 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:12:44.204840 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'cache_selected_only': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'debug': 'False', 'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'static_parser': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'log_cache_events': 'False', 'log_format': 'default', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'quiet': 'False', 'introspect': 'True', 'fail_fast': 'False', 'target_path': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'no_print': 'None', 'use_colors': 'True'}
[0m07:12:44.186131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1b28a440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a0cc2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a0cc250>]}


============================== 07:12:44.209523 | ba3bdfcc-1977-4e70-ba0f-6568e3d576f9 ==============================
[0m07:12:44.209523 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:12:44.214209 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'version_check': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'log_format': 'default', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'introspect': 'True', 'quiet': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/opt/dagster/app/dbt', 'printer_width': '80', 'target_path': 'None'}
[0m07:12:44.248296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1ce762f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1bc282b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1bc28250>]}


============================== 07:12:44.268100 | 6638bc84-f761-4d54-97c8-2e57270af312 ==============================
[0m07:12:44.268100 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:12:44.271263 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/dagster/app/dbt/logs', 'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'profiles_dir': '/opt/dagster/app/dbt', 'log_cache_events': 'False', 'use_colors': 'True', 'static_parser': 'True', 'target_path': 'None', 'write_json': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'fail_fast': 'False', 'warn_error': 'None', 'log_format': 'default', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'no_print': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'version_check': 'True'}
[0m07:12:44.579481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e07aa02200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e0797e4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e0797e41f0>]}


============================== 07:12:44.588774 | 6cf08a32-4c27-40fb-8c30-2a3d5df46bfc ==============================
[0m07:12:44.588774 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:12:44.591852 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'profiles_dir': '/opt/dagster/app/dbt', 'target_path': 'None', 'empty': 'None', 'introspect': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'fail_fast': 'False', 'quiet': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'write_json': 'True', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name": "account_interest_summary", "local_folder": "/opt/dagster/app/target", "filename": "account_summary.csv"} --profiles-dir /opt/dagster/app/dbt --target dev', 'log_path': '/opt/dagster/app/dbt/logs', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'static_parser': 'True', 'log_cache_events': 'False'}
[0m07:12:46.439815 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:12:46.441882 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:12:46.444068 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:12:46.446898 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:12:46.449276 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:12:46.452176 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:12:46.486875 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:12:46.490424 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:12:46.492988 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:12:46.880521 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:12:46.884353 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:12:46.887396 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:12:48.807840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9e099030>]}
[0m07:12:48.820155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a0cc490>]}
[0m07:12:48.893150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1bc285b0>]}
[0m07:12:49.034383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9934ace0>]}
[0m07:12:49.036989 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:12:49.038585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a9716f0>]}
[0m07:12:49.041007 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:12:49.053627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6cf08a32-4c27-40fb-8c30-2a3d5df46bfc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e0797e4280>]}
[0m07:12:49.125600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af6724310>]}
[0m07:12:49.128231 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:12:49.289513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6cf08a32-4c27-40fb-8c30-2a3d5df46bfc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e07979a260>]}
[0m07:12:49.291899 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:12:49.379922 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:12:49.409942 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:12:49.423258 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:12:49.594384 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:12:50.163482 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:12:50.169044 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:12:50.182212 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:12:50.185950 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:12:50.187724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:12:50.190281 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:12:50.226748 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:12:50.236310 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:12:50.242762 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:12:50.468389 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:12:50.469083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1c370b50>]}
[0m07:12:50.470543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9ba18bb0>]}
[0m07:12:50.470751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a740b20>]}
[0m07:12:50.470602 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:12:50.493493 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:12:50.763235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6cf08a32-4c27-40fb-8c30-2a3d5df46bfc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e079eebe50>]}
[0m07:12:51.027320 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:12:51.037077 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:12:51.037011 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:12:51.044666 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:12:51.046330 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:12:51.058264 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:12:51.080111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9ba79060>]}
[0m07:12:51.082387 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:12:51.085423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9ba78910>]}
[0m07:12:51.090027 [info ] [MainThread]: 
[0m07:12:51.092073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:12:51.093975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a7dd690>]}
[0m07:12:51.094046 [info ] [MainThread]: 
[0m07:12:51.096431 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:12:51.096743 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:12:51.098505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a7dd450>]}
[0m07:12:51.099069 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:12:51.103703 [info ] [MainThread]: 
[0m07:12:51.105675 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:12:51.107946 [info ] [MainThread]: 
[0m07:12:51.110944 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:12:51.113146 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:12:51.116926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af5d13520>]}
[0m07:12:51.119027 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:12:51.119780 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:12:51.120978 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:12:51.122116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af5d13880>]}
[0m07:12:51.127684 [info ] [MainThread]: 
[0m07:12:51.129924 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:12:51.132275 [info ] [MainThread]: 
[0m07:12:51.133765 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:12:51.135855 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:12:51.136452 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:12:51.138391 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:12:51.152715 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:12:51.154856 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:12:51.156979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:12:51.157659 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:12:51.159545 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:12:51.164412 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:12:51.166766 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:12:51.170154 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:12:51.198304 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:12:51.201048 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:12:51.203095 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:12:51.214642 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:12:51.226702 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:12:51.252539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6cf08a32-4c27-40fb-8c30-2a3d5df46bfc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e079ed3790>]}
[0m07:12:51.254982 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:12:51.257235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6cf08a32-4c27-40fb-8c30-2a3d5df46bfc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e079ed2e00>]}
[0m07:12:51.260005 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m07:12:51.261931 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m07:12:51.285623 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m07:12:51.288234 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
    select * from `workspace`.`marts`.`account_interest_summary`

  
[0m07:12:51.290360 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:12:52.081122 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7221-1f3e-b769-941446ebd261) - Created
[0m07:12:52.082899 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-721d-100b-8174-755f0997820e) - Created
[0m07:12:52.088076 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7219-1305-8ec3-c1759ca147ec) - Created
[0m07:12:52.087225 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ec61-7222-125f-a19a-79f4cd31a8e3) - Created
[0m07:13:10.212786 [debug] [ThreadPool]: SQL status: OK in 19.060 seconds
[0m07:13:10.213128 [debug] [ThreadPool]: SQL status: OK in 19.040 seconds
[0m07:13:10.241435 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7221-1f3e-b769-941446ebd261, command-id=01f0ec61-724c-1260-845b-1dcf4b6e9291) - Closing
[0m07:13:10.242575 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-721d-100b-8174-755f0997820e, command-id=01f0ec61-724c-1829-b8b5-b77456c91c49) - Closing
[0m07:13:10.811233 [debug] [ThreadPool]: On list_workspace: Close
[0m07:13:10.811395 [debug] [ThreadPool]: On list_workspace: Close
[0m07:13:10.816071 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-721d-100b-8174-755f0997820e) - Closing
[0m07:13:10.816059 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7221-1f3e-b769-941446ebd261) - Closing
[0m07:13:10.962961 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:13:10.966422 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:13:10.967290 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:13:10.970696 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:13:10.976921 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:13:10.980267 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:13:10.983506 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:10.996540 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:13:11.000654 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:13:11.004662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:11.532120 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7dcc-1975-9653-836bf4ea62bd) - Created
[0m07:13:11.533126 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7dcd-1138-90cb-0994fdf54bab) - Created
[0m07:13:13.283579 [debug] [ThreadPool]: SQL status: OK in 22.080 seconds
[0m07:13:13.312461 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7219-1305-8ec3-c1759ca147ec, command-id=01f0ec61-7248-17a7-97bb-bfdef043aefe) - Closing
[0m07:13:13.678659 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:13:13.681900 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7219-1305-8ec3-c1759ca147ec) - Closing
[0m07:13:13.806107 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:13:13.809615 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:13:13.820551 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:13:13.824097 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:13:13.827434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:14.296949 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7f72-1c71-ad1c-ed8aad1778df) - Created
[0m07:13:14.408859 [debug] [ThreadPool]: SQL status: OK in 3.420 seconds
[0m07:13:14.427062 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7dcc-1975-9653-836bf4ea62bd, command-id=01f0ec61-7dde-1e04-8fab-b54f0d0807dd) - Closing
[0m07:13:14.435454 [debug] [ThreadPool]: On list_workspace: Close
[0m07:13:14.442688 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7dcc-1975-9653-836bf4ea62bd) - Closing
[0m07:13:14.590437 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:13:14.600339 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:13:14.663217 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:13:14.668128 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:13:14.672243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:15.174119 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7ff7-1ead-b459-4a6aeece9cd3) - Created
[0m07:13:16.102202 [debug] [ThreadPool]: SQL status: OK in 5.100 seconds
[0m07:13:16.122964 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7dcd-1138-90cb-0994fdf54bab, command-id=01f0ec61-7dde-1a69-9760-90d7f858ba12) - Closing
[0m07:13:16.130600 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:13:16.137610 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7dcd-1138-90cb-0994fdf54bab) - Closing
[0m07:13:16.353469 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:13:16.364552 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:13:16.424806 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:13:16.427697 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:13:16.431220 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:16.913217 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8102-1848-a68e-40cfa3303f5a) - Created
[0m07:13:17.269177 [debug] [ThreadPool]: SQL status: OK in 2.600 seconds
[0m07:13:17.276924 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7ff7-1ead-b459-4a6aeece9cd3, command-id=01f0ec61-800b-1aaa-b399-bd482309d45e) - Closing
[0m07:13:17.280558 [debug] [ThreadPool]: On list_workspace: Close
[0m07:13:17.283773 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7ff7-1ead-b459-4a6aeece9cd3) - Closing
[0m07:13:17.431654 [debug] [ThreadPool]: SQL status: OK in 3.600 seconds
[0m07:13:17.436692 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:13:17.443510 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:13:17.451379 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-7f72-1c71-ad1c-ed8aad1778df, command-id=01f0ec61-7f85-1a7d-b9d1-6ef0ea23030b) - Closing
[0m07:13:17.460449 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:13:17.471541 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-7f72-1c71-ad1c-ed8aad1778df) - Closing
[0m07:13:17.527128 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:13:17.529948 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:13:17.532734 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:17.617229 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:13:17.620937 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:13:17.635519 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:13:17.637785 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:13:17.639602 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:17.960595 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-819d-1d72-8d5d-66e3f3e7a49e) - Created
[0m07:13:18.070914 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-81b2-11ab-9f9b-21c7c13ce70d) - Created
[0m07:13:18.929493 [debug] [ThreadPool]: SQL status: OK in 2.500 seconds
[0m07:13:18.947753 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-8102-1848-a68e-40cfa3303f5a, command-id=01f0ec61-8113-1f0a-8819-920f0b39ad77) - Closing
[0m07:13:18.956406 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:13:18.963995 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8102-1848-a68e-40cfa3303f5a) - Closing
[0m07:13:19.108601 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:13:19.119317 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:13:19.155359 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:13:19.163887 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:13:19.175122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:19.642816 [debug] [ThreadPool]: SQL status: OK in 2.110 seconds
[0m07:13:19.656666 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-819d-1d72-8d5d-66e3f3e7a49e, command-id=01f0ec61-81b4-1881-8d63-89bcf6be69fd) - Closing
[0m07:13:19.660348 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:13:19.663313 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-819d-1d72-8d5d-66e3f3e7a49e) - Closing
[0m07:13:19.721711 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-82af-1087-9dbc-e9a23fa2c4bd) - Created
[0m07:13:19.798219 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:13:19.804469 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:13:19.830496 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:13:19.834965 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:13:19.839292 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:20.344505 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8309-167e-90ad-132e9c491f47) - Created
[0m07:13:20.492270 [debug] [ThreadPool]: SQL status: OK in 2.850 seconds
[0m07:13:20.511897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-81b2-11ab-9f9b-21c7c13ce70d, command-id=01f0ec61-81c7-1313-830b-149ce280b770) - Closing
[0m07:13:20.521248 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:13:20.528924 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-81b2-11ab-9f9b-21c7c13ce70d) - Closing
[0m07:13:20.691076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6638bc84-f761-4d54-97c8-2e57270af312', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1e985210>]}
[0m07:13:20.707906 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:13:20.710651 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m07:13:20.713821 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m07:13:20.715881 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m07:13:20.719994 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:13:20.791228 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:13:20.820659 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:13:20.864865 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:13:20.878271 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:13:20.879818 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m07:13:20.881093 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:21.061244 [debug] [ThreadPool]: SQL status: OK in 1.890 seconds
[0m07:13:21.071081 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-82af-1087-9dbc-e9a23fa2c4bd, command-id=01f0ec61-82c0-1fe8-86c1-e9440b187d78) - Closing
[0m07:13:21.075140 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:13:21.078323 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-82af-1087-9dbc-e9a23fa2c4bd) - Closing
[0m07:13:21.227042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e99206530>]}
[0m07:13:21.247872 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m07:13:21.257821 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m07:13:21.267128 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m07:13:21.273978 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m07:13:21.280923 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m07:13:21.288394 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m07:13:21.298938 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-839f-1823-9a4c-4740e5ecf908) - Created
[0m07:13:21.342377 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:13:21.350169 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e75401c30>]}
[0m07:13:21.441861 [debug] [ThreadPool]: SQL status: OK in 1.600 seconds
[0m07:13:21.459914 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-8309-167e-90ad-132e9c491f47, command-id=01f0ec61-8320-121a-8dd5-cc4c9c49363b) - Closing
[0m07:13:21.467507 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:13:21.474394 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8309-167e-90ad-132e9c491f47) - Closing
[0m07:13:21.624397 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:13:21.629293 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:13:21.640218 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:13:21.642572 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:13:21.644000 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:13:21.644491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:13:21.646330 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e75420730>]}
[0m07:13:21.692338 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:13:21.694712 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:13:21.696687 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:13:22.090894 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8419-189d-a7d5-d867d766106a) - Created
[0m07:13:22.166187 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec61-8421-14ae-a103-541471f12c34) - Created
[0m07:13:23.339811 [debug] [Thread-4 (]: SQL status: OK in 2.460 seconds
[0m07:13:23.343407 [debug] [ThreadPool]: SQL status: OK in 1.700 seconds
[0m07:13:23.361815 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-839f-1823-9a4c-4740e5ecf908, command-id=01f0ec61-83b2-1e9f-8756-f78208ee219f) - Closing
[0m07:13:23.365464 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec61-8419-189d-a7d5-d867d766106a, command-id=01f0ec61-8429-1e20-ae3e-987daf395e2a) - Closing
[0m07:13:23.377085 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:13:23.384893 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec61-8419-189d-a7d5-d867d766106a) - Closing
[0m07:13:23.434393 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m07:13:23.447011 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-839f-1823-9a4c-4740e5ecf908) - Closing
[0m07:13:23.538153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1cbb3940>]}
[0m07:13:23.548554 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m07:13:23.553499 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m07:13:23.558444 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m07:13:23.562574 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m07:13:23.566407 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m07:13:23.592974 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 2.88s]
[0m07:13:23.597686 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:13:23.601697 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:13:23.606007 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m07:13:23.610316 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m07:13:23.610525 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m07:13:23.613890 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m07:13:23.617452 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:13:23.633601 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:13:23.646084 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m07:13:23.676991 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:13:23.693070 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:13:23.724117 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:13:23.728002 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m07:13:23.730824 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:13:23.731498 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:23.741602 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:13:23.744806 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af0713910>]}
[0m07:13:23.776322 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m07:13:23.795541 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m07:13:23.821640 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m07:13:23.824620 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m07:13:23.827497 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:13:24.183294 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8555-16f1-9500-5444be772100) - Created
[0m07:13:24.300056 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8568-18b1-8a9c-1ce11e4c522f) - Created
[0m07:13:24.728377 [debug] [MainThread]: SQL status: OK in 33.440 seconds
[0m07:13:24.766347 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ec61-7222-125f-a19a-79f4cd31a8e3, command-id=01f0ec61-724c-1690-83b9-563fa568d332) - Closing
[0m07:13:24.995234 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Compilation Error
  'write_csv' is undefined
[0m07:13:25.002948 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m07:13:25.010360 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ec61-7222-125f-a19a-79f4cd31a8e3) - Closing
[0m07:13:25.163576 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  'write_csv' is undefined
[0m07:13:25.179869 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 418, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 44, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 399, in call
    if not __self.is_safe_callable(__obj):
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 265, in is_safe_callable
    getattr(obj, "unsafe_callable", False) or getattr(obj, "alters_data", False)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 870, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 859, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'write_csv' is undefined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 393, in call_macro
    with self.exception_handler():
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 420, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  'write_csv' is undefined

[0m07:13:25.275130 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m07:13:25.285553 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 41.148476, "process_in_blocks": "0", "process_kernel_time": 1.173942, "process_mem_max_rss": "240704", "process_out_blocks": "0", "process_user_time": 10.002803}
[0m07:13:25.293500 [debug] [MainThread]: Command `dbt run-operation` failed at 07:13:25.292706 after 41.16 seconds
[0m07:13:25.300397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e07aa02200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e07979a260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e0580a7580>]}
[0m07:13:25.311781 [debug] [MainThread]: Flushing usage events
[0m07:13:25.759526 [debug] [Thread-4 (]: SQL status: OK in 2.030 seconds
[0m07:13:25.788369 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-8555-16f1-9500-5444be772100, command-id=01f0ec61-8569-1d99-984d-90225584236f) - Closing
[0m07:13:25.803632 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m07:13:25.809741 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8555-16f1-9500-5444be772100) - Closing
[0m07:13:25.876773 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:13:25.950560 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 2.34s]
[0m07:13:25.959938 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:13:25.971973 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:13:25.982179 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m07:13:25.992449 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m07:13:26.002749 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m07:13:26.011855 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:13:26.013225 [debug] [Thread-7 (]: SQL status: OK in 2.190 seconds
[0m07:13:26.026095 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec61-8568-18b1-8a9c-1ce11e4c522f, command-id=01f0ec61-857c-1783-8180-c8325bb88189) - Closing
[0m07:13:26.063835 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:13:26.085240 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:13:26.091208 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:13:26.092652 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m07:13:26.095113 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8568-18b1-8a9c-1ce11e4c522f) - Closing
[0m07:13:26.109285 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:13:26.133537 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:13:26.136776 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m07:13:26.139691 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:26.244864 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af812c6a0>]}
[0m07:13:26.249607 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.68s]
[0m07:13:26.256718 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m07:13:26.261160 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m07:13:26.265931 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m07:13:26.271723 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m07:13:26.275317 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m07:13:26.278638 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m07:13:26.293117 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m07:13:26.325935 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m07:13:26.337027 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:13:26.342669 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m07:13:26.346980 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m07:13:26.378129 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m07:13:26.381953 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m07:13:26.385848 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:13:26.540573 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-86bf-13de-9eba-b7508127dd86) - Created
[0m07:13:26.868108 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-86f1-1838-a82e-3607e15cf795) - Created
[0m07:13:28.151907 [debug] [Thread-4 (]: SQL status: OK in 2.010 seconds
[0m07:13:28.169256 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-86bf-13de-9eba-b7508127dd86, command-id=01f0ec61-86d0-1b1e-8916-f8aa5cae8655) - Closing
[0m07:13:28.178321 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m07:13:28.184928 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-86bf-13de-9eba-b7508127dd86) - Closing
[0m07:13:28.200990 [debug] [Thread-5 (]: SQL status: OK in 6.500 seconds
[0m07:13:28.212562 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec61-8421-14ae-a103-541471f12c34, command-id=01f0ec61-8434-1ca0-9b8f-b179d24a29f3) - Closing
[0m07:13:28.311689 [debug] [Thread-7 (]: SQL status: OK in 1.930 seconds
[0m07:13:28.322864 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec61-86f1-1838-a82e-3607e15cf795, command-id=01f0ec61-8702-1138-853c-eec8e87ecdc5) - Closing
[0m07:13:28.331545 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:13:28.344126 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m07:13:28.343872 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 2.35s]
[0m07:13:28.353133 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:13:28.352630 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-86f1-1838-a82e-3607e15cf795) - Closing
[0m07:13:28.361692 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:13:28.371298 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m07:13:28.383991 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m07:13:28.394062 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m07:13:28.401711 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:13:28.449273 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:13:28.491584 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:13:28.500126 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af8b80be0>]}
[0m07:13:28.500045 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:13:28.505720 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m07:13:28.506738 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 2.23s]
[0m07:13:28.509724 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:13:28.512020 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m07:13:28.519916 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m07:13:28.526057 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m07:13:28.531261 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m07:13:28.535918 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m07:13:28.541260 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m07:13:28.550135 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:13:28.555420 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m07:13:28.560672 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:28.577558 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m07:13:28.616837 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m07:13:28.637421 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:13:28.647090 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m07:13:28.652548 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m07:13:28.694716 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m07:13:28.699211 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m07:13:28.704016 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:13:29.058490 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-883f-1f23-8c3a-830f6c5a25c0) - Created
[0m07:13:29.202000 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8856-17b0-a8e3-97cd8c3fb797) - Created
[0m07:13:30.691654 [debug] [Thread-4 (]: SQL status: OK in 2.130 seconds
[0m07:13:30.700165 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-883f-1f23-8c3a-830f6c5a25c0, command-id=01f0ec61-8850-1445-9bee-c591ba565083) - Closing
[0m07:13:30.703941 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m07:13:30.707138 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-883f-1f23-8c3a-830f6c5a25c0) - Closing
[0m07:13:30.846592 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 2.46s]
[0m07:13:30.857868 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:13:30.861933 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:13:30.865718 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m07:13:30.871223 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m07:13:30.874925 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m07:13:30.878085 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:13:30.895509 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:13:30.910728 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:13:30.917629 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:13:30.932456 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:13:30.934241 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m07:13:30.936895 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:30.970865 [debug] [Thread-7 (]: SQL status: OK in 2.270 seconds
[0m07:13:30.975035 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec61-8856-17b0-a8e3-97cd8c3fb797, command-id=01f0ec61-8867-10c6-b9c4-cfcc39dba5a9) - Closing
[0m07:13:30.978135 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:13:30.981774 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m07:13:30.984281 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8856-17b0-a8e3-97cd8c3fb797) - Closing
[0m07:13:31.140105 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af8127a30>]}
[0m07:13:31.155759 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.60s]
[0m07:13:31.163420 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m07:13:31.172013 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m07:13:31.178275 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m07:13:31.182931 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m07:13:31.187181 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m07:13:31.190895 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m07:13:31.222796 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m07:13:31.257467 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m07:13:31.351917 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m07:13:31.402697 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:13:31.406165 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af0753ca0>]}
[0m07:13:31.445076 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-89ac-1693-9fa1-24650f6a6168) - Created
[0m07:13:31.449990 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m07:13:31.476733 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m07:13:31.480687 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m07:13:31.484256 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:13:32.010694 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8a00-1265-b591-b2a2db2f0c36) - Created
[0m07:13:32.941009 [debug] [Thread-4 (]: SQL status: OK in 2.000 seconds
[0m07:13:32.960880 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-89ac-1693-9fa1-24650f6a6168, command-id=01f0ec61-89bb-1de0-bb4d-61fa21edb6c2) - Closing
[0m07:13:32.969877 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m07:13:32.974409 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-89ac-1693-9fa1-24650f6a6168) - Closing
[0m07:13:33.151607 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.28s]
[0m07:13:33.154355 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:13:33.156471 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:13:33.158516 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m07:13:33.161405 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m07:13:33.164086 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m07:13:33.166739 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:13:33.194834 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:13:33.223658 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:13:33.233660 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:13:33.254693 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:13:33.257953 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m07:13:33.261245 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:33.764858 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8b0d-146e-a9fb-aef08c2ffe5f) - Created
[0m07:13:35.274351 [debug] [Thread-5 (]: SQL status: OK in 6.760 seconds
[0m07:13:35.282628 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec61-8421-14ae-a103-541471f12c34, command-id=01f0ec61-87fc-17c2-8ee2-376fb6c7307f) - Closing
[0m07:13:35.707844 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m07:13:35.799469 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m07:13:35.802253 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec61-8421-14ae-a103-541471f12c34) - Closing
[0m07:13:36.278701 [debug] [Thread-4 (]: SQL status: OK in 3.020 seconds
[0m07:13:36.278557 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9e2c1b10>]}
[0m07:13:36.289017 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 15.00s]
[0m07:13:36.292192 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-8b0d-146e-a9fb-aef08c2ffe5f, command-id=01f0ec61-8b20-1444-9c15-087e5cc457aa) - Closing
[0m07:13:36.295186 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m07:13:36.298132 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m07:13:36.299102 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m07:13:36.301101 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8b0d-146e-a9fb-aef08c2ffe5f) - Closing
[0m07:13:36.305072 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m07:13:36.311824 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m07:13:36.315120 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m07:13:36.318180 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m07:13:36.321760 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m07:13:36.355384 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:13:36.358307 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:13:36.360525 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:13:36.482763 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 3.32s]
[0m07:13:36.490687 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:13:36.496709 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:13:36.503835 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m07:13:36.512481 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m07:13:36.520421 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m07:13:36.527334 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:13:36.572004 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:13:36.635081 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:13:36.668023 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:13:36.732410 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:13:36.741763 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m07:13:36.749266 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:37.047137 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec61-8d02-1907-bdab-df03bd677272) - Created
[0m07:13:37.325923 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8d2c-1b13-91eb-3c2d7d752208) - Created
[0m07:13:38.772633 [debug] [Thread-4 (]: SQL status: OK in 2.020 seconds
[0m07:13:38.780521 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-8d2c-1b13-91eb-3c2d7d752208, command-id=01f0ec61-8d3d-1a61-8ebc-05a256003d75) - Closing
[0m07:13:38.784846 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m07:13:38.788358 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8d2c-1b13-91eb-3c2d7d752208) - Closing
[0m07:13:38.924959 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 2.41s]
[0m07:13:38.929182 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:13:38.940847 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:13:38.944699 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m07:13:38.948611 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m07:13:38.952504 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m07:13:38.955972 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:13:39.021857 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:13:39.061392 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:13:39.098638 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:13:39.133734 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:13:39.138753 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:13:39.143151 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:39.659191 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8e8e-1368-90e9-86cd1a46afc3) - Created
[0m07:13:39.880368 [debug] [Thread-5 (]: SQL status: OK in 3.520 seconds
[0m07:13:39.893279 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec61-8d02-1907-bdab-df03bd677272, command-id=01f0ec61-8d13-1f16-8408-fa7dd303566c) - Closing
[0m07:13:39.932969 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:13:39.949914 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m07:13:40.380733 [debug] [Thread-7 (]: SQL status: OK in 8.900 seconds
[0m07:13:40.393572 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec61-8a00-1265-b591-b2a2db2f0c36, command-id=01f0ec61-8a14-19e8-8b82-15605b606135) - Closing
[0m07:13:40.662662 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:13:40.789346 [debug] [Thread-4 (]: SQL status: OK in 1.640 seconds
[0m07:13:40.807133 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-8e8e-1368-90e9-86cd1a46afc3, command-id=01f0ec61-8ea2-178b-985b-cea920d6cada) - Closing
[0m07:13:40.814305 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m07:13:40.823337 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m07:13:40.832914 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec61-8a00-1265-b591-b2a2db2f0c36) - Closing
[0m07:13:40.829693 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8e8e-1368-90e9-86cd1a46afc3) - Closing
[0m07:13:40.968618 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba3bdfcc-1977-4e70-ba0f-6568e3d576f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777af8b80be0>]}
[0m07:13:40.980778 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 2.03s]
[0m07:13:40.981870 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 9.79s]
[0m07:13:40.989837 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:13:40.991752 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m07:13:40.996231 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:13:41.002195 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:13:41.005129 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m07:13:41.010564 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:13:41.012052 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m07:13:41.023811 [info ] [MainThread]: 
[0m07:13:41.032011 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m07:13:41.048068 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:13:41.047806 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 49.91 seconds (49.91s).
[0m07:13:41.072762 [debug] [MainThread]: Command end result
[0m07:13:41.129967 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:13:41.191849 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:13:41.223793 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:13:41.244541 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:13:41.247700 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:13:41.250448 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:41.513872 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:13:41.539655 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:13:41.598726 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:13:41.601586 [info ] [MainThread]: 
[0m07:13:41.604575 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:13:41.607404 [info ] [MainThread]: 
[0m07:13:41.610036 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m07:13:41.614967 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 57.759537, "process_in_blocks": "0", "process_kernel_time": 1.486987, "process_mem_max_rss": "246412", "process_out_blocks": "664", "process_user_time": 12.751426}
[0m07:13:41.621374 [debug] [MainThread]: Command `dbt run` succeeded at 07:13:41.620972 after 57.77 seconds
[0m07:13:41.624194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1b28a440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1b1c5a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777b1a0cc490>]}
[0m07:13:41.649646 [debug] [MainThread]: Flushing usage events
[0m07:13:41.866398 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8fe1-1b61-8347-47b8e3fe8e77) - Created
[0m07:13:42.172253 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:13:43.075351 [debug] [Thread-5 (]: SQL status: OK in 3.100 seconds
[0m07:13:43.081223 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec61-8d02-1907-bdab-df03bd677272, command-id=01f0ec61-8ee6-1d10-9afe-559fa7a850b8) - Closing
[0m07:13:43.085270 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m07:13:43.120984 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m07:13:43.147044 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec61-8d02-1907-bdab-df03bd677272) - Closing
[0m07:13:43.286146 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63cbfb42-5639-4073-ae95-b92fd240a740', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9c87b070>]}
[0m07:13:43.290235 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 6.97s]
[0m07:13:43.294711 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m07:13:43.303237 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:13:43.307730 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:13:43.313715 [info ] [MainThread]: 
[0m07:13:43.317403 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 52.22 seconds (52.22s).
[0m07:13:43.330652 [debug] [MainThread]: Command end result
[0m07:13:43.357883 [debug] [Thread-4 (]: SQL status: OK in 2.110 seconds
[0m07:13:43.384655 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-8fe1-1b61-8347-47b8e3fe8e77, command-id=01f0ec61-8ff5-1c1e-bd41-3f03245248d2) - Closing
[0m07:13:43.388823 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m07:13:43.394273 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-8fe1-1b61-8347-47b8e3fe8e77) - Closing
[0m07:13:43.539950 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.53s]
[0m07:13:43.543810 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:13:43.556433 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:13:43.559208 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m07:13:43.572008 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m07:13:43.574759 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m07:13:43.588984 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:13:43.592515 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:13:43.803568 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:13:43.828471 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:13:43.899973 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:13:43.906663 [info ] [MainThread]: 
[0m07:13:43.909856 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:13:43.913148 [info ] [MainThread]: 
[0m07:13:43.917953 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m07:13:43.926121 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 60.08346, "process_in_blocks": "0", "process_kernel_time": 1.197485, "process_mem_max_rss": "246696", "process_out_blocks": "416", "process_user_time": 12.043865}
[0m07:13:43.929626 [debug] [MainThread]: Command `dbt seed` succeeded at 07:13:43.929221 after 60.09 seconds
[0m07:13:43.942237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9c3e22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9bdbe650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774e9e099030>]}
[0m07:13:43.945155 [debug] [MainThread]: Flushing usage events
[0m07:13:44.011130 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:44.022284 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:13:44.026902 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:45.401943 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f) - Created
[0m07:13:45.543140 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:13:45.990884 [debug] [Thread-4 (]: SQL status: OK in 1.960 seconds
[0m07:13:46.006888 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f, command-id=01f0ec61-920e-1860-bdeb-6230d591a276) - Closing
[0m07:13:46.261152 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:46.511343 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:46.961800 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:46.976086 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:13:47.343339 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m07:13:47.352237 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f, command-id=01f0ec61-9300-1a25-b2b8-4866ba7691a3) - Closing
[0m07:13:47.381155 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:47.387117 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m07:13:47.679121 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m07:13:47.693731 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f, command-id=01f0ec61-933d-1d5b-bc2f-d46afb457e97) - Closing
[0m07:13:47.777310 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:47.812742 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:47.816655 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:13:48.360831 [debug] [Thread-4 (]: SQL status: OK in 0.530 seconds
[0m07:13:48.376685 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f, command-id=01f0ec61-937f-17ad-98e4-9ee5cbf23262) - Closing
[0m07:13:48.447569 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:13:48.477778 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:13:48.493629 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:13:49.042492 [debug] [Thread-4 (]: SQL status: OK in 0.540 seconds
[0m07:13:49.072237 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f, command-id=01f0ec61-93ea-19ed-bfe3-296759cd3c75) - Closing
[0m07:13:49.100859 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m07:13:49.111974 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-91fd-1e0c-9394-3e31f197fb6f) - Closing
[0m07:13:49.275435 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 5.70s]
[0m07:13:49.280104 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:13:49.283517 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:13:49.292956 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m07:13:49.299199 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m07:13:49.304558 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m07:13:49.308251 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:13:49.314248 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:13:49.428020 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:49.440378 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:13:49.443460 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:50.020986 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e) - Created
[0m07:13:50.402913 [debug] [Thread-4 (]: SQL status: OK in 0.960 seconds
[0m07:13:50.419757 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e, command-id=01f0ec61-94d0-1094-8ec3-59fc00b57b46) - Closing
[0m07:13:50.492037 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:50.567881 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:50.612564 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:50.625396 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:13:51.061493 [debug] [Thread-4 (]: SQL status: OK in 0.430 seconds
[0m07:13:51.067698 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e, command-id=01f0ec61-952c-1bdd-b30a-4411e0784e9d) - Closing
[0m07:13:51.083730 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:51.094004 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m07:13:51.350897 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m07:13:51.360213 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e, command-id=01f0ec61-9573-1864-b887-3957b5e8d44c) - Closing
[0m07:13:51.373711 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:51.404702 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:51.408059 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:13:51.796908 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m07:13:51.804514 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e, command-id=01f0ec61-95a3-12a9-b06d-2e70f0f0d2ff) - Closing
[0m07:13:51.825628 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:13:51.829023 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:13:51.831727 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:13:52.325266 [debug] [Thread-4 (]: SQL status: OK in 0.490 seconds
[0m07:13:52.330012 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e, command-id=01f0ec61-95e4-1161-a357-c0783f367528) - Closing
[0m07:13:52.340329 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m07:13:52.343060 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-94bf-12ac-b2c9-825071cfb66e) - Closing
[0m07:13:52.479628 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 3.18s]
[0m07:13:52.482963 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:13:52.485576 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:13:52.488580 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m07:13:52.491997 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m07:13:52.494428 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m07:13:52.497536 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:13:52.501154 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:13:52.547964 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:52.574519 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:13:52.578387 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:13:53.113045 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2) - Created
[0m07:13:53.495995 [debug] [Thread-4 (]: SQL status: OK in 0.920 seconds
[0m07:13:53.503448 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-96a7-18ce-becb-835481213645) - Closing
[0m07:13:53.511616 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:53.579548 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:53.641224 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:53.644378 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m07:13:53.956841 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m07:13:53.963192 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-96f8-1bfb-b765-17756ada22f3) - Closing
[0m07:13:53.967790 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:53.971079 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:13:54.363400 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m07:13:54.372880 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-972e-14c4-8dd6-36f2135d95b8) - Closing
[0m07:13:54.395195 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:54.398715 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m07:13:54.936363 [debug] [Thread-4 (]: SQL status: OK in 0.530 seconds
[0m07:13:54.948702 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-976c-18b7-bc2e-80d53ca87c52) - Closing
[0m07:13:54.953759 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:54.980931 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:54.984564 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:13:55.385586 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m07:13:55.394898 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-97c5-1cd1-8cea-2606b539cb8b) - Closing
[0m07:13:55.414614 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m07:13:55.418717 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:13:55.422628 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m07:13:55.766260 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m07:13:55.772934 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2, command-id=01f0ec61-9808-1002-94a6-2ada48d9d514) - Closing
[0m07:13:55.781097 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m07:13:55.784055 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec61-9696-1245-87b1-a4a40515fff2) - Closing
[0m07:13:56.218501 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 3.73s]
[0m07:13:56.222205 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:13:56.228714 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:13:56.232461 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:13:56.236535 [info ] [MainThread]: 
[0m07:13:56.238992 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 1 minutes and 5.10 seconds (65.10s).
[0m07:13:56.250613 [debug] [MainThread]: Command end result
[0m07:13:56.738923 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:13:56.751144 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:13:56.778147 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:13:56.780101 [info ] [MainThread]: 
[0m07:13:56.782119 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m07:13:56.783893 [info ] [MainThread]: 
[0m07:13:56.785793 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m07:13:56.788724 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m07:13:56.792473 [info ] [MainThread]: 
[0m07:13:56.795782 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m07:13:56.799467 [info ] [MainThread]: 
[0m07:13:56.802942 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m07:13:56.807458 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 72.93863, "process_in_blocks": "0", "process_kernel_time": 1.544064, "process_mem_max_rss": "255640", "process_out_blocks": "16", "process_user_time": 16.295624}
[0m07:13:56.810348 [debug] [MainThread]: Command `dbt test` failed at 07:13:56.810060 after 72.94 seconds
[0m07:13:56.812539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8b1c0b7970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af41c6f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af41c7f70>]}
[0m07:13:56.814331 [debug] [MainThread]: Flushing usage events
[0m07:13:57.208685 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:26:19.756378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc44d66470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43b9c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43b9c250>]}
[0m07:26:19.756128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd40262c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd2e042b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd2e04250>]}


============================== 07:26:19.774229 | d1c52065-2103-4ff4-b63b-38bc6d3f6435 ==============================
[0m07:26:19.774229 [info ] [MainThread]: Running with dbt=1.10.18


============================== 07:26:19.774158 | 8fe3b5d6-3c9e-4adb-baf3-745f082d605a ==============================
[0m07:26:19.774158 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:26:19.761548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dd6d2290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dc5082b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dc508250>]}
[0m07:26:19.762046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f6682410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f54c4280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f54c4220>]}
[0m07:26:19.776776 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'log_cache_events': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'invocation_command': 'dbt seed --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'log_path': '/opt/dagster/app/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'write_json': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'warn_error': 'None', 'quiet': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'no_print': 'None', 'target_path': 'None', 'introspect': 'True', 'printer_width': '80', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'debug': 'False'}
[0m07:26:19.776910 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'use_colors': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'version_check': 'True', 'printer_width': '80', 'partial_parse': 'True', 'no_print': 'None', 'warn_error': 'None', 'target_path': 'None', 'invocation_command': 'dbt test --profiles-dir /opt/dagster/app/dbt --target dev', 'log_cache_events': 'False'}


============================== 07:26:19.778917 | aabe02f8-8d1b-4eca-a61e-b262ee402eb5 ==============================
[0m07:26:19.778917 [info ] [MainThread]: Running with dbt=1.10.18


============================== 07:26:19.778914 | cec4d3c9-8403-4b29-96ea-8147593772e0 ==============================
[0m07:26:19.778914 [info ] [MainThread]: Running with dbt=1.10.18
[0m07:26:19.781367 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'write_json': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'invocation_command': 'dbt run --full-refresh --profiles-dir /opt/dagster/app/dbt --target dev', 'debug': 'False', 'static_parser': 'True', 'version_check': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'empty': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False'}
[0m07:26:19.781555 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'write_json': 'True', 'empty': 'None', 'introspect': 'True', 'log_format': 'default', 'partial_parse': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name": "account_interest_summary", "local_folder": "/opt/dagster/app/target", "filename": "account_summary.csv"} --profiles-dir /opt/dagster/app/dbt --target dev', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'target_path': 'None', 'use_colors': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'debug': 'False', 'version_check': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'quiet': 'False'}
[0m07:26:21.688847 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:26:21.689341 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:26:21.690315 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:26:21.691047 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:26:21.691725 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:26:21.692653 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:26:21.693095 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:26:21.693769 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:26:21.694804 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:26:21.707023 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m07:26:21.709127 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m07:26:21.711235 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m07:26:23.937893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cada912d0>]}
[0m07:26:23.961584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43d834f0>]}
[0m07:26:23.976900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767df2050c0>]}
[0m07:26:24.015135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cec4d3c9-8403-4b29-96ea-8147593772e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f3857880>]}
[0m07:26:24.184879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd3638d00>]}
[0m07:26:24.187960 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:26:24.237225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43efed40>]}
[0m07:26:24.240084 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:26:24.279293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dc68f730>]}
[0m07:26:24.296621 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:26:24.309812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cec4d3c9-8403-4b29-96ea-8147593772e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f5486440>]}
[0m07:26:24.315336 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m07:26:24.907428 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:26:24.972182 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:26:25.000982 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:26:25.025655 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m07:26:26.015195 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:26:26.017108 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:26:26.040102 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:26:26.063721 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:26:26.065459 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:26:26.091778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:26:26.093845 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:26:26.094299 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:26:26.120983 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:26:26.207395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd34f8ca0>]}
[0m07:26:26.218568 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:26:26.220638 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:26:26.243275 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m07:26:26.274685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc1dc70b80>]}
[0m07:26:26.280309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b65d8b50>]}
[0m07:26:26.379357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cec4d3c9-8403-4b29-96ea-8147593772e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9cf52fe50>]}
[0m07:26:26.605033 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:26:26.614823 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:26:26.634126 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:26:26.644573 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:26:26.674952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b65db430>]}
[0m07:26:26.676211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739caceefca0>]}
[0m07:26:26.677335 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:26:26.678481 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:26:26.679367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b65d92a0>]}
[0m07:26:26.680239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739caceeda80>]}
[0m07:26:26.685133 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:26:26.686085 [info ] [MainThread]: 
[0m07:26:26.687742 [info ] [MainThread]: 
[0m07:26:26.688811 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:26:26.689920 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:26:26.691248 [info ] [MainThread]: 
[0m07:26:26.692170 [info ] [MainThread]: 
[0m07:26:26.693600 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:26:26.694733 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:26:26.695515 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:26:26.696904 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:26:26.697339 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:26:26.716080 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:26:26.719278 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:26:26.720777 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:26:26.723170 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:26:26.726756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc1dc737f0>]}
[0m07:26:26.729582 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:26:26.732062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc1dc5eb00>]}
[0m07:26:26.739193 [info ] [MainThread]: 
[0m07:26:26.741846 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:26:26.744105 [info ] [MainThread]: 
[0m07:26:26.747071 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:26:26.749197 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:26:26.751428 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:26:26.754045 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:26:26.756403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:26.760291 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:26:26.762756 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:26:26.765108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:26.767867 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:26:26.770225 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:26:26.798716 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:26:26.801011 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:26:26.803678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:26.808910 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:26:26.823547 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:26:26.848315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cec4d3c9-8403-4b29-96ea-8147593772e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f546ceb0>]}
[0m07:26:26.851764 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m07:26:26.855218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cec4d3c9-8403-4b29-96ea-8147593772e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9cf513a30>]}
[0m07:26:26.858360 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m07:26:26.860610 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m07:26:26.892212 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m07:26:26.894888 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
    select * from `workspace`.`marts`.`account_interest_summary`

  
[0m07:26:26.897017 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:26:27.709899 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ec63-5844-178b-8338-1505465bdc9d) - Created
[0m07:26:27.713225 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-5841-1080-af1d-84968c38edf7) - Created
[0m07:26:27.714106 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-5844-1771-b016-d311ad82c4dc) - Created
[0m07:26:27.712760 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-584b-18cb-9c8e-106e578bb026) - Created
[0m07:26:46.445773 [debug] [ThreadPool]: SQL status: OK in 19.640 seconds
[0m07:26:46.447763 [debug] [ThreadPool]: SQL status: OK in 19.690 seconds
[0m07:26:46.479570 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-5841-1080-af1d-84968c38edf7, command-id=01f0ec63-5873-174b-95c0-a3966107d036) - Closing
[0m07:26:46.482436 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-5844-1771-b016-d311ad82c4dc, command-id=01f0ec63-5873-17a4-b8c9-bcbaf31799be) - Closing
[0m07:26:46.949174 [debug] [ThreadPool]: On list_workspace: Close
[0m07:26:46.949174 [debug] [ThreadPool]: On list_workspace: Close
[0m07:26:46.956655 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-5844-1771-b016-d311ad82c4dc) - Closing
[0m07:26:46.957311 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-5841-1080-af1d-84968c38edf7) - Closing
[0m07:26:47.124731 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:26:47.141549 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:26:47.147726 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:26:47.154886 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:26:47.157910 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:26:47.160205 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:26:47.162178 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:47.174129 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:26:47.177754 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:26:47.179635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:47.627449 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-643b-1080-ab2d-22d6d220e6ab) - Created
[0m07:26:47.648692 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-643e-1a53-832e-99f78d90adfc) - Created
[0m07:26:49.229267 [debug] [ThreadPool]: SQL status: OK in 22.460 seconds
[0m07:26:49.261982 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-584b-18cb-9c8e-106e578bb026, command-id=01f0ec63-5873-1eac-a21e-5576058d2587) - Closing
[0m07:26:49.608751 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:26:49.615933 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-584b-18cb-9c8e-106e578bb026) - Closing
[0m07:26:49.758296 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:26:49.768323 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:26:49.790181 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:26:49.797887 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:26:49.806212 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:50.288820 [debug] [ThreadPool]: SQL status: OK in 3.130 seconds
[0m07:26:50.294875 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-643e-1a53-832e-99f78d90adfc, command-id=01f0ec63-6452-1516-a407-98e726eb262a) - Closing
[0m07:26:50.297958 [debug] [ThreadPool]: On list_workspace: Close
[0m07:26:50.301442 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-643e-1a53-832e-99f78d90adfc) - Closing
[0m07:26:50.424695 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-65e2-1f6b-8f64-948fc8ae94f9) - Created
[0m07:26:50.453913 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m07:26:50.458934 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m07:26:50.487664 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m07:26:50.491958 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m07:26:50.495488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:51.029790 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6642-1fbd-b7d8-7025e13f04a1) - Created
[0m07:26:51.285813 [debug] [ThreadPool]: SQL status: OK in 4.110 seconds
[0m07:26:51.297035 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-643b-1080-ab2d-22d6d220e6ab, command-id=01f0ec63-644f-1a9d-a56e-f44d1626c306) - Closing
[0m07:26:51.301713 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:26:51.304092 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-643b-1080-ab2d-22d6d220e6ab) - Closing
[0m07:26:51.431108 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:26:51.435389 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:26:51.457231 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:26:51.460027 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:26:51.464249 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:51.940014 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-66ce-12a9-a3c0-4719d2aacc58) - Created
[0m07:26:52.170649 [debug] [ThreadPool]: SQL status: OK in 1.680 seconds
[0m07:26:52.176292 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-6642-1fbd-b7d8-7025e13f04a1, command-id=01f0ec63-6655-18f5-b00d-e09ad1c96b1e) - Closing
[0m07:26:52.179244 [debug] [ThreadPool]: On list_workspace: Close
[0m07:26:52.181720 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6642-1fbd-b7d8-7025e13f04a1) - Closing
[0m07:26:52.319807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:26:52.323882 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:26:52.364399 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:26:52.367822 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:26:52.370654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:52.474355 [debug] [ThreadPool]: SQL status: OK in 2.670 seconds
[0m07:26:52.480042 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-65e2-1f6b-8f64-948fc8ae94f9, command-id=01f0ec63-65f8-1cf1-b2e6-6500a8b53047) - Closing
[0m07:26:52.482621 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:26:52.484842 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-65e2-1f6b-8f64-948fc8ae94f9) - Closing
[0m07:26:52.608291 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m07:26:52.611664 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m07:26:52.625494 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m07:26:52.628800 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m07:26:52.631874 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:52.804157 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6752-12bd-8475-0ddab5a326fa) - Created
[0m07:26:53.092652 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-677a-1f5e-bc6d-415263f3fc10) - Created
[0m07:26:54.544019 [debug] [ThreadPool]: SQL status: OK in 3.080 seconds
[0m07:26:54.550880 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-66ce-12a9-a3c0-4719d2aacc58, command-id=01f0ec63-66df-13ea-a29a-5b41edda9adc) - Closing
[0m07:26:54.556061 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:26:54.558649 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-66ce-12a9-a3c0-4719d2aacc58) - Closing
[0m07:26:54.699774 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:26:54.706054 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:26:54.720635 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:26:54.723772 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:26:54.726589 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:55.025867 [debug] [ThreadPool]: SQL status: OK in 2.660 seconds
[0m07:26:55.038361 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-6752-12bd-8475-0ddab5a326fa, command-id=01f0ec63-6763-1afb-836c-42946cd321fe) - Closing
[0m07:26:55.042651 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:26:55.045976 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6752-12bd-8475-0ddab5a326fa) - Closing
[0m07:26:55.190881 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m07:26:55.195315 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m07:26:55.196852 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-68be-1fe2-9b73-978b18daaf0b) - Created
[0m07:26:55.210107 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m07:26:55.212896 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m07:26:55.215379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:55.419191 [debug] [ThreadPool]: SQL status: OK in 2.790 seconds
[0m07:26:55.425769 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-677a-1f5e-bc6d-415263f3fc10, command-id=01f0ec63-678f-1613-8818-11adca8c4e72) - Closing
[0m07:26:55.429627 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m07:26:55.432269 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-677a-1f5e-bc6d-415263f3fc10) - Closing
[0m07:26:55.564759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1c52065-2103-4ff4-b63b-38bc6d3f6435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd5b47820>]}
[0m07:26:55.577464 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:26:55.579886 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m07:26:55.583162 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m07:26:55.585870 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m07:26:55.588332 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:26:55.628616 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:26:55.640302 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6903-16a8-bb59-3c8e90082053) - Created
[0m07:26:55.650209 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:26:55.704278 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:26:55.724918 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m07:26:55.728225 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m07:26:55.731628 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:26:56.155421 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6952-1189-a52c-dd0444eeb6ef) - Created
[0m07:26:56.893757 [debug] [ThreadPool]: SQL status: OK in 2.170 seconds
[0m07:26:56.902624 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-68be-1fe2-9b73-978b18daaf0b, command-id=01f0ec63-68d0-184c-9134-2c5c6517a7a1) - Closing
[0m07:26:56.907310 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:26:56.911009 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-68be-1fe2-9b73-978b18daaf0b) - Closing
[0m07:26:57.050112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc41b0e560>]}
[0m07:26:57.064345 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m07:26:57.069054 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m07:26:57.072979 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m07:26:57.075607 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m07:26:57.078219 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m07:26:57.080603 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m07:26:57.101094 [debug] [ThreadPool]: SQL status: OK in 1.890 seconds
[0m07:26:57.103942 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:26:57.107945 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-6903-16a8-bb59-3c8e90082053, command-id=01f0ec63-6914-1273-9a24-4884ce3bd91d) - Closing
[0m07:26:57.107809 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc1dbf1c30>]}
[0m07:26:57.111070 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m07:26:57.113129 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6903-16a8-bb59-3c8e90082053) - Closing
[0m07:26:57.207315 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:26:57.209682 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc1dc10730>]}
[0m07:26:57.253236 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m07:26:57.256600 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m07:26:57.262419 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:26:57.262946 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m07:26:57.264838 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:26:57.264944 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m07:26:57.267403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:26:57.267866 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:26:57.744029 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6a3f-1f38-aef7-1bb58a174def) - Created
[0m07:26:57.747922 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec63-6a40-1090-b943-eb753944fca2) - Created
[0m07:26:58.013326 [debug] [Thread-4 (]: SQL status: OK in 2.280 seconds
[0m07:26:58.027282 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-6952-1189-a52c-dd0444eeb6ef, command-id=01f0ec63-6962-1d73-905b-a11ba7afdb87) - Closing
[0m07:26:58.045058 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m07:26:58.050428 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6952-1189-a52c-dd0444eeb6ef) - Closing
[0m07:26:58.196953 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 2.61s]
[0m07:26:58.203578 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m07:26:58.208516 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:26:58.212710 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m07:26:58.218701 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m07:26:58.223263 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m07:26:58.226896 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:26:58.253821 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:26:58.291085 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:26:58.307303 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:26:58.352553 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m07:26:58.357962 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m07:26:58.362635 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:26:58.871991 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6aee-1a1d-8284-57d1e77e31d3) - Created
[0m07:26:59.195386 [debug] [MainThread]: SQL status: OK in 32.300 seconds
[0m07:26:59.210635 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ec63-5844-178b-8338-1505465bdc9d, command-id=01f0ec63-5873-197d-8244-cadbc4a4aea3) - Closing
[0m07:26:59.545418 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Compilation Error
  'write_csv' is undefined
[0m07:26:59.555704 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m07:26:59.563231 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ec63-5844-178b-8338-1505465bdc9d) - Closing
[0m07:26:59.565309 [debug] [ThreadPool]: SQL status: OK in 2.300 seconds
[0m07:26:59.582679 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec63-6a3f-1f38-aef7-1bb58a174def, command-id=01f0ec63-6a59-17b2-af0a-dcfe03b633b9) - Closing
[0m07:26:59.592991 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m07:26:59.600111 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec63-6a3f-1f38-aef7-1bb58a174def) - Closing
[0m07:26:59.700603 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  'write_csv' is undefined
[0m07:26:59.715779 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 418, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 44, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 399, in call
    if not __self.is_safe_callable(__obj):
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 265, in is_safe_callable
    getattr(obj, "unsafe_callable", False) or getattr(obj, "alters_data", False)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 870, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 859, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'write_csv' is undefined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 393, in call_macro
    with self.exception_handler():
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 420, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  'write_csv' is undefined

[0m07:26:59.752022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767da492560>]}
[0m07:26:59.777053 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m07:26:59.786939 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m07:26:59.797324 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m07:26:59.806382 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m07:26:59.807391 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m07:26:59.814029 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m07:26:59.817502 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 40.24945, "process_in_blocks": "0", "process_kernel_time": 1.158618, "process_mem_max_rss": "240980", "process_out_blocks": "584", "process_user_time": 10.431634}
[0m07:26:59.826582 [debug] [MainThread]: Command `dbt run-operation` failed at 07:26:59.825344 after 40.26 seconds
[0m07:26:59.835571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f6682410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9f546ceb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da9cfd62110>]}
[0m07:26:59.844624 [debug] [MainThread]: Flushing usage events
[0m07:26:59.889234 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m07:26:59.938407 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m07:27:00.004680 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:27:00.012133 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:27:00.015022 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b6593eb0>]}
[0m07:27:00.049369 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m07:27:00.067735 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m07:27:00.089785 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m07:27:00.092742 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m07:27:00.095148 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:27:00.304368 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:27:00.490400 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6be7-18af-9be0-b94e80203c82) - Created
[0m07:27:01.144522 [debug] [Thread-4 (]: SQL status: OK in 2.780 seconds
[0m07:27:01.156814 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-6aee-1a1d-8284-57d1e77e31d3, command-id=01f0ec63-6b03-11a0-861e-baf1abb60bad) - Closing
[0m07:27:01.161665 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m07:27:01.165480 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6aee-1a1d-8284-57d1e77e31d3) - Closing
[0m07:27:01.311541 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 3.09s]
[0m07:27:01.317592 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m07:27:01.325083 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:27:01.330184 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m07:27:01.338222 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m07:27:01.342965 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m07:27:01.348620 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:27:01.385746 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:27:01.414408 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:27:01.433409 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:27:01.454515 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m07:27:01.457451 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m07:27:01.459689 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:01.919745 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6cc0-1e75-b1f4-931fe84fd132) - Created
[0m07:27:02.101292 [debug] [Thread-7 (]: SQL status: OK in 2.010 seconds
[0m07:27:02.106271 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec63-6be7-18af-9be0-b94e80203c82, command-id=01f0ec63-6bf8-116a-b6a4-2e6059e582a1) - Closing
[0m07:27:02.135167 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:27:02.141349 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m07:27:02.143661 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6be7-18af-9be0-b94e80203c82) - Closing
[0m07:27:02.311678 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b655c6d0>]}
[0m07:27:02.323743 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.50s]
[0m07:27:02.337353 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m07:27:02.349719 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m07:27:02.358696 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m07:27:02.370328 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m07:27:02.377712 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m07:27:02.383671 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m07:27:02.430891 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m07:27:02.494749 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m07:27:02.517257 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:27:02.528602 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m07:27:02.538848 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m07:27:02.599479 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m07:27:02.607295 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m07:27:02.613992 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:27:03.202711 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6d84-143a-9f49-88f956c31906) - Created
[0m07:27:03.855258 [debug] [Thread-5 (]: SQL status: OK in 6.590 seconds
[0m07:27:03.862172 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec63-6a40-1090-b943-eb753944fca2, command-id=01f0ec63-6a59-1509-b331-a7c663c1f810) - Closing
[0m07:27:03.937922 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m07:27:03.943437 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m07:27:04.565166 [debug] [Thread-4 (]: SQL status: OK in 3.110 seconds
[0m07:27:04.578988 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-6cc0-1e75-b1f4-931fe84fd132, command-id=01f0ec63-6cd2-12e6-b002-63748cc10d12) - Closing
[0m07:27:04.583199 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m07:27:04.588186 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6cc0-1e75-b1f4-931fe84fd132) - Closing
[0m07:27:04.710670 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 3.37s]
[0m07:27:04.714424 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m07:27:04.717250 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:27:04.721222 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m07:27:04.726160 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m07:27:04.728932 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m07:27:04.731637 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:27:04.750809 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:27:04.779629 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:27:04.790371 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:27:04.819772 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m07:27:04.823246 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m07:27:04.826169 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:05.365297 [debug] [Thread-7 (]: SQL status: OK in 2.750 seconds
[0m07:27:05.377711 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec63-6d84-143a-9f49-88f956c31906, command-id=01f0ec63-6d97-1caf-8882-fb9cf2530636) - Closing
[0m07:27:05.384868 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6ed0-103a-a38a-bcfd2ebb546f) - Created
[0m07:27:05.389581 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:27:05.413890 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m07:27:05.423352 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6d84-143a-9f49-88f956c31906) - Closing
[0m07:27:05.575494 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767debe8460>]}
[0m07:27:05.579167 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 3.21s]
[0m07:27:05.582932 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m07:27:05.586901 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m07:27:05.589284 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m07:27:05.591646 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m07:27:05.593755 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m07:27:05.595646 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m07:27:05.614512 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m07:27:05.633358 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m07:27:05.640884 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m07:27:05.644093 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m07:27:05.647404 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m07:27:05.667117 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m07:27:05.669865 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m07:27:05.672538 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:27:06.151772 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6f3e-13b7-b6f5-75f17643b849) - Created
[0m07:27:07.232082 [debug] [Thread-4 (]: SQL status: OK in 2.410 seconds
[0m07:27:07.244977 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-6ed0-103a-a38a-bcfd2ebb546f, command-id=01f0ec63-6ee4-16f6-967b-8c4b5cd9cc51) - Closing
[0m07:27:07.251644 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m07:27:07.258586 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-6ed0-103a-a38a-bcfd2ebb546f) - Closing
[0m07:27:07.409783 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 2.68s]
[0m07:27:07.415360 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m07:27:07.422893 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:27:07.428919 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m07:27:07.434718 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m07:27:07.440322 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m07:27:07.445291 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:27:07.487362 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:27:07.509891 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:27:07.519338 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:27:07.542368 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m07:27:07.544965 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m07:27:07.547143 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:07.971736 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-705c-1835-a6b5-487e3a8abbb4) - Created
[0m07:27:08.022062 [debug] [Thread-7 (]: SQL status: OK in 2.350 seconds
[0m07:27:08.029946 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec63-6f3e-13b7-b6f5-75f17643b849, command-id=01f0ec63-6f5a-1079-8dd0-12a8d799ccbb) - Closing
[0m07:27:08.037938 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:27:08.042848 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m07:27:08.045840 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-6f3e-13b7-b6f5-75f17643b849) - Closing
[0m07:27:08.181078 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b655cac0>]}
[0m07:27:08.187555 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.59s]
[0m07:27:08.191475 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m07:27:08.197722 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m07:27:08.201476 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m07:27:08.205778 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m07:27:08.208174 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m07:27:08.210355 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m07:27:08.224126 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m07:27:08.243882 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m07:27:08.278700 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m07:27:08.305032 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m07:27:08.307746 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767b43d7be0>]}
[0m07:27:08.356720 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m07:27:08.381319 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m07:27:08.384646 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m07:27:08.387576 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m07:27:08.846768 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-70e1-1b1d-9cbc-b6ed74570d16) - Created
[0m07:27:10.395961 [debug] [Thread-5 (]: SQL status: OK in 6.450 seconds
[0m07:27:10.398282 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec63-6a40-1090-b943-eb753944fca2, command-id=01f0ec63-6e06-1df6-928b-40fd20b65a4c) - Closing
[0m07:27:10.606746 [debug] [Thread-4 (]: SQL status: OK in 3.060 seconds
[0m07:27:10.616696 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-705c-1835-a6b5-487e3a8abbb4, command-id=01f0ec63-7073-1a0d-980c-d162bf929c7d) - Closing
[0m07:27:10.622077 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m07:27:10.626057 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-705c-1835-a6b5-487e3a8abbb4) - Closing
[0m07:27:10.656398 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m07:27:10.746327 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m07:27:10.750467 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec63-6a40-1090-b943-eb753944fca2) - Closing
[0m07:27:10.763143 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 3.33s]
[0m07:27:10.768587 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m07:27:10.771916 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:27:10.775413 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m07:27:10.779789 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m07:27:10.784421 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m07:27:10.788710 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:27:10.826377 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:27:10.866397 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:27:10.880787 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:27:10.885038 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc46ad1930>]}
[0m07:27:10.890576 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mCREATE 7[0m in 13.81s]
[0m07:27:10.895502 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m07:27:10.901388 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m07:27:10.906115 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m07:27:10.910498 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m07:27:10.914599 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m07:27:10.919071 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m07:27:10.919533 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m07:27:10.922517 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m07:27:10.923413 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m07:27:10.926933 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:10.957690 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:27:10.960931 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m07:27:10.964494 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m07:27:11.430078 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-726b-102b-9055-2cf082be6e05) - Created
[0m07:27:11.430653 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec63-726b-10e7-b720-b8ef25d159c6) - Created
[0m07:27:12.652535 [debug] [Thread-4 (]: SQL status: OK in 1.730 seconds
[0m07:27:12.666206 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-726b-102b-9055-2cf082be6e05, command-id=01f0ec63-727e-1eb9-bd1e-4174c2a6371e) - Closing
[0m07:27:12.670374 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m07:27:12.673131 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-726b-102b-9055-2cf082be6e05) - Closing
[0m07:27:12.809470 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 2.03s]
[0m07:27:12.813959 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m07:27:12.818302 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:27:12.822338 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m07:27:12.826291 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m07:27:12.829598 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m07:27:12.833439 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:27:12.858318 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:27:12.895666 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:27:12.908993 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:27:12.946290 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m07:27:12.950974 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m07:27:12.954751 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:13.460583 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-73a1-1720-991c-91af9dedf9c8) - Created
[0m07:27:14.297935 [debug] [Thread-5 (]: SQL status: OK in 3.330 seconds
[0m07:27:14.312216 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec63-726b-10e7-b720-b8ef25d159c6, command-id=01f0ec63-727f-1833-b138-c4d1c52ded51) - Closing
[0m07:27:14.364528 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m07:27:14.377370 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m07:27:15.698202 [debug] [Thread-4 (]: SQL status: OK in 2.740 seconds
[0m07:27:15.706671 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-73a1-1720-991c-91af9dedf9c8, command-id=01f0ec63-73b4-17ec-8623-4bbefb71918e) - Closing
[0m07:27:15.711105 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m07:27:15.713897 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-73a1-1720-991c-91af9dedf9c8) - Closing
[0m07:27:15.845269 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 3.02s]
[0m07:27:15.847921 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m07:27:15.851074 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:27:15.853871 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m07:27:15.857181 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m07:27:15.859649 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m07:27:15.862073 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:27:15.883935 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:27:15.908212 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:27:15.919953 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:27:15.943832 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m07:27:15.946298 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:27:15.948835 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:16.462737 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-756b-132e-b6be-5f237a3c3c31) - Created
[0m07:27:17.030791 [debug] [Thread-7 (]: SQL status: OK in 8.640 seconds
[0m07:27:17.039537 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec63-70e1-1b1d-9cbc-b6ed74570d16, command-id=01f0ec63-70f6-14f7-873c-cf95aa48bd10) - Closing
[0m07:27:17.207862 [debug] [Thread-7 (]: Applying tags to relation None
[0m07:27:17.270135 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m07:27:17.276988 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec63-70e1-1b1d-9cbc-b6ed74570d16) - Closing
[0m07:27:17.409415 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aabe02f8-8d1b-4eca-a61e-b262ee402eb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767debe8460>]}
[0m07:27:17.413759 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 9.20s]
[0m07:27:17.419135 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m07:27:17.426121 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:27:17.429391 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:27:17.433192 [info ] [MainThread]: 
[0m07:27:17.437246 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 50.74 seconds (50.74s).
[0m07:27:17.443543 [debug] [MainThread]: Command end result
[0m07:27:17.687048 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:27:17.706899 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:27:17.742038 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:27:17.745217 [info ] [MainThread]: 
[0m07:27:17.748981 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:27:17.753468 [info ] [MainThread]: 
[0m07:27:17.758471 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m07:27:17.764923 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 58.205917, "process_in_blocks": "0", "process_kernel_time": 1.282589, "process_mem_max_rss": "245684", "process_out_blocks": "496", "process_user_time": 12.140238}
[0m07:27:17.772519 [debug] [MainThread]: Command `dbt run` succeeded at 07:27:17.771806 after 58.21 seconds
[0m07:27:17.777001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dd6d2290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dc68f730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7767dd60ce20>]}
[0m07:27:17.782537 [debug] [MainThread]: Flushing usage events
[0m07:27:17.857442 [debug] [Thread-4 (]: SQL status: OK in 1.910 seconds
[0m07:27:17.864735 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-756b-132e-b6be-5f237a3c3c31, command-id=01f0ec63-757e-1897-93e9-a426c7a2d3b0) - Closing
[0m07:27:17.869525 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m07:27:17.873223 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-756b-132e-b6be-5f237a3c3c31) - Closing
[0m07:27:18.011502 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 2.15s]
[0m07:27:18.015649 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m07:27:18.020632 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:27:18.026648 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m07:27:18.033166 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m07:27:18.038979 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m07:27:18.043836 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:27:18.080299 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:27:18.119327 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:27:18.130090 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:27:18.157326 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m07:27:18.160124 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m07:27:18.162569 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:18.267580 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:27:18.640627 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-76b8-1870-8f5d-e9dfae7a6199) - Created
[0m07:27:19.623644 [debug] [Thread-5 (]: SQL status: OK in 5.230 seconds
[0m07:27:19.626002 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec63-726b-10e7-b720-b8ef25d159c6, command-id=01f0ec63-7442-1a41-a274-3acfdbb5fc29) - Closing
[0m07:27:19.629076 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m07:27:19.654176 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m07:27:19.656707 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec63-726b-10e7-b720-b8ef25d159c6) - Closing
[0m07:27:19.782973 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8fe3b5d6-3c9e-4adb-baf3-745f082d605a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc44ed2ef0>]}
[0m07:27:19.787312 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mCREATE 6[0m in 8.87s]
[0m07:27:19.791143 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m07:27:19.797279 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:27:19.799414 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:27:19.802383 [info ] [MainThread]: 
[0m07:27:19.805161 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 53.06 seconds (53.06s).
[0m07:27:19.809523 [debug] [MainThread]: Command end result
[0m07:27:19.963668 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:27:19.978085 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:27:20.003572 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:27:20.005820 [info ] [MainThread]: 
[0m07:27:20.008378 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:27:20.011027 [info ] [MainThread]: 
[0m07:27:20.013636 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m07:27:20.017124 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 60.459164, "process_in_blocks": "0", "process_kernel_time": 1.251238, "process_mem_max_rss": "246472", "process_out_blocks": "536", "process_user_time": 11.431767}
[0m07:27:20.020537 [debug] [MainThread]: Command `dbt seed` succeeded at 07:27:20.020248 after 60.46 seconds
[0m07:27:20.022952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc44d66470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43efed40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cdc43d837c0>]}
[0m07:27:20.025365 [debug] [MainThread]: Flushing usage events
[0m07:27:20.242033 [debug] [Thread-4 (]: SQL status: OK in 2.080 seconds
[0m07:27:20.250420 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-76b8-1870-8f5d-e9dfae7a6199, command-id=01f0ec63-76c9-11ff-8304-cdfe601d8652) - Closing
[0m07:27:20.255398 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m07:27:20.259315 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-76b8-1870-8f5d-e9dfae7a6199) - Closing
[0m07:27:20.408129 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:27:20.410313 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.38s]
[0m07:27:20.414694 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m07:27:20.419051 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:27:20.423609 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m07:27:20.427755 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m07:27:20.431434 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m07:27:20.436016 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:27:20.440363 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:27:20.581339 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:20.584348 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:27:20.587108 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:20.993070 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c) - Created
[0m07:27:21.587963 [debug] [Thread-4 (]: SQL status: OK in 1.000 seconds
[0m07:27:21.602532 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c, command-id=01f0ec63-7830-1d02-a4e5-4ee6627669a6) - Closing
[0m07:27:21.690075 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:21.740806 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:21.885812 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:21.888191 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:27:22.376518 [debug] [Thread-4 (]: SQL status: OK in 0.490 seconds
[0m07:27:22.382917 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c, command-id=01f0ec63-78b8-1bcd-bea8-df5acc4535b4) - Closing
[0m07:27:22.396005 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:22.399507 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m07:27:22.799658 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m07:27:22.813798 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c, command-id=01f0ec63-7907-16a6-ad4b-b1c185b149fc) - Closing
[0m07:27:22.913550 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:22.999416 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:23.008023 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:27:23.875213 [debug] [Thread-4 (]: SQL status: OK in 0.860 seconds
[0m07:27:23.933659 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c, command-id=01f0ec63-7965-1b1e-9ef4-73257b8a1347) - Closing
[0m07:27:23.955618 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:27:23.971933 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m07:27:23.975068 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m07:27:24.334991 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m07:27:24.347384 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c, command-id=01f0ec63-79f8-1311-a4a3-ba74202153bd) - Closing
[0m07:27:24.376225 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m07:27:24.379687 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-781f-1e58-9cca-8e6d562c390c) - Closing
[0m07:27:24.521716 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 4.09s]
[0m07:27:24.526530 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m07:27:24.532364 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:27:24.537724 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m07:27:24.543058 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m07:27:24.546525 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m07:27:24.550199 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:27:24.553242 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:27:24.586104 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:24.588487 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:27:24.590369 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:25.050874 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-7a88-1988-8288-a708359ef42f) - Created
[0m07:27:25.573732 [debug] [Thread-4 (]: SQL status: OK in 0.980 seconds
[0m07:27:25.593694 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7a88-1988-8288-a708359ef42f, command-id=01f0ec63-7a9c-1cd3-9250-6bf56955ea87) - Closing
[0m07:27:25.630301 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:25.763561 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:25.812650 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:25.815735 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:27:26.371255 [debug] [Thread-4 (]: SQL status: OK in 0.550 seconds
[0m07:27:26.380795 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7a88-1988-8288-a708359ef42f, command-id=01f0ec63-7b10-1fb0-912c-4c577fca5e1e) - Closing
[0m07:27:26.387965 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:26.391479 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m07:27:26.780735 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m07:27:26.789392 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7a88-1988-8288-a708359ef42f, command-id=01f0ec63-7b68-17e8-9824-0af840a21d85) - Closing
[0m07:27:26.804882 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:26.850432 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:26.854607 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:27:27.213932 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m07:27:27.227561 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7a88-1988-8288-a708359ef42f, command-id=01f0ec63-7baf-1b8e-b5ca-b1c9fb9a8fbd) - Closing
[0m07:27:27.247101 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:27:27.250472 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m07:27:27.253787 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m07:27:27.599929 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m07:27:27.603741 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7a88-1988-8288-a708359ef42f, command-id=01f0ec63-7bec-131a-8754-9a8494b23172) - Closing
[0m07:27:27.610173 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m07:27:27.612858 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-7a88-1988-8288-a708359ef42f) - Closing
[0m07:27:27.760124 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 3.22s]
[0m07:27:27.767808 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m07:27:27.775564 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:27:27.781386 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m07:27:27.788850 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m07:27:27.793146 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m07:27:27.797537 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:27:27.802300 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:27:27.841549 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:27.844523 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m07:27:27.846988 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m07:27:28.376403 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e) - Created
[0m07:27:28.845282 [debug] [Thread-4 (]: SQL status: OK in 1.000 seconds
[0m07:27:28.860064 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7c98-18e6-97ac-cae0e612f770) - Closing
[0m07:27:28.881467 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:29.034970 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:29.101076 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:29.104898 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m07:27:29.566871 [debug] [Thread-4 (]: SQL status: OK in 0.460 seconds
[0m07:27:29.582115 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7d09-139d-8b31-98f68aa15075) - Closing
[0m07:27:29.595376 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:29.602261 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m07:27:29.963300 [debug] [Thread-4 (]: SQL status: OK in 0.350 seconds
[0m07:27:29.975392 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7d52-1aac-9928-451b92e84515) - Closing
[0m07:27:30.001827 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:30.012410 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m07:27:30.375672 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m07:27:30.391861 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7d92-1e1d-b74b-4e9411252c45) - Closing
[0m07:27:30.404095 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:30.492068 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:30.500048 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m07:27:30.883662 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m07:27:30.893155 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7ddd-1bd6-8e7b-4af58e5faca7) - Closing
[0m07:27:30.905804 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m07:27:30.910167 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m07:27:30.913657 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m07:27:31.198101 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m07:27:31.211800 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e, command-id=01f0ec63-7e19-1fd6-bb0a-8efc398f5910) - Closing
[0m07:27:31.234701 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m07:27:31.242990 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec63-7c86-192a-8e47-8badbac1c12e) - Closing
[0m07:27:31.400056 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 3.61s]
[0m07:27:31.410835 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m07:27:31.426306 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m07:27:31.432917 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m07:27:31.441549 [info ] [MainThread]: 
[0m07:27:31.449411 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 1 minutes and 4.75 seconds (64.75s).
[0m07:27:31.468738 [debug] [MainThread]: Command end result
[0m07:27:32.149162 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m07:27:32.175433 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m07:27:32.234324 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m07:27:32.239071 [info ] [MainThread]: 
[0m07:27:32.243668 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m07:27:32.248067 [info ] [MainThread]: 
[0m07:27:32.252600 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m07:27:32.257743 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m07:27:32.261900 [info ] [MainThread]: 
[0m07:27:32.266370 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m07:27:32.270373 [info ] [MainThread]: 
[0m07:27:32.275727 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m07:27:32.282744 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 72.72939, "process_in_blocks": "0", "process_kernel_time": 1.457618, "process_mem_max_rss": "256048", "process_out_blocks": "568", "process_user_time": 14.668796}
[0m07:27:32.287972 [debug] [MainThread]: Command `dbt test` failed at 07:27:32.287069 after 72.73 seconds
[0m07:27:32.292687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739cd40262c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739c86229e70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x739c86228670>]}
[0m07:27:32.296242 [debug] [MainThread]: Flushing usage events
[0m07:27:32.690791 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:02:37.666159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995dba2350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995c9e4280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995c9e4220>]}


============================== 08:02:37.678582 | 47b67d8a-d412-4825-a8f3-1e909face96a ==============================
[0m08:02:37.678582 [info ] [MainThread]: Running with dbt=1.10.18
[0m08:02:37.680502 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'log_path': '/opt/dagster/app/dbt/logs', 'empty': 'None', 'quiet': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt seed --profiles-dir . --target dev', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'profiles_dir': '.', 'no_print': 'None', 'printer_width': '80', 'fail_fast': 'False', 'use_colors': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'version_check': 'True'}
[0m08:02:38.890072 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:02:38.891953 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:02:38.893546 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:02:40.319406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769937499180>]}
[0m08:02:40.464914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995d297640>]}
[0m08:02:40.467008 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m08:02:40.665311 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m08:02:41.196221 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:02:41.198193 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:02:41.215558 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m08:02:41.306972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769936a9ca90>]}
[0m08:02:41.554825 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:02:41.565441 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:02:41.596192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769936ac3c10>]}
[0m08:02:41.598203 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m08:02:41.599931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769936ac1420>]}
[0m08:02:41.606058 [info ] [MainThread]: 
[0m08:02:41.609628 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:02:41.611588 [info ] [MainThread]: 
[0m08:02:41.613564 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:02:41.615191 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:02:41.628463 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m08:02:41.630176 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:02:41.647714 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:02:41.649602 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m08:02:41.651560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:02:42.488543 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-688a-10fc-ba9e-547066213ca1) - Created
[0m08:02:58.541977 [debug] [ThreadPool]: SQL status: OK in 16.890 seconds
[0m08:02:58.550400 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-688a-10fc-ba9e-547066213ca1, command-id=01f0ec68-68b8-170f-a6ad-1aa8065051bc) - Closing
[0m08:02:58.782676 [debug] [ThreadPool]: On list_workspace: Close
[0m08:02:58.784375 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-688a-10fc-ba9e-547066213ca1) - Closing
[0m08:02:58.946497 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m08:02:58.948090 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m08:02:58.957643 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m08:02:58.959458 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m08:02:58.961051 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:02:59.503644 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-72c3-1c82-b483-9346a2d4fc13) - Created
[0m08:03:03.380518 [debug] [ThreadPool]: SQL status: OK in 4.420 seconds
[0m08:03:03.385648 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-72c3-1c82-b483-9346a2d4fc13, command-id=01f0ec68-72da-1a8e-8696-2ddd1da0cb16) - Closing
[0m08:03:03.387942 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m08:03:03.389873 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-72c3-1c82-b483-9346a2d4fc13) - Closing
[0m08:03:03.518618 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m08:03:03.520637 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m08:03:03.532271 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m08:03:03.534112 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m08:03:03.536153 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:03.995511 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-7572-19f2-bc3c-8f0d36934caf) - Created
[0m08:03:05.116644 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m08:03:05.123529 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-7572-19f2-bc3c-8f0d36934caf, command-id=01f0ec68-7589-18e8-af44-cd7ed01b9ced) - Closing
[0m08:03:05.126258 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m08:03:05.128225 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-7572-19f2-bc3c-8f0d36934caf) - Closing
[0m08:03:05.299041 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m08:03:05.301082 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m08:03:05.306929 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m08:03:05.308309 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m08:03:05.309636 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:05.713545 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-7679-1cd6-b363-fea5b5d5e2e5) - Created
[0m08:03:06.453571 [debug] [ThreadPool]: SQL status: OK in 1.140 seconds
[0m08:03:06.459880 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-7679-1cd6-b363-fea5b5d5e2e5, command-id=01f0ec68-768c-1df4-82d4-5b575613c784) - Closing
[0m08:03:06.462700 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m08:03:06.464747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-7679-1cd6-b363-fea5b5d5e2e5) - Closing
[0m08:03:06.628189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995de98310>]}
[0m08:03:06.651077 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m08:03:06.655391 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m08:03:06.659085 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m08:03:06.661947 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m08:03:06.665642 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m08:03:06.667965 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m08:03:06.683296 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m08:03:06.685218 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769936a21c00>]}
[0m08:03:06.735697 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m08:03:06.737645 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769936a44730>]}
[0m08:03:06.765620 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m08:03:06.767537 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m08:03:06.769619 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m08:03:07.245650 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec68-7760-19b1-a6a1-c1d973b26818) - Created
[0m08:03:11.955695 [debug] [Thread-5 (]: SQL status: OK in 5.190 seconds
[0m08:03:11.959753 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec68-7760-19b1-a6a1-c1d973b26818, command-id=01f0ec68-7775-13e6-bfd0-27b554b3530c) - Closing
[0m08:03:11.982782 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m08:03:11.984441 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m08:03:16.870510 [debug] [Thread-5 (]: SQL status: OK in 4.880 seconds
[0m08:03:16.873899 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec68-7760-19b1-a6a1-c1d973b26818, command-id=01f0ec68-7a4b-11d0-91c7-938d8328fc06) - Closing
[0m08:03:16.885829 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m08:03:16.909584 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m08:03:16.911218 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec68-7760-19b1-a6a1-c1d973b26818) - Closing
[0m08:03:17.048608 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995f8f5c00>]}
[0m08:03:17.050617 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 10.39s]
[0m08:03:17.053071 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m08:03:17.055498 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m08:03:17.058305 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m08:03:17.062453 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m08:03:17.064373 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m08:03:17.066274 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m08:03:17.069118 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m08:03:17.086275 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m08:03:17.088290 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m08:03:17.089665 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m08:03:17.509228 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec68-7d80-1788-a2c1-d5168fa0e48a) - Created
[0m08:03:19.891962 [debug] [Thread-5 (]: SQL status: OK in 2.800 seconds
[0m08:03:19.897412 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec68-7d80-1788-a2c1-d5168fa0e48a, command-id=01f0ec68-7d95-133b-a2f4-e0f5a5e65db0) - Closing
[0m08:03:19.904444 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m08:03:19.906609 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m08:03:22.915855 [debug] [Thread-5 (]: SQL status: OK in 3.010 seconds
[0m08:03:22.917461 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ec68-7d80-1788-a2c1-d5168fa0e48a, command-id=01f0ec68-7f02-127e-a5c4-df3e9115a776) - Closing
[0m08:03:22.919250 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m08:03:22.931383 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m08:03:22.932636 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ec68-7d80-1788-a2c1-d5168fa0e48a) - Closing
[0m08:03:23.071742 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47b67d8a-d412-4825-a8f3-1e909face96a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7699373eafe0>]}
[0m08:03:23.074705 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 6.01s]
[0m08:03:23.077603 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m08:03:23.084330 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:03:23.086288 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:03:23.089567 [info ] [MainThread]: 
[0m08:03:23.092427 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 41.48 seconds (41.48s).
[0m08:03:23.096854 [debug] [MainThread]: Command end result
[0m08:03:23.210755 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:03:23.216842 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:03:23.228893 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m08:03:23.229934 [info ] [MainThread]: 
[0m08:03:23.231278 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:03:23.233027 [info ] [MainThread]: 
[0m08:03:23.234815 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m08:03:23.238170 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 45.710358, "process_in_blocks": "0", "process_kernel_time": 1.118173, "process_mem_max_rss": "245348", "process_out_blocks": "0", "process_user_time": 5.902357}
[0m08:03:23.239718 [debug] [MainThread]: Command `dbt seed` succeeded at 08:03:23.239555 after 45.71 seconds
[0m08:03:23.240801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995dba2350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995d297640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76995d91a9e0>]}
[0m08:03:23.241832 [debug] [MainThread]: Flushing usage events
[0m08:03:23.682384 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:03:42.546997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786949fe6380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786948dd4280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786948dd4220>]}


============================== 08:03:42.550591 | 30f246fe-f389-4abc-a201-b29b8b83a31e ==============================
[0m08:03:42.550591 [info ] [MainThread]: Running with dbt=1.10.18
[0m08:03:42.552048 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '.', 'log_cache_events': 'False', 'printer_width': '80', 'empty': 'False', 'invocation_command': 'dbt run --profiles-dir . --target dev', 'use_colors': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'no_print': 'None', 'version_check': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True'}
[0m08:03:43.387958 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:03:43.389651 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:03:43.391446 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:03:44.294582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786923a7d180>]}
[0m08:03:44.393418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786946de4b80>]}
[0m08:03:44.395392 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m08:03:44.589269 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m08:03:45.008925 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:03:45.010542 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:03:45.022259 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m08:03:45.099668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786949538a90>]}
[0m08:03:45.315838 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:03:45.324834 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:03:45.346870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78694953b3d0>]}
[0m08:03:45.348530 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m08:03:45.350039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78694953b160>]}
[0m08:03:45.354149 [info ] [MainThread]: 
[0m08:03:45.356642 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:03:45.359002 [info ] [MainThread]: 
[0m08:03:45.360867 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:03:45.362403 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:03:45.372980 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m08:03:45.375042 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:03:45.391529 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:03:45.393376 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m08:03:45.394930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:45.908673 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8e6f-111d-8e43-e0b14543250d) - Created
[0m08:03:46.322020 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m08:03:46.334330 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-8e6f-111d-8e43-e0b14543250d, command-id=01f0ec68-8e82-11e6-8cef-6d2a2ab8acc5) - Closing
[0m08:03:46.336731 [debug] [ThreadPool]: On list_workspace: Close
[0m08:03:46.338661 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8e6f-111d-8e43-e0b14543250d) - Closing
[0m08:03:46.493102 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m08:03:46.499590 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:03:46.511121 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:03:46.513765 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m08:03:46.516156 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:46.939604 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8f0d-1dbc-a223-37e8edb1ffc1) - Created
[0m08:03:47.338241 [debug] [ThreadPool]: SQL status: OK in 0.820 seconds
[0m08:03:47.341742 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-8f0d-1dbc-a223-37e8edb1ffc1, command-id=01f0ec68-8f1e-1d9d-862a-4aa068a2db47) - Closing
[0m08:03:47.343594 [debug] [ThreadPool]: On list_workspace: Close
[0m08:03:47.344915 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8f0d-1dbc-a223-37e8edb1ffc1) - Closing
[0m08:03:47.485805 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m08:03:47.492026 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:03:47.511160 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:03:47.513369 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m08:03:47.515320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:47.904003 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8fa1-1989-9952-79a3957e2f13) - Created
[0m08:03:48.316666 [debug] [ThreadPool]: SQL status: OK in 0.800 seconds
[0m08:03:48.320138 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-8fa1-1989-9952-79a3957e2f13, command-id=01f0ec68-8fb1-18f3-94c5-f3cae7433ae7) - Closing
[0m08:03:48.322146 [debug] [ThreadPool]: On list_workspace: Close
[0m08:03:48.323744 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-8fa1-1989-9952-79a3957e2f13) - Closing
[0m08:03:48.459522 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m08:03:48.461287 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m08:03:48.473896 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m08:03:48.475560 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m08:03:48.476915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:48.845125 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-9032-138b-add5-b0349e0347e7) - Created
[0m08:03:49.349223 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m08:03:49.354057 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-9032-138b-add5-b0349e0347e7, command-id=01f0ec68-9041-15b9-85b0-637292200846) - Closing
[0m08:03:49.356116 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m08:03:49.357805 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-9032-138b-add5-b0349e0347e7) - Closing
[0m08:03:49.518388 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m08:03:49.523120 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m08:03:49.532372 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m08:03:49.534686 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m08:03:49.536809 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:49.919166 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-90d4-1c04-90da-1188038ed19b) - Created
[0m08:03:50.386914 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m08:03:50.390512 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-90d4-1c04-90da-1188038ed19b, command-id=01f0ec68-90e5-1364-9fe5-bf1e73bb1d04) - Closing
[0m08:03:50.392151 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m08:03:50.393347 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-90d4-1c04-90da-1188038ed19b) - Closing
[0m08:03:50.531602 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m08:03:50.533505 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m08:03:50.539807 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m08:03:50.541455 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m08:03:50.542926 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:03:50.930174 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-916f-157c-b1f3-6ebad51df681) - Created
[0m08:03:51.507484 [debug] [ThreadPool]: SQL status: OK in 0.960 seconds
[0m08:03:51.515501 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-916f-157c-b1f3-6ebad51df681, command-id=01f0ec68-917f-148e-bead-7704ff9b1315) - Closing
[0m08:03:51.518515 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m08:03:51.520658 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-916f-157c-b1f3-6ebad51df681) - Closing
[0m08:03:51.691909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786946dba590>]}
[0m08:03:51.702518 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m08:03:51.705403 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m08:03:51.708117 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m08:03:51.710273 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m08:03:51.712367 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m08:03:51.728644 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m08:03:51.741244 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m08:03:51.762730 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m08:03:51.765999 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m08:03:51.768019 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786922647dc0>]}
[0m08:03:51.785852 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m08:03:51.798052 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m08:03:51.810813 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m08:03:51.812089 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m08:03:51.813220 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:03:52.240397 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9235-1ede-a877-db80759532ea) - Created
[0m08:03:53.266140 [debug] [Thread-7 (]: SQL status: OK in 1.450 seconds
[0m08:03:53.275556 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec68-9235-1ede-a877-db80759532ea, command-id=01f0ec68-9247-17f9-9e1f-b12033bfd025) - Closing
[0m08:03:53.309350 [debug] [Thread-7 (]: Applying tags to relation None
[0m08:03:53.313980 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m08:03:53.315751 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9235-1ede-a877-db80759532ea) - Closing
[0m08:03:53.451672 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786922614490>]}
[0m08:03:53.453510 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.74s]
[0m08:03:53.455269 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m08:03:53.457018 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m08:03:53.459231 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m08:03:53.461971 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m08:03:53.463652 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m08:03:53.464975 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m08:03:53.470161 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m08:03:53.486292 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m08:03:53.491273 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m08:03:53.493787 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m08:03:53.495547 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m08:03:53.507893 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m08:03:53.509145 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m08:03:53.510402 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:03:53.906951 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9333-1cb5-8ffd-14ea584e5f7c) - Created
[0m08:03:54.713809 [debug] [Thread-7 (]: SQL status: OK in 1.200 seconds
[0m08:03:54.716067 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec68-9333-1cb5-8ffd-14ea584e5f7c, command-id=01f0ec68-9346-1d0d-8390-c947efc0f074) - Closing
[0m08:03:54.718008 [debug] [Thread-7 (]: Applying tags to relation None
[0m08:03:54.720307 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m08:03:54.721729 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9333-1cb5-8ffd-14ea584e5f7c) - Closing
[0m08:03:54.856907 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7869238f6ad0>]}
[0m08:03:54.860279 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.39s]
[0m08:03:54.862542 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m08:03:54.865537 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m08:03:54.867989 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m08:03:54.870615 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m08:03:54.872130 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m08:03:54.874446 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m08:03:54.888166 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m08:03:54.903612 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m08:03:54.922050 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m08:03:54.928083 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m08:03:54.932512 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m08:03:54.968155 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m08:03:54.972095 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m08:03:54.975585 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:03:55.383566 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9415-11a5-ae78-51f667fc2cbc) - Created
[0m08:03:56.449964 [debug] [Thread-7 (]: SQL status: OK in 1.470 seconds
[0m08:03:56.459899 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec68-9415-11a5-ae78-51f667fc2cbc, command-id=01f0ec68-9428-1c6e-b107-a18d268b15f7) - Closing
[0m08:03:56.464082 [debug] [Thread-7 (]: Applying tags to relation None
[0m08:03:56.468014 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m08:03:56.469883 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9415-11a5-ae78-51f667fc2cbc) - Closing
[0m08:03:56.622765 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786922ef8790>]}
[0m08:03:56.628147 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.75s]
[0m08:03:56.632863 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m08:03:56.637759 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m08:03:56.640493 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m08:03:56.643161 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m08:03:56.645122 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m08:03:56.647092 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m08:03:56.658441 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m08:03:56.678387 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m08:03:56.717628 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m08:03:56.737245 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m08:03:56.739237 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7869204ec910>]}
[0m08:03:56.779143 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m08:03:56.788815 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m08:03:56.790250 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m08:03:56.791599 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:03:57.229984 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9530-1bfb-bc30-ad7f8a1469d5) - Created
[0m08:04:02.305535 [debug] [Thread-7 (]: SQL status: OK in 5.510 seconds
[0m08:04:02.307932 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ec68-9530-1bfb-bc30-ad7f8a1469d5, command-id=01f0ec68-9540-1c3a-9399-1c673fc872c2) - Closing
[0m08:04:02.312715 [debug] [Thread-7 (]: Applying tags to relation None
[0m08:04:02.331496 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m08:04:02.333160 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ec68-9530-1bfb-bc30-ad7f8a1469d5) - Closing
[0m08:04:02.466550 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30f246fe-f389-4abc-a201-b29b8b83a31e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7869236df9d0>]}
[0m08:04:02.474352 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 5.82s]
[0m08:04:02.480045 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m08:04:02.484791 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:04:02.486793 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:04:02.488791 [info ] [MainThread]: 
[0m08:04:02.490234 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 17.13 seconds (17.13s).
[0m08:04:02.494180 [debug] [MainThread]: Command end result
[0m08:04:02.585937 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:04:02.595483 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:04:02.611070 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m08:04:02.612903 [info ] [MainThread]: 
[0m08:04:02.614522 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:04:02.616017 [info ] [MainThread]: 
[0m08:04:02.617330 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m08:04:02.620044 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 20.174412, "process_in_blocks": "0", "process_kernel_time": 0.531038, "process_mem_max_rss": "246136", "process_out_blocks": "0", "process_user_time": 5.801498}
[0m08:04:02.621816 [debug] [MainThread]: Command `dbt run` succeeded at 08:04:02.621629 after 20.18 seconds
[0m08:04:02.623501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786949fe6380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786946db9ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786949f2a590>]}
[0m08:04:02.626045 [debug] [MainThread]: Flushing usage events
[0m08:04:03.107587 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:04:17.111423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3696ece320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3695d00280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3695d00220>]}


============================== 08:04:17.116394 | ff3e00ef-97d3-40b6-b450-69cc46f804df ==============================
[0m08:04:17.116394 [info ] [MainThread]: Running with dbt=1.10.18
[0m08:04:17.117961 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/dagster/app/dbt/logs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'debug': 'False', 'partial_parse': 'True', 'log_format': 'default', 'empty': 'None', 'static_parser': 'True', 'no_print': 'None', 'use_colors': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'write_json': 'True', 'warn_error': 'None', 'quiet': 'False', 'profiles_dir': '.', 'use_experimental_parser': 'False', 'invocation_command': 'dbt test --profiles-dir . --target dev'}
[0m08:04:17.914545 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:04:17.916423 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:04:17.917990 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:04:18.878731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3695d00580>]}
[0m08:04:18.978667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3693ceb2e0>]}
[0m08:04:18.980547 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m08:04:19.138136 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m08:04:19.514942 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:04:19.516735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:04:19.527978 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m08:04:19.612007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f36742d4b80>]}
[0m08:04:19.834781 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:04:19.843714 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:04:19.874243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f36963f7880>]}
[0m08:04:19.875874 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m08:04:19.876977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f36964109a0>]}
[0m08:04:19.880717 [info ] [MainThread]: 
[0m08:04:19.882714 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:04:19.884982 [info ] [MainThread]: 
[0m08:04:19.886907 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:04:19.888377 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:04:19.901054 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m08:04:19.902894 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m08:04:19.920412 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m08:04:19.922225 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m08:04:19.924362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:04:20.324178 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a2f3-19d7-91d1-1c8e29f3c771) - Created
[0m08:04:20.799788 [debug] [ThreadPool]: SQL status: OK in 0.880 seconds
[0m08:04:20.805927 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-a2f3-19d7-91d1-1c8e29f3c771, command-id=01f0ec68-a305-1c0f-a423-d82959f41470) - Closing
[0m08:04:20.807934 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m08:04:20.809406 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a2f3-19d7-91d1-1c8e29f3c771) - Closing
[0m08:04:20.947138 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m08:04:20.948963 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m08:04:20.953330 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m08:04:20.954914 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m08:04:20.956466 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:04:21.368321 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a392-1fcf-9f2c-98ad8cffade1) - Created
[0m08:04:21.960252 [debug] [ThreadPool]: SQL status: OK in 1.000 seconds
[0m08:04:21.963515 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-a392-1fcf-9f2c-98ad8cffade1, command-id=01f0ec68-a3a5-14a3-83b5-8109bdc351bf) - Closing
[0m08:04:21.965351 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m08:04:21.966751 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a392-1fcf-9f2c-98ad8cffade1) - Closing
[0m08:04:22.092350 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m08:04:22.094631 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m08:04:22.101754 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m08:04:22.103610 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m08:04:22.105233 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:04:22.488449 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a43e-1975-913e-ac0d2c185934) - Created
[0m08:04:23.039808 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m08:04:23.048289 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ec68-a43e-1975-913e-ac0d2c185934, command-id=01f0ec68-a44f-194b-800d-7b4bd0c9d355) - Closing
[0m08:04:23.051768 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m08:04:23.054896 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ec68-a43e-1975-913e-ac0d2c185934) - Closing
[0m08:04:23.182178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff3e00ef-97d3-40b6-b450-69cc46f804df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3693cca560>]}
[0m08:04:23.189056 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m08:04:23.191412 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m08:04:23.196369 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m08:04:23.200268 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m08:04:23.202995 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m08:04:23.231467 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m08:04:23.245328 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m08:04:23.268353 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m08:04:23.282593 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m08:04:23.284107 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m08:04:23.285278 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:23.676799 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a4f3-103f-892a-e68b52b5fe68) - Created
[0m08:04:24.676511 [debug] [Thread-4 (]: SQL status: OK in 1.390 seconds
[0m08:04:24.680667 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a4f3-103f-892a-e68b52b5fe68, command-id=01f0ec68-a505-1293-9e26-d096fa138d99) - Closing
[0m08:04:24.685681 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m08:04:24.687227 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a4f3-103f-892a-e68b52b5fe68) - Closing
[0m08:04:24.826398 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.63s]
[0m08:04:24.828465 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m08:04:24.830547 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m08:04:24.832683 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m08:04:24.836058 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m08:04:24.837681 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m08:04:24.839204 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m08:04:24.850664 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m08:04:24.866434 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m08:04:24.873864 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m08:04:24.892195 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m08:04:24.894098 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m08:04:24.896205 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:25.303569 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a5e9-1ca7-a120-e3053d4f641e) - Created
[0m08:04:26.037352 [debug] [Thread-4 (]: SQL status: OK in 1.140 seconds
[0m08:04:26.041882 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a5e9-1ca7-a120-e3053d4f641e, command-id=01f0ec68-a5fd-112a-8b3e-8a6215623137) - Closing
[0m08:04:26.044140 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m08:04:26.045834 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a5e9-1ca7-a120-e3053d4f641e) - Closing
[0m08:04:26.177650 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.34s]
[0m08:04:26.179675 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m08:04:26.181635 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m08:04:26.183690 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m08:04:26.186293 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m08:04:26.188159 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m08:04:26.189616 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m08:04:26.202523 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m08:04:26.217391 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m08:04:26.227310 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m08:04:26.242013 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m08:04:26.244634 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m08:04:26.246226 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:26.632044 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a6b6-1127-b727-0aac2c736bd6) - Created
[0m08:04:27.448652 [debug] [Thread-4 (]: SQL status: OK in 1.200 seconds
[0m08:04:27.452434 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a6b6-1127-b727-0aac2c736bd6, command-id=01f0ec68-a6c7-153b-9e2b-984f0bfac88c) - Closing
[0m08:04:27.454553 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m08:04:27.456155 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a6b6-1127-b727-0aac2c736bd6) - Closing
[0m08:04:27.597733 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.41s]
[0m08:04:27.599767 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m08:04:27.601703 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m08:04:27.604111 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m08:04:27.606933 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m08:04:27.608889 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m08:04:27.610583 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m08:04:27.619053 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m08:04:27.636654 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m08:04:27.642580 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m08:04:27.656259 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m08:04:27.657709 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m08:04:27.659090 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:28.063570 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a790-1728-95bd-b316dc86062f) - Created
[0m08:04:28.930089 [debug] [Thread-4 (]: SQL status: OK in 1.270 seconds
[0m08:04:28.938036 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a790-1728-95bd-b316dc86062f, command-id=01f0ec68-a7a4-167e-a4ab-1df6afc821d4) - Closing
[0m08:04:28.941314 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m08:04:28.943759 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a790-1728-95bd-b316dc86062f) - Closing
[0m08:04:29.080367 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.47s]
[0m08:04:29.088823 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m08:04:29.093719 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m08:04:29.097460 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m08:04:29.100846 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m08:04:29.103245 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m08:04:29.105210 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m08:04:29.117270 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m08:04:29.131539 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m08:04:29.135779 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m08:04:29.146529 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m08:04:29.147629 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m08:04:29.148572 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:29.536076 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a871-175e-8953-445852ab5fd5) - Created
[0m08:04:30.297724 [debug] [Thread-4 (]: SQL status: OK in 1.150 seconds
[0m08:04:30.301138 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a871-175e-8953-445852ab5fd5, command-id=01f0ec68-a882-1729-b3d0-f208810cc2f7) - Closing
[0m08:04:30.302943 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m08:04:30.304521 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a871-175e-8953-445852ab5fd5) - Closing
[0m08:04:30.445222 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.34s]
[0m08:04:30.450815 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m08:04:30.453945 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m08:04:30.456848 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m08:04:30.460633 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m08:04:30.463121 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m08:04:30.465518 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m08:04:30.477669 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m08:04:30.491085 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m08:04:30.495470 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m08:04:30.508600 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m08:04:30.509881 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m08:04:30.510890 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:30.905543 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a942-17fa-a752-e0aa994ac5b3) - Created
[0m08:04:31.431612 [debug] [Thread-4 (]: SQL status: OK in 0.920 seconds
[0m08:04:31.435877 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a942-17fa-a752-e0aa994ac5b3, command-id=01f0ec68-a953-13d3-80b3-e18a4bf22042) - Closing
[0m08:04:31.438159 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m08:04:31.439905 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a942-17fa-a752-e0aa994ac5b3) - Closing
[0m08:04:31.568816 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.11s]
[0m08:04:31.573192 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m08:04:31.576844 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m08:04:31.580811 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m08:04:31.585259 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m08:04:31.588087 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m08:04:31.590777 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m08:04:31.605257 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m08:04:31.620680 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m08:04:31.624952 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m08:04:31.639198 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m08:04:31.640644 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m08:04:31.642319 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:32.032916 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a9ef-1317-866c-40a01ec77b2c) - Created
[0m08:04:32.679646 [debug] [Thread-4 (]: SQL status: OK in 1.040 seconds
[0m08:04:32.683833 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-a9ef-1317-866c-40a01ec77b2c, command-id=01f0ec68-aa00-1a47-b8c4-de4095e0caee) - Closing
[0m08:04:32.685957 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m08:04:32.687547 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-a9ef-1317-866c-40a01ec77b2c) - Closing
[0m08:04:32.817734 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.23s]
[0m08:04:32.820653 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m08:04:32.823014 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m08:04:32.825509 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m08:04:32.828315 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m08:04:32.830089 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m08:04:32.831691 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m08:04:32.842848 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m08:04:32.860154 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m08:04:32.865766 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m08:04:32.878534 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m08:04:32.879652 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m08:04:32.880865 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:33.291042 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-aaac-1c59-9f1b-65b153a08ef7) - Created
[0m08:04:34.051920 [debug] [Thread-4 (]: SQL status: OK in 1.170 seconds
[0m08:04:34.060871 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-aaac-1c59-9f1b-65b153a08ef7, command-id=01f0ec68-aabf-12f8-8368-d158354f2ebc) - Closing
[0m08:04:34.063870 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m08:04:34.065346 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-aaac-1c59-9f1b-65b153a08ef7) - Closing
[0m08:04:34.184950 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.36s]
[0m08:04:34.187231 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m08:04:34.189280 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m08:04:34.191495 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m08:04:34.194302 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m08:04:34.195948 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m08:04:34.197497 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m08:04:34.211051 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m08:04:34.229061 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m08:04:34.234054 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m08:04:34.246914 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m08:04:34.248370 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m08:04:34.250469 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:34.700949 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-ab84-1f86-a34b-eccddf0ccc14) - Created
[0m08:04:35.305960 [debug] [Thread-4 (]: SQL status: OK in 1.060 seconds
[0m08:04:35.311007 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ab84-1f86-a34b-eccddf0ccc14, command-id=01f0ec68-ab97-123c-ba74-69a40d7bdc55) - Closing
[0m08:04:35.313844 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m08:04:35.316177 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-ab84-1f86-a34b-eccddf0ccc14) - Closing
[0m08:04:35.451908 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.26s]
[0m08:04:35.454950 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m08:04:35.457182 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m08:04:35.459318 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m08:04:35.462031 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m08:04:35.463871 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m08:04:35.465474 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m08:04:35.467133 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m08:04:35.524286 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:35.526061 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m08:04:35.527498 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:35.929144 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c) - Created
[0m08:04:36.460658 [debug] [Thread-4 (]: SQL status: OK in 0.930 seconds
[0m08:04:36.469714 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c, command-id=01f0ec68-ac52-1b40-a476-a9f0effa916a) - Closing
[0m08:04:36.497562 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:36.522741 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:36.575733 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:36.577609 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m08:04:37.012365 [debug] [Thread-4 (]: SQL status: OK in 0.430 seconds
[0m08:04:37.015125 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c, command-id=01f0ec68-acb5-1943-81a9-72c590fff945) - Closing
[0m08:04:37.019886 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:37.021442 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m08:04:37.257908 [debug] [Thread-4 (]: SQL status: OK in 0.240 seconds
[0m08:04:37.262375 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c, command-id=01f0ec68-acf8-1329-8687-7cef18930613) - Closing
[0m08:04:37.274406 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:37.297428 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:37.298789 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m08:04:37.697377 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m08:04:37.703850 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c, command-id=01f0ec68-ad23-13be-9305-74d1ae3ce44b) - Closing
[0m08:04:37.714649 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m08:04:37.720479 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m08:04:37.722079 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m08:04:38.017105 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m08:04:38.019704 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c, command-id=01f0ec68-ad63-186f-bd67-3116b45ca139) - Closing
[0m08:04:38.029780 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m08:04:38.031328 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-ac41-135b-b605-b4bf7294888c) - Closing
[0m08:04:38.171115 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.71s]
[0m08:04:38.178317 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m08:04:38.180374 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m08:04:38.182435 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m08:04:38.185353 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m08:04:38.187125 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m08:04:38.189985 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m08:04:38.192223 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m08:04:38.209695 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:38.211488 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m08:04:38.212911 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:38.598082 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3) - Created
[0m08:04:38.944544 [debug] [Thread-4 (]: SQL status: OK in 0.730 seconds
[0m08:04:38.947470 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3, command-id=01f0ec68-ade9-13de-b22b-e0dca98522e6) - Closing
[0m08:04:38.951635 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:38.974121 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:38.992274 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:38.994029 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m08:04:39.274090 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m08:04:39.277286 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3, command-id=01f0ec68-ae27-1b31-a5d5-e9aa90572935) - Closing
[0m08:04:39.282461 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:39.284396 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m08:04:39.547260 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m08:04:39.551055 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3, command-id=01f0ec68-ae54-1941-8d7f-abef66982be3) - Closing
[0m08:04:39.554882 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:39.573814 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:39.575335 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m08:04:39.913111 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m08:04:39.918102 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3, command-id=01f0ec68-ae80-11be-a10c-f0b57bf25233) - Closing
[0m08:04:39.925827 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m08:04:39.928314 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m08:04:39.930920 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m08:04:40.235169 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m08:04:40.270470 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3, command-id=01f0ec68-aeb5-1e35-954d-8f80b0154798) - Closing
[0m08:04:40.301047 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m08:04:40.325579 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-add9-1ed4-868f-916ba921a5a3) - Closing
[0m08:04:40.457923 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.27s]
[0m08:04:40.460852 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m08:04:40.463086 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m08:04:40.465678 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m08:04:40.470683 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m08:04:40.472658 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m08:04:40.474985 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m08:04:40.478690 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m08:04:40.503034 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:40.505235 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m08:04:40.506654 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m08:04:40.875335 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899) - Created
[0m08:04:41.195414 [debug] [Thread-4 (]: SQL status: OK in 0.690 seconds
[0m08:04:41.200843 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-af44-1e81-b058-085183c5a803) - Closing
[0m08:04:41.208354 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:41.242715 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:41.267398 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:41.269167 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m08:04:41.536163 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m08:04:41.541489 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-af81-1034-8a46-227c3b52d159) - Closing
[0m08:04:41.544308 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:41.545897 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m08:04:41.863352 [debug] [Thread-4 (]: SQL status: OK in 0.320 seconds
[0m08:04:41.867349 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-afad-1d2f-9153-f663e81aa873) - Closing
[0m08:04:41.873614 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:41.875955 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m08:04:42.160424 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m08:04:42.168563 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-afde-114c-95bf-53138b3cdc42) - Closing
[0m08:04:42.174949 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:42.216832 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:42.218410 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m08:04:42.611227 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m08:04:42.615191 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-b011-1e1c-82ca-c9ad88b6eb84) - Closing
[0m08:04:42.620502 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m08:04:42.622638 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m08:04:42.624324 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m08:04:42.893515 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m08:04:42.898099 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899, command-id=01f0ec68-b051-16f7-8dce-b5cdc5ce1aa6) - Closing
[0m08:04:42.905152 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m08:04:42.907877 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ec68-af34-1377-9e79-c41a0d09f899) - Closing
[0m08:04:43.035778 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.57s]
[0m08:04:43.038701 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m08:04:43.044310 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:04:43.047477 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:04:43.050189 [info ] [MainThread]: 
[0m08:04:43.052467 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 23.16 seconds (23.16s).
[0m08:04:43.058655 [debug] [MainThread]: Command end result
[0m08:04:43.378138 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m08:04:43.386417 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m08:04:43.403838 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m08:04:43.405943 [info ] [MainThread]: 
[0m08:04:43.408647 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m08:04:43.411171 [info ] [MainThread]: 
[0m08:04:43.413440 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m08:04:43.414875 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m08:04:43.416288 [info ] [MainThread]: 
[0m08:04:43.418132 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m08:04:43.419511 [info ] [MainThread]: 
[0m08:04:43.421265 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m08:04:43.425123 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 26.428644, "process_in_blocks": "0", "process_kernel_time": 0.641327, "process_mem_max_rss": "255588", "process_out_blocks": "0", "process_user_time": 6.614954}
[0m08:04:43.427047 [debug] [MainThread]: Command `dbt test` failed at 08:04:43.426835 after 26.43 seconds
[0m08:04:43.429518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3696ece320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f364d174340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f364d174a60>]}
[0m08:04:43.431071 [debug] [MainThread]: Flushing usage events
[0m08:04:43.989650 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:14:33.032691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcf02164a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef0681c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef068160>]}


============================== 17:14:33.048856 | 726648eb-d943-45cd-9b99-232a17c5e0e6 ==============================
[0m17:14:33.048856 [info ] [MainThread]: Running with dbt=1.10.18
[0m17:14:33.051925 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'cache_selected_only': 'False', 'empty': 'None', 'static_parser': 'True', 'write_json': 'True', 'fail_fast': 'False', 'log_format': 'default', 'no_print': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'version_check': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name": "account_interest_summary", "local_folder": "/opt/dagster/app/target", "filename": "account_summary.csv"} --profiles-dir dbt --project-dir dbt --target dev', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'quiet': 'False', 'profiles_dir': 'dbt', 'warn_error': 'None', 'log_path': 'dbt/logs', 'printer_width': '80', 'partial_parse': 'True'}
[0m17:14:35.199631 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:14:35.203450 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:14:35.206754 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:14:38.542174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '726648eb-d943-45cd-9b99-232a17c5e0e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef289420>]}
[0m17:14:38.720537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '726648eb-d943-45cd-9b99-232a17c5e0e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef00cf10>]}
[0m17:14:38.723405 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m17:14:38.962955 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m17:14:40.059160 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:14:40.062454 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:14:40.090645 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:14:40.258198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '726648eb-d943-45cd-9b99-232a17c5e0e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef6c3e50>]}
[0m17:14:40.655662 [debug] [MainThread]: Wrote artifact WritableManifest to dbt/target/manifest.json
[0m17:14:40.678003 [debug] [MainThread]: Wrote artifact SemanticManifest to dbt/target/semantic_manifest.json
[0m17:14:40.727971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '726648eb-d943-45cd-9b99-232a17c5e0e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcf0355c90>]}
[0m17:14:40.732049 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m17:14:40.736000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '726648eb-d943-45cd-9b99-232a17c5e0e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef6aba30>]}
[0m17:14:40.741108 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m17:14:40.745692 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m17:14:40.784522 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m17:14:40.787166 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
    select * from `workspace`.`marts`.`account_interest_summary`

  
[0m17:14:40.789377 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:14:41.834101 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecb5-8536-1757-8db6-e9dc441103d4) - Created
[0m17:15:07.797515 [debug] [MainThread]: SQL status: OK in 27.010 seconds
[0m17:15:07.850170 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ecb5-8536-1757-8db6-e9dc441103d4, command-id=01f0ecb5-855d-11dc-af97-c8c2537592fb) - Closing
[0m17:15:08.041655 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Compilation Error
  'write_csv' is undefined
[0m17:15:08.070133 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m17:15:08.109972 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecb5-8536-1757-8db6-e9dc441103d4) - Closing
[0m17:15:08.316683 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  'write_csv' is undefined
[0m17:15:08.334330 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 418, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 44, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 399, in call
    if not __self.is_safe_callable(__obj):
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 265, in is_safe_callable
    getattr(obj, "unsafe_callable", False) or getattr(obj, "alters_data", False)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 870, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 859, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'write_csv' is undefined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 393, in call_macro
    with self.exception_handler():
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 420, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  'write_csv' is undefined

[0m17:15:08.433461 [debug] [MainThread]: Wrote artifact RunResultsArtifact to dbt/target/run_results.json
[0m17:15:08.443011 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 35.657024, "process_in_blocks": "242096", "process_kernel_time": 2.119272, "process_mem_max_rss": "237972", "process_out_blocks": "816", "process_user_time": 10.21336}
[0m17:15:08.452229 [debug] [MainThread]: Command `dbt run-operation` failed at 17:15:08.451807 after 35.67 seconds
[0m17:15:08.455349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcf02164a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcef00cf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbcc99861d0>]}
[0m17:15:08.458455 [debug] [MainThread]: Flushing usage events
[0m17:15:08.990619 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:51:18.827224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f411ea470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f40018970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f40018910>]}


============================== 17:51:18.837369 | 53fd6977-f9bf-4ebf-9a8e-ac61afbbc135 ==============================
[0m17:51:18.837369 [info ] [MainThread]: Running with dbt=1.10.18
[0m17:51:18.839244 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'empty': 'None', 'log_cache_events': 'False', 'target_path': 'None', 'no_print': 'None', 'write_json': 'True', 'use_colors': 'True', 'log_path': 'dbt/logs', 'introspect': 'True', 'warn_error': 'None', 'debug': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name": "account_interest_summary", "local_folder": "/opt/dagster/app/target", "filename": "account_summary.csv"} --profiles-dir dbt --project-dir dbt --target dev', 'profiles_dir': 'dbt', 'partial_parse': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'quiet': 'False', 'printer_width': '80'}
[0m17:51:20.063854 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:51:20.066017 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:51:20.067733 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:51:21.849039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f3dfb2a10>]}
[0m17:51:21.973932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f40a4e410>]}
[0m17:51:21.976277 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m17:51:22.187224 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m17:51:22.437856 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m17:51:22.439598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f40a4d450>]}
[0m17:51:25.298930 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:25.322867 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:25.328742 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:25.336013 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:25.590305 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:25.596637 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.006944 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.011669 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.017047 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.022394 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.026403 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.030515 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.035356 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.250412 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.294341 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m17:51:26.504976 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m17:51:26.524249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f18ecd180>]}
[0m17:51:26.745732 [debug] [MainThread]: Wrote artifact WritableManifest to dbt/target/manifest.json
[0m17:51:26.758064 [debug] [MainThread]: Wrote artifact SemanticManifest to dbt/target/semantic_manifest.json
[0m17:51:26.784701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f1a056bc0>]}
[0m17:51:26.786570 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m17:51:26.788125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53fd6977-f9bf-4ebf-9a8e-ac61afbbc135', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f1a706f20>]}
[0m17:51:26.790945 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m17:51:26.793018 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m17:51:26.807546 [info ] [MainThread]: Exporting account_interest_summary to CSV
[0m17:51:26.833692 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m17:51:26.835972 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        SELECT * FROM `workspace`.`marts`.`account_interest_summary`
    
  
[0m17:51:26.837621 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:51:27.866696 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecba-a7be-1a7b-bbbd-54198046b6a7) - Created
[0m17:51:48.869100 [debug] [MainThread]: SQL status: OK in 22.030 seconds
[0m17:51:48.885090 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ecba-a7be-1a7b-bbbd-54198046b6a7, command-id=01f0ecba-a806-19b3-b3ef-11dd73e0159e) - Closing
[0m17:51:49.059651 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m17:51:49.062480 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        SELECT * FROM `workspace`.`marts`.`account_interest_summary`
    
  
[0m17:51:50.085339 [debug] [MainThread]: SQL status: OK in 1.020 seconds
[0m17:51:50.091659 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ecba-a7be-1a7b-bbbd-54198046b6a7, command-id=01f0ecba-b4a4-16a8-8595-475943d4a9f4) - Closing
[0m17:51:50.094486 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Compilation Error
  'agate.table.Table object' has no attribute 'to_pandas'
[0m17:51:50.096947 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m17:51:50.099220 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecba-a7be-1a7b-bbbd-54198046b6a7) - Closing
[0m17:51:50.245713 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  'agate.table.Table object' has no attribute 'to_pandas'
[0m17:51:50.266511 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 418, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 63, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 399, in call
    if not __self.is_safe_callable(__obj):
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 265, in is_safe_callable
    getattr(obj, "unsafe_callable", False) or getattr(obj, "alters_data", False)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 870, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 859, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'agate.table.Table object' has no attribute 'to_pandas'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 393, in call_macro
    with self.exception_handler():
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 420, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  'agate.table.Table object' has no attribute 'to_pandas'

[0m17:51:50.287707 [debug] [MainThread]: Wrote artifact RunResultsArtifact to dbt/target/run_results.json
[0m17:51:50.291157 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 31.583666, "process_in_blocks": "242472", "process_kernel_time": 1.020624, "process_mem_max_rss": "248732", "process_out_blocks": "816", "process_user_time": 9.034119}
[0m17:51:50.292646 [debug] [MainThread]: Command `dbt run-operation` failed at 17:51:50.292504 after 31.59 seconds
[0m17:51:50.294038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f411ea470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f40a4e410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f414fb610>]}
[0m17:51:50.295412 [debug] [MainThread]: Flushing usage events
[0m17:51:50.803006 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:02:27.479066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5b0fa4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec59f30940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec59f308e0>]}


============================== 18:02:27.491818 | 55d19a6f-73fd-4082-97da-9b9a244fd824 ==============================
[0m18:02:27.491818 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:02:27.493985 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name": "account_interest_summary", "local_folder": "/opt/dagster/app/target", "filename": "account_summary.csv"} --profiles-dir dbt --project-dir dbt --target dev', 'version_check': 'True', 'printer_width': '80', 'log_path': 'dbt/logs', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'debug': 'False', 'static_parser': 'True', 'quiet': 'False', 'target_path': 'None', 'log_format': 'default', 'introspect': 'True', 'write_json': 'True', 'profiles_dir': 'dbt', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'None'}
[0m18:02:28.569507 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:02:28.571578 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:02:28.573282 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:02:30.457475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5a154d60>]}
[0m18:02:30.568718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5a0d03d0>]}
[0m18:02:30.570638 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:02:30.761626 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:02:30.992663 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m18:02:30.994724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5a781960>]}
[0m18:02:33.730811 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:33.757713 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:33.767207 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:33.774003 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.069589 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.075348 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.504835 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.509514 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.514811 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.520451 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.525320 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.531021 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.535077 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.755259 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:34.832189 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m18:02:35.112437 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:02:35.129612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec32b3d1b0>]}
[0m18:02:35.347530 [debug] [MainThread]: Wrote artifact WritableManifest to dbt/target/manifest.json
[0m18:02:35.357896 [debug] [MainThread]: Wrote artifact SemanticManifest to dbt/target/semantic_manifest.json
[0m18:02:35.377490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec38889990>]}
[0m18:02:35.378921 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:02:35.380202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55d19a6f-73fd-4082-97da-9b9a244fd824', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec38889990>]}
[0m18:02:35.382165 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m18:02:35.383734 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m18:02:35.397461 [info ] [MainThread]: Exporting account_interest_summary to CSV
[0m18:02:35.407618 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m18:02:35.410143 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        select * from `workspace`.`marts`.`account_interest_summary`
    
  
[0m18:02:35.412751 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:02:36.239399 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecbc-3642-1806-91ca-c007ecb5ce90) - Created
[0m18:02:58.002276 [debug] [MainThread]: SQL status: OK in 22.590 seconds
[0m18:02:58.020834 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0ecbc-3642-1806-91ca-c007ecb5ce90, command-id=01f0ecbc-3667-19af-a1ad-7b6d1bcad85b) - Closing
[0m18:02:58.220869 [info ] [MainThread]: Query executed successfully
[0m18:02:58.227128 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Compilation Error
  'python' is undefined
[0m18:02:58.230817 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m18:02:58.234525 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecbc-3642-1806-91ca-c007ecb5ce90) - Closing
[0m18:02:58.374660 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  'python' is undefined
[0m18:02:58.384327 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 418, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 112, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 319, in getattr
    value = getattr(obj, attribute)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 870, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 859, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'python' is undefined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 393, in call_macro
    with self.exception_handler():
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 420, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  'python' is undefined

[0m18:02:58.401160 [debug] [MainThread]: Wrote artifact RunResultsArtifact to dbt/target/run_results.json
[0m18:02:58.403794 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 31.073997, "process_in_blocks": "243440", "process_kernel_time": 1.029473, "process_mem_max_rss": "248688", "process_out_blocks": "816", "process_user_time": 8.800213}
[0m18:02:58.405120 [debug] [MainThread]: Command `dbt run-operation` failed at 18:02:58.404999 after 31.08 seconds
[0m18:02:58.406293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5b0fa4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5a154d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ec5b4074f0>]}
[0m18:02:58.407645 [debug] [MainThread]: Flushing usage events
[0m18:02:58.851203 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:12:41.541779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66aa02470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66982c220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66982c1c0>]}


============================== 18:12:41.556832 | ffd376e6-0870-48fb-bfc3-e5a33e3b60fa ==============================
[0m18:12:41.556832 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:12:41.559111 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'version_check': 'True', 'write_json': 'True', 'log_path': 'dbt/logs', 'use_colors': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'empty': 'None', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'introspect': 'True', 'profiles_dir': 'dbt', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'invocation_command': 'dbt run-operation export_to_local_csv --args {"model_name":"account_interest_summary","local_folder":"/opt/dagster/app/target","filename":"account_summary"} --project-dir dbt --profiles-dir dbt --target dev'}
[0m18:12:43.737700 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:12:43.739965 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:12:43.743345 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:12:46.550943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffd376e6-0870-48fb-bfc3-e5a33e3b60fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6677caa10>]}
[0m18:12:46.744732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffd376e6-0870-48fb-bfc3-e5a33e3b60fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66a26a410>]}
[0m18:12:46.753308 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:12:47.325883 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:12:48.176508 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:12:48.182849 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://macros/export_to_local_csv.sql
[0m18:12:48.459814 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:12:48.492571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffd376e6-0870-48fb-bfc3-e5a33e3b60fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd64392a260>]}
[0m18:12:48.982283 [debug] [MainThread]: Wrote artifact WritableManifest to dbt/target/manifest.json
[0m18:12:48.992843 [debug] [MainThread]: Wrote artifact SemanticManifest to dbt/target/semantic_manifest.json
[0m18:12:49.018068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffd376e6-0870-48fb-bfc3-e5a33e3b60fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd64393dcc0>]}
[0m18:12:49.021964 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:12:49.026847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffd376e6-0870-48fb-bfc3-e5a33e3b60fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd64393ded0>]}
[0m18:12:49.036337 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m18:12:49.045235 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m18:12:49.118749 [debug] [MainThread]: Using databricks connection "macro_export_to_local_csv"
[0m18:12:49.148206 [debug] [MainThread]: On macro_export_to_local_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
[0m18:12:49.155576 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:12:50.501109 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecbd-a441-1434-8a41-bf320faee932) - Created
[0m18:12:50.922509 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:791)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:782)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:570)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0ecbd-a48b-1b95-bdbd-9b7e78abc4f2
[0m18:12:50.925138 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_local_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */
  
      
          COPY INTO '/opt/dagster/app/target/account_summary'
  ------------------^^^
          FROM (
              SELECT * FROM `workspace`.`marts`.`account_interest_summary`
          )
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  
[0m18:12:50.927260 [debug] [MainThread]: On macro_export_to_local_csv: Close
[0m18:12:50.929179 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecbd-a441-1434-8a41-bf320faee932) - Closing
[0m18:12:51.087608 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */
    
        
            COPY INTO '/opt/dagster/app/target/account_summary'
    ------------------^^^
            FROM (
                SELECT * FROM `workspace`.`marts`.`account_interest_summary`
            )
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    
[0m18:12:51.106973 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 944, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */

    
        COPY INTO '/opt/dagster/app/target/account_summary'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 58, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */
  
      
          COPY INTO '/opt/dagster/app/target/account_summary'
  ------------------^^^
          FROM (
              SELECT * FROM `workspace`.`marts`.`account_interest_summary`
          )
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/opt/dagster/app/target/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_local_csv"} */
    
        
            COPY INTO '/opt/dagster/app/target/account_summary'
    ------------------^^^
            FROM (
                SELECT * FROM `workspace`.`marts`.`account_interest_summary`
            )
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    

[0m18:12:51.128394 [debug] [MainThread]: Wrote artifact RunResultsArtifact to dbt/target/run_results.json
[0m18:12:51.135516 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 9.800076, "process_in_blocks": "236400", "process_kernel_time": 2.036439, "process_mem_max_rss": "235176", "process_out_blocks": "816", "process_user_time": 9.640638}
[0m18:12:51.137645 [debug] [MainThread]: Command `dbt run-operation` failed at 18:12:51.137378 after 9.80 seconds
[0m18:12:51.140547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66aa02470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6484d1360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd66ad15660>]}
[0m18:12:51.142171 [debug] [MainThread]: Flushing usage events
[0m18:12:51.563004 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:27:36.277477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b99066560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b97e9c280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b97e9c220>]}


============================== 18:27:36.287213 | 8168e852-38e9-4afc-829e-f2fbe794549a ==============================
[0m18:27:36.287213 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:27:36.288952 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'empty': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'cache_selected_only': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --target dev', 'quiet': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'partial_parse': 'True', 'write_json': 'True', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/opt/dagster/app/dbt/logs', 'no_print': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'printer_width': '80'}
[0m18:27:37.537333 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:27:37.539255 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:27:37.540889 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:27:39.536483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8168e852-38e9-4afc-829e-f2fbe794549a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b9addff70>]}
[0m18:27:39.672761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8168e852-38e9-4afc-829e-f2fbe794549a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b98706230>]}
[0m18:27:39.675053 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:27:39.883051 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:27:40.435591 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:27:40.437551 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://macros/export_to_local_csv.sql
[0m18:27:40.645332 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:27:40.685590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8168e852-38e9-4afc-829e-f2fbe794549a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b71f9e380>]}
[0m18:27:40.970844 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m18:27:40.980133 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m18:27:40.999610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8168e852-38e9-4afc-829e-f2fbe794549a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b71f7ed40>]}
[0m18:27:41.001173 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:27:41.002368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8168e852-38e9-4afc-829e-f2fbe794549a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b71f7eb00>]}
[0m18:27:41.004266 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m18:27:41.005964 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m18:27:41.017709 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Compilation Error
  The name argument to ref() must be a string, got <class 'jinja2.runtime.Undefined'>
  
  > in macro export_to_dbfs_csv (macros/export_to_local_csv.sql)
  > called by <Unknown>
[0m18:27:41.019841 [error] [MainThread]: Encountered an error while running operation: Database Error
  Compilation Error
    The name argument to ref() must be a string, got <class 'jinja2.runtime.Undefined'>
    
    > in macro export_to_dbfs_csv (macros/export_to_local_csv.sql)
    > called by <Unknown>
[0m18:27:41.030587 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 39, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/context/providers.py", line 348, in __call__
    self.validate_args(name, package, version)
  File "/usr/local/lib/python3.10/site-packages/dbt/context/providers.py", line 321, in validate_args
    raise CompilationError(
dbt_common.exceptions.base.CompilationError: Compilation Error
  The name argument to ref() must be a string, got <class 'jinja2.runtime.Undefined'>
  
  > in macro export_to_dbfs_csv (macros/export_to_local_csv.sql)
  > called by <Unknown>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Compilation Error
    The name argument to ref() must be a string, got <class 'jinja2.runtime.Undefined'>
    
    > in macro export_to_dbfs_csv (macros/export_to_local_csv.sql)
    > called by <Unknown>

[0m18:27:41.050521 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m18:27:41.053322 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.9179163, "process_in_blocks": "232000", "process_kernel_time": 0.889902, "process_mem_max_rss": "232052", "process_out_blocks": "816", "process_user_time": 5.754702}
[0m18:27:41.054825 [debug] [MainThread]: Command `dbt run-operation` failed at 18:27:41.054667 after 4.92 seconds
[0m18:27:41.055979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b99066560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b98fb8ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x776b71fd2740>]}
[0m18:27:41.057121 [debug] [MainThread]: Flushing usage events
[0m18:27:41.549084 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:29:57.479649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90533e350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e904160250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e9041601f0>]}


============================== 18:29:57.484621 | 10f22084-b590-4db9-91b2-b851e7262698 ==============================
[0m18:29:57.484621 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:29:57.486221 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run-operation export_to_local_csv --target dev', 'log_format': 'default', 'log_path': '/opt/dagster/app/dbt/logs', 'partial_parse': 'True', 'static_parser': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'use_colors': 'True', 'no_print': 'None', 'quiet': 'False', 'debug': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'empty': 'None', 'cache_selected_only': 'False', 'introspect': 'True', 'printer_width': '80'}
[0m18:29:58.611181 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:29:58.612831 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:29:58.614190 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:30:00.412608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10f22084-b590-4db9-91b2-b851e7262698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e906e40fa0>]}
[0m18:30:00.533453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10f22084-b590-4db9-91b2-b851e7262698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90412e3b0>]}
[0m18:30:00.535570 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:30:00.741243 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:30:01.275368 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:30:01.276974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:30:01.290705 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:30:01.387243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10f22084-b590-4db9-91b2-b851e7262698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e9047b3fa0>]}
[0m18:30:01.635977 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m18:30:01.649464 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m18:30:01.679701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10f22084-b590-4db9-91b2-b851e7262698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e904b666b0>]}
[0m18:30:01.682112 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:30:01.684243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10f22084-b590-4db9-91b2-b851e7262698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90479f9d0>]}
[0m18:30:01.687728 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_local_csv) - Creating connection
[0m18:30:01.690556 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_local_csv'
[0m18:30:01.694120 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "export_to_local_csv" in any package
[0m18:30:01.698877 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1294, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "export_to_local_csv" in any package

[0m18:30:01.701370 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt could not find a macro with the name 'export_to_local_csv' in any package. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m18:30:01.706621 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.352357, "process_in_blocks": "231360", "process_kernel_time": 0.619387, "process_mem_max_rss": "230052", "process_out_blocks": "0", "process_user_time": 5.187861}
[0m18:30:01.711524 [debug] [MainThread]: Command `dbt run-operation` failed at 18:30:01.711187 after 4.36 seconds
[0m18:30:01.714343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90533e350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90479fca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73e90479fcd0>]}
[0m18:30:01.716705 [debug] [MainThread]: Flushing usage events
[0m18:30:02.246588 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:32:31.347863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7076bde500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7075a382b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7075a38250>]}


============================== 18:32:31.358114 | 0214a29d-3ebc-4d1f-98a6-f73187e094d2 ==============================
[0m18:32:31.358114 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:32:31.359905 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'debug': 'False', 'target_path': 'None', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'write_json': 'True', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --args {"model_name": "account_interest_summary"} --profiles-dir /opt/dagster/app/dbt --project-dir /opt/dagster/app/dbt --target dev', 'profiles_dir': '/opt/dagster/app/dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'empty': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'quiet': 'False', 'fail_fast': 'False', 'introspect': 'True', 'cache_selected_only': 'False'}
[0m18:32:32.595878 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:32:32.597640 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:32:32.599086 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:32:34.651742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0214a29d-3ebc-4d1f-98a6-f73187e094d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b70739f1f30>]}
[0m18:32:34.781034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0214a29d-3ebc-4d1f-98a6-f73187e094d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b70762716f0>]}
[0m18:32:34.783351 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:32:34.995154 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:32:35.571485 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m18:32:35.573528 [debug] [MainThread]: Partial parsing: added file: banking_pipeline://macros/export_to_dbfs_csv.sql
[0m18:32:35.575228 [debug] [MainThread]: Partial parsing: deleted file: banking_pipeline://macros/export_to_local_csv.sql
[0m18:32:35.792404 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:32:35.821225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0214a29d-3ebc-4d1f-98a6-f73187e094d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7076092260>]}
[0m18:32:36.078658 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m18:32:36.115579 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m18:32:36.148403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0214a29d-3ebc-4d1f-98a6-f73187e094d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7076d27be0>]}
[0m18:32:36.149960 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:32:36.151395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0214a29d-3ebc-4d1f-98a6-f73187e094d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b70760aeb00>]}
[0m18:32:36.153555 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m18:32:36.155590 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m18:32:36.174132 [debug] [MainThread]: Using databricks connection "macro_export_to_dbfs_csv"
[0m18:32:36.176047 [debug] [MainThread]: On macro_export_to_dbfs_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO '/'
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
[0m18:32:36.178089 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:32:37.096869 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecc0-679a-1f1d-b20a-f914776dbdb7) - Created
[0m18:32:46.358502 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO '/'
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO '/'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

Error properties: diagnostic-info=None, operation-id=01f0ecc0-67ce-12cc-874d-6825c3346f23
[0m18:32:46.365520 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO '/'
  ------------------^^^
          FROM (
              SELECT * FROM `workspace`.`marts`.`account_interest_summary`
          )
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  
[0m18:32:46.372050 [debug] [MainThread]: On macro_export_to_dbfs_csv: Close
[0m18:32:46.378275 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecc0-679a-1f1d-b20a-f914776dbdb7) - Closing
[0m18:32:46.521670 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO '/'
    ------------------^^^
            FROM (
                SELECT * FROM `workspace`.`marts`.`account_interest_summary`
            )
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    
[0m18:32:46.555770 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 957, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(op_handle, poll_resp)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO '/'
------------------^^^
        FROM (
            SELECT * FROM `workspace`.`marts`.`account_interest_summary`
        )
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 58, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO '/'
  ------------------^^^
          FROM (
              SELECT * FROM `workspace`.`marts`.`account_interest_summary`
          )
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO '/'
    ------------------^^^
            FROM (
                SELECT * FROM `workspace`.`marts`.`account_interest_summary`
            )
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    

[0m18:32:46.572405 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m18:32:46.575108 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 15.3548355, "process_in_blocks": "218840", "process_kernel_time": 0.899059, "process_mem_max_rss": "234512", "process_out_blocks": "816", "process_user_time": 5.892053}
[0m18:32:46.576441 [debug] [MainThread]: Command `dbt run-operation` failed at 18:32:46.576325 after 15.36 seconds
[0m18:32:46.577848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7076bde500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7076d27be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b70760aded0>]}
[0m18:32:46.579558 [debug] [MainThread]: Flushing usage events
[0m18:32:47.049790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:46:41.322191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff907a4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff7ea4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff7ea41f0>]}


============================== 18:46:41.331716 | 9612b5e8-d28f-42a1-9375-e60135aa94d7 ==============================
[0m18:46:41.331716 [info ] [MainThread]: Running with dbt=1.10.18
[0m18:46:41.333806 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'printer_width': '80', 'empty': 'None', 'quiet': 'False', 'static_parser': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --args {"model_name": "account_interest_summary"} --profiles-dir /opt/dagster/app/dbt --project-dir /opt/dagster/app/dbt --target dev', 'version_check': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None'}
[0m18:46:42.410556 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:46:42.412173 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:46:42.413494 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:46:44.169820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9612b5e8-d28f-42a1-9375-e60135aa94d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9fd2b426b0>]}
[0m18:46:44.277902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9612b5e8-d28f-42a1-9375-e60135aa94d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff88e05b0>]}
[0m18:46:44.279949 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m18:46:44.502028 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m18:46:44.976048 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:46:44.978052 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://macros/export_to_dbfs_csv.sql
[0m18:46:45.161927 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m18:46:45.186059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9612b5e8-d28f-42a1-9375-e60135aa94d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9fd1f762c0>]}
[0m18:46:45.402966 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m18:46:45.413573 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m18:46:45.433913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9612b5e8-d28f-42a1-9375-e60135aa94d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9fd1fb2110>]}
[0m18:46:45.435399 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m18:46:45.436682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9612b5e8-d28f-42a1-9375-e60135aa94d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9fd1fb2380>]}
[0m18:46:45.438708 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m18:46:45.440313 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m18:46:45.451267 [info ] [MainThread]: Exporting model: account_interest_summary
[0m18:46:45.461503 [debug] [MainThread]: Using databricks connection "macro_export_to_dbfs_csv"
[0m18:46:45.463498 [debug] [MainThread]: On macro_export_to_dbfs_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
[0m18:46:45.465282 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:46:46.284811 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecc2-6212-1b49-84b4-cb8241b48428) - Created
[0m18:46:55.793159 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

Error properties: diagnostic-info=None, operation-id=01f0ecc2-623b-1bc3-b68a-686ad7da2083
[0m18:46:55.795652 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  
[0m18:46:55.798031 [debug] [MainThread]: On macro_export_to_dbfs_csv: Close
[0m18:46:55.799994 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecc2-6212-1b49-84b4-cb8241b48428) - Closing
[0m18:46:55.932278 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    
[0m18:46:55.956365 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 957, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(op_handle, poll_resp)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 66, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    

[0m18:46:55.974580 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m18:46:55.977917 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 14.76963, "process_in_blocks": "236392", "process_kernel_time": 0.789861, "process_mem_max_rss": "234744", "process_out_blocks": "816", "process_user_time": 5.140054}
[0m18:46:55.979847 [debug] [MainThread]: Command `dbt run-operation` failed at 18:46:55.979625 after 14.77 seconds
[0m18:46:55.981671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff907a4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9ff88e05b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b9fd2b5c730>]}
[0m18:46:55.983367 [debug] [MainThread]: Flushing usage events
[0m18:46:56.480797 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:09:40.616051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7caec22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7c9e982b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7c9e98250>]}


============================== 22:09:40.625568 | fecbab0e-fa7a-41aa-b182-7b5bc596db79 ==============================
[0m22:09:40.625568 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:09:40.627315 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'log_cache_events': 'False', 'introspect': 'True', 'no_print': 'None', 'empty': 'None', 'printer_width': '80', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'cache_selected_only': 'False', 'version_check': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'write_json': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/opt/dagster/app/dbt', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True'}
[0m22:09:41.607922 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:09:41.610210 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:09:41.612203 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:09:42.766026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7caf90040>]}
[0m22:09:42.909044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7c7e39570>]}
[0m22:09:42.911493 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:09:43.128722 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:09:43.591998 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:09:43.594141 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:09:43.607708 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:09:43.707569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7ca513e50>]}
[0m22:09:43.949598 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:09:43.958942 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:09:43.987338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7ca56d3f0>]}
[0m22:09:43.988703 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:09:43.989744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7ca56eb30>]}
[0m22:09:43.992416 [info ] [MainThread]: 
[0m22:09:43.993544 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:09:43.994760 [info ] [MainThread]: 
[0m22:09:43.996589 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:09:43.997968 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:09:44.008722 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:09:44.010759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:09:44.027509 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:09:44.029477 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:09:44.031231 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:09:44.944792 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-bcec-1148-8285-9cdb76f4613d) - Created
[0m22:10:00.779710 [debug] [ThreadPool]: SQL status: OK in 16.750 seconds
[0m22:10:00.787453 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-bcec-1148-8285-9cdb76f4613d, command-id=01f0ecde-bd17-14e5-9d18-4be41232c192) - Closing
[0m22:10:01.012522 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:01.014194 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-bcec-1148-8285-9cdb76f4613d) - Closing
[0m22:10:01.146145 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:10:01.147723 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:10:01.156682 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:10:01.158466 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:10:01.160341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:01.585188 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-c6ea-12ec-8bbe-d0294ff4f753) - Created
[0m22:10:06.450554 [debug] [ThreadPool]: SQL status: OK in 5.290 seconds
[0m22:10:06.464894 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-c6ea-12ec-8bbe-d0294ff4f753, command-id=01f0ecde-c6fd-10c0-b721-b36a91d140dc) - Closing
[0m22:10:06.468424 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:10:06.469918 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-c6ea-12ec-8bbe-d0294ff4f753) - Closing
[0m22:10:06.633137 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:10:06.635931 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:10:06.666373 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:10:06.668595 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:10:06.671016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:07.154387 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-ca3a-13f5-a950-dd16438108a0) - Created
[0m22:10:07.967057 [debug] [ThreadPool]: SQL status: OK in 1.300 seconds
[0m22:10:07.972464 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-ca3a-13f5-a950-dd16438108a0, command-id=01f0ecde-ca51-1ab8-b1bd-236d519dfb03) - Closing
[0m22:10:07.975365 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:10:07.977646 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-ca3a-13f5-a950-dd16438108a0) - Closing
[0m22:10:08.114619 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:10:08.118873 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:10:08.125448 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:10:08.127903 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:10:08.129935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:08.547473 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-cb10-1c18-861a-5f5b692f3f02) - Created
[0m22:10:09.194433 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m22:10:09.202710 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-cb10-1c18-861a-5f5b692f3f02, command-id=01f0ecde-cb24-13b4-afac-ffc0bd381e74) - Closing
[0m22:10:09.205533 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:10:09.207812 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-cb10-1c18-861a-5f5b692f3f02) - Closing
[0m22:10:09.352805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7a89a4970>]}
[0m22:10:09.376958 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m22:10:09.379826 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m22:10:09.381988 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m22:10:09.384029 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m22:10:09.385727 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m22:10:09.387243 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m22:10:09.398737 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:10:09.400486 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7a83f9ea0>]}
[0m22:10:09.461869 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:10:09.464017 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7a83eeb00>]}
[0m22:10:09.499769 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:10:09.502124 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:10:09.503729 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:10:09.935383 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecde-cbe8-159d-b3ae-645d956e63d2) - Created
[0m22:10:15.253237 [debug] [Thread-5 (]: SQL status: OK in 5.750 seconds
[0m22:10:15.259141 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecde-cbe8-159d-b3ae-645d956e63d2, command-id=01f0ecde-cbf9-1d45-963c-28e0570b5cf8) - Closing
[0m22:10:15.297747 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:10:15.299964 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m22:10:21.249661 [debug] [Thread-5 (]: SQL status: OK in 5.950 seconds
[0m22:10:21.252752 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecde-cbe8-159d-b3ae-645d956e63d2, command-id=01f0ecde-cf2b-1cf7-86eb-ac8b56badaba) - Closing
[0m22:10:21.473692 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m22:10:21.504655 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m22:10:21.506425 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecde-cbe8-159d-b3ae-645d956e63d2) - Closing
[0m22:10:21.642118 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7ca552980>]}
[0m22:10:21.644812 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 12.26s]
[0m22:10:21.647464 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m22:10:21.649871 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m22:10:21.652296 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m22:10:21.654649 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m22:10:21.656563 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m22:10:21.658540 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m22:10:21.660487 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m22:10:21.679911 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:10:21.681815 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:10:21.682984 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:10:25.179357 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecde-d4fc-1cee-9602-96b624f3fbb7) - Created
[0m22:10:27.553346 [debug] [Thread-5 (]: SQL status: OK in 5.870 seconds
[0m22:10:27.557037 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecde-d4fc-1cee-9602-96b624f3fbb7, command-id=01f0ecde-d510-134a-9e10-a12a12991f3b) - Closing
[0m22:10:27.563645 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:10:27.566490 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m22:10:30.222144 [debug] [Thread-5 (]: SQL status: OK in 2.650 seconds
[0m22:10:30.224743 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecde-d4fc-1cee-9602-96b624f3fbb7, command-id=01f0ecde-d67b-10b6-b6c2-33e509de1f18) - Closing
[0m22:10:30.227698 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m22:10:30.254009 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m22:10:30.256378 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecde-d4fc-1cee-9602-96b624f3fbb7) - Closing
[0m22:10:30.410761 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fecbab0e-fa7a-41aa-b182-7b5bc596db79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7cb356680>]}
[0m22:10:30.415050 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 8.76s]
[0m22:10:30.420059 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m22:10:30.427840 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:10:30.431989 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:10:30.436709 [info ] [MainThread]: 
[0m22:10:30.440310 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 46.44 seconds (46.44s).
[0m22:10:30.445245 [debug] [MainThread]: Command end result
[0m22:10:30.562620 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:10:30.572390 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:10:30.588925 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:10:30.590412 [info ] [MainThread]: 
[0m22:10:30.591681 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:10:30.592983 [info ] [MainThread]: 
[0m22:10:30.594182 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:10:30.600315 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 50.125122, "process_in_blocks": "2896", "process_kernel_time": 0.787333, "process_mem_max_rss": "246756", "process_out_blocks": "712", "process_user_time": 6.366955}
[0m22:10:30.602359 [debug] [MainThread]: Command `dbt seed` succeeded at 22:10:30.602168 after 50.13 seconds
[0m22:10:30.603957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7caec22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7a89813f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77f7caf90040>]}
[0m22:10:30.605550 [debug] [MainThread]: Flushing usage events
[0m22:10:31.071591 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:10:38.868007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a39dbe2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a38b6c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a38b6c250>]}


============================== 22:10:38.872710 | df2b927a-603d-48e2-99a9-040f30a9564a ==============================
[0m22:10:38.872710 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:10:38.874454 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'static_parser': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'log_path': '/opt/dagster/app/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'no_print': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'target_path': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'introspect': 'True', 'log_format': 'default', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'debug': 'False'}
[0m22:10:40.291015 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:10:40.293258 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:10:40.294988 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:10:41.569261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a1380ad40>]}
[0m22:10:41.728256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a38d396c0>]}
[0m22:10:41.731167 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:10:41.986689 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:10:42.592224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:10:42.594141 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:10:42.610807 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:10:42.727493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12c17e50>]}
[0m22:10:43.049348 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:10:43.059879 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:10:43.086255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12c15cc0>]}
[0m22:10:43.088089 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:10:43.089710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12c17910>]}
[0m22:10:43.092943 [info ] [MainThread]: 
[0m22:10:43.094575 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:10:43.096072 [info ] [MainThread]: 
[0m22:10:43.099169 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:10:43.101409 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:10:43.114320 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:10:43.117410 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:10:43.137912 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:10:43.139842 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:10:43.141568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:43.627079 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-dffa-1dbd-92ba-4fde299c1338) - Created
[0m22:10:44.002532 [debug] [ThreadPool]: SQL status: OK in 0.860 seconds
[0m22:10:44.008543 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-dffa-1dbd-92ba-4fde299c1338, command-id=01f0ecde-e00f-1828-aef7-f0c0cf745e3f) - Closing
[0m22:10:44.010500 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:44.012025 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-dffa-1dbd-92ba-4fde299c1338) - Closing
[0m22:10:44.159018 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:10:44.161654 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:10:44.170556 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:10:44.172550 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:10:44.174280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:44.628831 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e08f-1950-8263-2d4be9bdb8b0) - Created
[0m22:10:45.025328 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m22:10:45.029808 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-e08f-1950-8263-2d4be9bdb8b0, command-id=01f0ecde-e0a9-1326-81b3-2a4360767f70) - Closing
[0m22:10:45.032316 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:45.034449 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e08f-1950-8263-2d4be9bdb8b0) - Closing
[0m22:10:45.217438 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:10:45.220099 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:10:45.238346 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:10:45.240507 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:10:45.242451 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:45.746846 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e135-1111-826c-7ac9c5ea0790) - Created
[0m22:10:46.196305 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m22:10:46.203779 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-e135-1111-826c-7ac9c5ea0790, command-id=01f0ecde-e153-1080-a260-67d60bb41fd4) - Closing
[0m22:10:46.206706 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:46.208841 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e135-1111-826c-7ac9c5ea0790) - Closing
[0m22:10:46.371362 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:10:46.373062 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:10:46.389404 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:10:46.391122 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:10:46.392661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:46.777917 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e1dd-1226-a22f-e92870e96e19) - Created
[0m22:10:47.358051 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m22:10:47.363277 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-e1dd-1226-a22f-e92870e96e19, command-id=01f0ecde-e1ee-1304-b8a6-c890c6b46ec7) - Closing
[0m22:10:47.366517 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:10:47.369308 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e1dd-1226-a22f-e92870e96e19) - Closing
[0m22:10:47.494526 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:10:47.499680 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:10:47.510693 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:10:47.513748 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:10:47.516975 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:47.906544 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e289-1d56-89d4-4b76f67364e6) - Created
[0m22:10:48.484351 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m22:10:48.490530 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-e289-1d56-89d4-4b76f67364e6, command-id=01f0ecde-e29a-1cf4-a755-9e03262a0592) - Closing
[0m22:10:48.493827 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:10:48.495845 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e289-1d56-89d4-4b76f67364e6) - Closing
[0m22:10:48.637356 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:10:48.644834 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:10:48.673678 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:10:48.676530 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:10:48.679351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:49.126324 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e342-1049-b1d8-d541231b69bf) - Created
[0m22:10:49.928960 [debug] [ThreadPool]: SQL status: OK in 1.250 seconds
[0m22:10:49.935838 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-e342-1049-b1d8-d541231b69bf, command-id=01f0ecde-e355-1582-bb25-d4b870f0f35c) - Closing
[0m22:10:49.938711 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:10:49.941004 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-e342-1049-b1d8-d541231b69bf) - Closing
[0m22:10:50.076619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a36b0e590>]}
[0m22:10:50.083420 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m22:10:50.085916 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m22:10:50.088174 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m22:10:50.089947 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m22:10:50.091731 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m22:10:50.108959 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m22:10:50.124119 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m22:10:50.147700 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:10:50.155381 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:10:50.157529 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12bc7d30>]}
[0m22:10:50.177650 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m22:10:50.191231 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m22:10:50.207174 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m22:10:50.208550 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m22:10:50.210085 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:10:50.631923 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e428-17c7-8a01-d5934866113a) - Created
[0m22:10:51.773537 [debug] [Thread-7 (]: SQL status: OK in 1.560 seconds
[0m22:10:51.776498 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecde-e428-17c7-8a01-d5934866113a, command-id=01f0ecde-e439-1d4e-801f-98bf617455e9) - Closing
[0m22:10:51.804914 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:10:51.813294 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m22:10:51.817289 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e428-17c7-8a01-d5934866113a) - Closing
[0m22:10:51.961219 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12b90dc0>]}
[0m22:10:51.963475 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.87s]
[0m22:10:51.966141 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m22:10:51.969175 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m22:10:51.971188 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m22:10:51.973283 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m22:10:51.975072 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m22:10:51.976924 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m22:10:51.984701 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m22:10:52.001741 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m22:10:52.008847 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:10:52.011847 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m22:10:52.014994 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m22:10:52.032120 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m22:10:52.034732 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m22:10:52.036536 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:10:52.414562 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e53a-10c1-ac4b-1ca3745b2bc8) - Created
[0m22:10:53.386694 [debug] [Thread-7 (]: SQL status: OK in 1.350 seconds
[0m22:10:53.389574 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecde-e53a-10c1-ac4b-1ca3745b2bc8, command-id=01f0ecde-e549-1c59-9e37-8157d0eace26) - Closing
[0m22:10:53.392143 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:10:53.394824 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m22:10:53.396629 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e53a-10c1-ac4b-1ca3745b2bc8) - Closing
[0m22:10:53.517905 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a3a057070>]}
[0m22:10:53.520392 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.54s]
[0m22:10:53.522269 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m22:10:53.524642 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m22:10:53.526330 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m22:10:53.528150 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m22:10:53.529605 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m22:10:53.531102 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m22:10:53.543285 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m22:10:53.558069 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m22:10:53.562051 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:10:53.564376 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m22:10:53.566827 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m22:10:53.579594 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m22:10:53.580954 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m22:10:53.582158 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:10:53.959873 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e625-1aae-8851-4be159e2685b) - Created
[0m22:10:54.898188 [debug] [Thread-7 (]: SQL status: OK in 1.320 seconds
[0m22:10:54.901116 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecde-e625-1aae-8851-4be159e2685b, command-id=01f0ecde-e635-1953-9c70-598bd218929a) - Closing
[0m22:10:54.903931 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:10:54.906438 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m22:10:54.908103 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e625-1aae-8851-4be159e2685b) - Closing
[0m22:10:55.041615 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a12c67c10>]}
[0m22:10:55.043409 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.51s]
[0m22:10:55.045258 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m22:10:55.047221 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m22:10:55.048966 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m22:10:55.051613 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m22:10:55.053460 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m22:10:55.055217 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m22:10:55.063142 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m22:10:55.076156 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m22:10:55.101800 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m22:10:55.120306 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:10:55.122666 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a10213c40>]}
[0m22:10:55.157473 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m22:10:55.174097 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m22:10:55.175461 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m22:10:55.176561 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:10:55.560353 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e718-1035-a563-72c93c94761f) - Created
[0m22:11:01.680619 [debug] [Thread-7 (]: SQL status: OK in 6.500 seconds
[0m22:11:01.685233 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecde-e718-1035-a563-72c93c94761f, command-id=01f0ecde-e72a-11af-bad2-4735fc8267c2) - Closing
[0m22:11:01.861420 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:11:01.885042 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m22:11:01.887075 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecde-e718-1035-a563-72c93c94761f) - Closing
[0m22:11:02.005754 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df2b927a-603d-48e2-99a9-040f30a9564a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a3a057070>]}
[0m22:11:02.007778 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 6.95s]
[0m22:11:02.009485 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m22:11:02.012210 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:11:02.013771 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:11:02.015530 [info ] [MainThread]: 
[0m22:11:02.017256 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 18.92 seconds (18.92s).
[0m22:11:02.020977 [debug] [MainThread]: Command end result
[0m22:11:02.111892 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:11:02.120321 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:11:02.134298 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:11:02.136726 [info ] [MainThread]: 
[0m22:11:02.138010 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:11:02.139119 [info ] [MainThread]: 
[0m22:11:02.140237 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m22:11:02.141977 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 23.439802, "process_in_blocks": "0", "process_kernel_time": 0.753526, "process_mem_max_rss": "246472", "process_out_blocks": "0", "process_user_time": 8.404415}
[0m22:11:02.143160 [debug] [MainThread]: Command `dbt run` succeeded at 22:11:02.143043 after 23.44 seconds
[0m22:11:02.144254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a39dbe2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a38d396c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9a39cb90c0>]}
[0m22:11:02.145371 [debug] [MainThread]: Flushing usage events
[0m22:11:02.594410 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:11:08.152461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73da8e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73c85c310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73c85c2b0>]}


============================== 22:11:08.158231 | a2432198-169f-4b78-8217-d6cff2791956 ==============================
[0m22:11:08.158231 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:11:08.160077 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'debug': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'printer_width': '80', 'log_format': 'default', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'quiet': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'static_parser': 'True', 'indirect_selection': 'eager', 'empty': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'log_cache_events': 'False', 'fail_fast': 'False', 'introspect': 'True'}
[0m22:11:09.138077 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:11:09.140169 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:11:09.141965 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:11:10.357467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73f5510c0>]}
[0m22:11:10.485405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf717473070>]}
[0m22:11:10.487479 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:11:10.691399 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:11:11.174636 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:11:11.176454 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:11:11.190965 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:11:11.287535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73cf68ac0>]}
[0m22:11:11.560679 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:11:11.570615 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:11:11.601942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73cf119f0>]}
[0m22:11:11.603673 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:11:11.605147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73cf12080>]}
[0m22:11:11.608140 [info ] [MainThread]: 
[0m22:11:11.609550 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:11:11.610840 [info ] [MainThread]: 
[0m22:11:11.612483 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:11:11.613899 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:11:11.626035 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:11:11.627796 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:11:11.647977 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:11:11.651368 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:11:11.653232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:11:12.098862 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f0f4-1d91-b14e-dcbffae3150d) - Created
[0m22:11:12.663681 [debug] [ThreadPool]: SQL status: OK in 1.010 seconds
[0m22:11:12.674343 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-f0f4-1d91-b14e-dcbffae3150d, command-id=01f0ecde-f106-1b2b-954d-5d5e11b08ba7) - Closing
[0m22:11:12.677342 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:11:12.679350 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f0f4-1d91-b14e-dcbffae3150d) - Closing
[0m22:11:12.820622 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:11:12.822498 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:11:12.827164 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:11:12.828963 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:11:12.830547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:11:13.226320 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f1a1-1528-8055-e086886180c3) - Created
[0m22:11:13.731105 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m22:11:13.736281 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-f1a1-1528-8055-e086886180c3, command-id=01f0ecde-f1b1-177e-8b38-d5f0ae0b25d4) - Closing
[0m22:11:13.738462 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:11:13.740204 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f1a1-1528-8055-e086886180c3) - Closing
[0m22:11:13.872269 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:11:13.874196 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:11:13.881667 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:11:13.884379 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:11:13.886079 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:11:14.258404 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f23f-135b-aa99-0f77e039a3bc) - Created
[0m22:11:14.786793 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m22:11:14.791033 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecde-f23f-135b-aa99-0f77e039a3bc, command-id=01f0ecde-f24f-119d-82df-9a886ab19bb5) - Closing
[0m22:11:14.793155 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:11:14.794799 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecde-f23f-135b-aa99-0f77e039a3bc) - Closing
[0m22:11:14.917420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2432198-169f-4b78-8217-d6cff2791956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73f363940>]}
[0m22:11:14.922539 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:11:14.924288 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m22:11:14.926009 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m22:11:14.927439 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m22:11:14.929141 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:11:14.956657 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:11:14.974016 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:11:15.005588 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:11:15.022486 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:11:15.024201 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m22:11:15.025556 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:15.402997 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f2ed-120a-a709-c95791647aa1) - Created
[0m22:11:17.070629 [debug] [Thread-4 (]: SQL status: OK in 2.040 seconds
[0m22:11:17.076602 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f2ed-120a-a709-c95791647aa1, command-id=01f0ecde-f2fd-1d0a-a9ff-05524fda4ab2) - Closing
[0m22:11:17.085552 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m22:11:17.088204 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f2ed-120a-a709-c95791647aa1) - Closing
[0m22:11:17.221573 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 2.29s]
[0m22:11:17.225638 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:11:17.228554 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:11:17.231116 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m22:11:17.234494 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m22:11:17.237280 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m22:11:17.239307 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:11:17.247497 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:11:17.263460 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:11:17.270241 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:11:17.280492 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:11:17.281759 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m22:11:17.284025 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:17.705115 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f44c-16df-87c8-d20a7d657a64) - Created
[0m22:11:18.518451 [debug] [Thread-4 (]: SQL status: OK in 1.230 seconds
[0m22:11:18.522716 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f44c-16df-87c8-d20a7d657a64, command-id=01f0ecde-f45d-162a-ba2a-9a5560d8c0a2) - Closing
[0m22:11:18.524824 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m22:11:18.526338 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f44c-16df-87c8-d20a7d657a64) - Closing
[0m22:11:18.641667 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.41s]
[0m22:11:18.644958 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:11:18.647838 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:11:18.651143 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m22:11:18.655012 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m22:11:18.657198 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m22:11:18.659389 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:11:18.671411 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:11:18.688438 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:11:18.696609 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:11:18.710996 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:11:18.712184 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m22:11:18.713298 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:19.075951 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f51e-1a3d-a94e-7583f2b51871) - Created
[0m22:11:20.041102 [debug] [Thread-4 (]: SQL status: OK in 1.330 seconds
[0m22:11:20.053843 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f51e-1a3d-a94e-7583f2b51871, command-id=01f0ecde-f52e-1251-9a1d-3b17386923ad) - Closing
[0m22:11:20.057647 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m22:11:20.059917 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f51e-1a3d-a94e-7583f2b51871) - Closing
[0m22:11:20.194474 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.54s]
[0m22:11:20.198485 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:11:20.203178 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:11:20.206755 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m22:11:20.210456 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m22:11:20.213522 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m22:11:20.216584 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:11:20.235414 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:11:20.253801 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:11:20.259846 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:11:20.275261 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:11:20.276544 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m22:11:20.277671 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:20.686364 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f613-17c8-a0cb-18952d27ab73) - Created
[0m22:11:21.703915 [debug] [Thread-4 (]: SQL status: OK in 1.430 seconds
[0m22:11:21.710506 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f613-17c8-a0cb-18952d27ab73, command-id=01f0ecde-f623-1af8-ab7c-bac5a8513e54) - Closing
[0m22:11:21.713288 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m22:11:21.715401 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f613-17c8-a0cb-18952d27ab73) - Closing
[0m22:11:21.843611 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.63s]
[0m22:11:21.845459 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:11:21.846917 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:11:21.848609 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m22:11:21.851030 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m22:11:21.853295 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m22:11:21.854980 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:11:21.864228 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:11:21.878380 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:11:21.883866 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:11:21.897479 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:11:21.899105 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m22:11:21.901426 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:22.284875 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f706-1a91-b56a-1ce9c8bd4a34) - Created
[0m22:11:22.997515 [debug] [Thread-4 (]: SQL status: OK in 1.100 seconds
[0m22:11:23.003055 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f706-1a91-b56a-1ce9c8bd4a34, command-id=01f0ecde-f718-1049-b2cb-9ce04e97c2c9) - Closing
[0m22:11:23.005450 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m22:11:23.007241 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f706-1a91-b56a-1ce9c8bd4a34) - Closing
[0m22:11:23.136829 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.29s]
[0m22:11:23.139202 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:11:23.140848 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:11:23.142366 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m22:11:23.144174 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m22:11:23.145519 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m22:11:23.146948 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:11:23.160265 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:11:23.175553 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:11:23.180299 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:11:23.194753 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:11:23.196191 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m22:11:23.197544 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:23.593196 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f7cf-1851-a5c0-6d8261a718e0) - Created
[0m22:11:24.248636 [debug] [Thread-4 (]: SQL status: OK in 1.050 seconds
[0m22:11:24.254643 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f7cf-1851-a5c0-6d8261a718e0, command-id=01f0ecde-f7df-13f5-a660-237999cddf99) - Closing
[0m22:11:24.256896 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m22:11:24.258305 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f7cf-1851-a5c0-6d8261a718e0) - Closing
[0m22:11:24.396754 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.25s]
[0m22:11:24.398713 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:11:24.400289 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:11:24.403546 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m22:11:24.405453 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m22:11:24.406722 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m22:11:24.407875 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:11:24.418864 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:11:24.434647 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:11:24.441062 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:11:24.457145 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:11:24.458983 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m22:11:24.460587 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:24.840132 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f88d-17f9-addc-2726cd40171c) - Created
[0m22:11:25.507565 [debug] [Thread-4 (]: SQL status: OK in 1.050 seconds
[0m22:11:25.513374 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f88d-17f9-addc-2726cd40171c, command-id=01f0ecde-f89d-1d20-bf58-587d464b09ec) - Closing
[0m22:11:25.515862 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m22:11:25.518329 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f88d-17f9-addc-2726cd40171c) - Closing
[0m22:11:25.656950 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.25s]
[0m22:11:25.660532 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:11:25.663543 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:11:25.666332 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m22:11:25.671966 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m22:11:25.675110 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m22:11:25.677324 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:11:25.692399 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:11:25.709096 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:11:25.714974 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:11:25.730645 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:11:25.732173 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:11:25.733787 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:26.120392 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f94f-1711-8210-b98dc7443b65) - Created
[0m22:11:26.790865 [debug] [Thread-4 (]: SQL status: OK in 1.060 seconds
[0m22:11:26.794409 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-f94f-1711-8210-b98dc7443b65, command-id=01f0ecde-f962-1381-9c6c-e58c946d7c6e) - Closing
[0m22:11:26.796536 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m22:11:26.798143 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-f94f-1711-8210-b98dc7443b65) - Closing
[0m22:11:26.919995 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.25s]
[0m22:11:26.922045 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:11:26.923651 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:11:26.925234 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m22:11:26.926943 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m22:11:26.928360 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m22:11:26.929726 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:11:26.939615 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:11:26.950975 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:11:26.957421 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:11:26.971543 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:11:26.972790 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:11:26.973851 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:27.332088 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fa09-16c8-bbe4-98538bf38681) - Created
[0m22:11:27.947358 [debug] [Thread-4 (]: SQL status: OK in 0.970 seconds
[0m22:11:27.953219 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fa09-16c8-bbe4-98538bf38681, command-id=01f0ecde-fa1a-1075-83d3-d44a0a8c8aeb) - Closing
[0m22:11:27.955657 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m22:11:27.957527 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fa09-16c8-bbe4-98538bf38681) - Closing
[0m22:11:28.100218 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.17s]
[0m22:11:28.103308 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:11:28.105222 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:11:28.106876 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m22:11:28.108705 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m22:11:28.110298 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m22:11:28.111792 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:11:28.113154 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:11:28.166619 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:28.169564 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:11:28.171497 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:28.564604 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921) - Created
[0m22:11:28.963601 [debug] [Thread-4 (]: SQL status: OK in 0.790 seconds
[0m22:11:28.966665 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921, command-id=01f0ecde-fad5-1e77-b25d-f6c4423c39a4) - Closing
[0m22:11:28.992322 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.016900 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.084327 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.088212 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:11:29.486180 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m22:11:29.498178 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921, command-id=01f0ecde-fb29-1b2e-b6c8-b626b71baac0) - Closing
[0m22:11:29.515948 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.522091 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m22:11:29.776160 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:11:29.787861 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921, command-id=01f0ecde-fb68-1372-b9fb-2b66d408f4c9) - Closing
[0m22:11:29.813026 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.839002 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:29.840533 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:11:30.313059 [debug] [Thread-4 (]: SQL status: OK in 0.470 seconds
[0m22:11:30.318790 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921, command-id=01f0ecde-fb98-1f3d-9d2e-d97757df6971) - Closing
[0m22:11:30.329867 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:11:30.337135 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:11:30.338967 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:11:30.622921 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m22:11:30.625264 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921, command-id=01f0ecde-fbe4-1713-bc12-6d908d0b463b) - Closing
[0m22:11:30.631347 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m22:11:30.633014 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fac4-1c42-9bca-23d85cbed921) - Closing
[0m22:11:30.766698 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.66s]
[0m22:11:30.771372 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:11:30.774529 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:11:30.777307 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m22:11:30.780366 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m22:11:30.783045 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m22:11:30.786776 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:11:30.790463 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:11:30.811393 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:30.813080 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:11:30.814661 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:31.208369 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec) - Created
[0m22:11:31.546013 [debug] [Thread-4 (]: SQL status: OK in 0.730 seconds
[0m22:11:31.550496 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec, command-id=01f0ecde-fc69-1082-9997-eab9e34e850b) - Closing
[0m22:11:31.557875 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:31.582467 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:31.607515 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:31.609321 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:11:31.889964 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m22:11:31.892846 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec, command-id=01f0ecde-fca6-12e6-84c7-d46e278742b8) - Closing
[0m22:11:31.897862 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:31.899638 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m22:11:32.261581 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m22:11:32.265838 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec, command-id=01f0ecde-fcd2-17f8-a421-9b196e25b00b) - Closing
[0m22:11:32.271474 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:32.291616 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:32.293602 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:11:32.621093 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m22:11:32.627444 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec, command-id=01f0ecde-fd0f-143c-af47-15c32376beca) - Closing
[0m22:11:32.636567 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:11:32.639413 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:11:32.641270 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:11:32.874471 [debug] [Thread-4 (]: SQL status: OK in 0.230 seconds
[0m22:11:32.877164 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec, command-id=01f0ecde-fd43-18be-b751-18a7f40de9f5) - Closing
[0m22:11:32.880862 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m22:11:32.882568 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fc57-18e0-9e61-9e77c47043ec) - Closing
[0m22:11:33.012071 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.23s]
[0m22:11:33.013729 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:11:33.015111 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:11:33.016535 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m22:11:33.018637 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m22:11:33.021511 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m22:11:33.023553 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:11:33.024803 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:11:33.041027 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:33.042552 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:11:33.043976 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:11:33.441514 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a) - Created
[0m22:11:33.728383 [debug] [Thread-4 (]: SQL status: OK in 0.680 seconds
[0m22:11:33.733309 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fdbd-189e-8e67-aaa6ca0d8fdd) - Closing
[0m22:11:33.742437 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:33.770533 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:33.796510 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:33.798470 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:11:34.092373 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m22:11:34.098708 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fdf4-1f37-accf-56291967b298) - Closing
[0m22:11:34.105533 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:34.108722 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:11:34.510508 [debug] [Thread-4 (]: SQL status: OK in 0.400 seconds
[0m22:11:34.516570 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fe26-1013-b426-9eff2ec6000d) - Closing
[0m22:11:34.525077 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:34.526986 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m22:11:34.762532 [debug] [Thread-4 (]: SQL status: OK in 0.230 seconds
[0m22:11:34.775760 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fe63-1812-84b4-cceea1c89749) - Closing
[0m22:11:34.782787 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:34.809175 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:34.810526 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:11:35.226892 [debug] [Thread-4 (]: SQL status: OK in 0.420 seconds
[0m22:11:35.230548 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fe90-1df9-a791-ce826ec97c26) - Closing
[0m22:11:35.236725 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m22:11:35.239199 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:11:35.240764 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m22:11:35.488256 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:11:35.490603 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a, command-id=01f0ecde-fed0-1a80-ae9d-669465da370e) - Closing
[0m22:11:35.495476 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m22:11:35.497305 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecde-fdad-14fa-91a6-b36b5b622d3a) - Closing
[0m22:11:35.624347 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.61s]
[0m22:11:35.626608 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:11:35.629939 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:11:35.631378 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:11:35.633127 [info ] [MainThread]: 
[0m22:11:35.634883 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 24.02 seconds (24.02s).
[0m22:11:35.639973 [debug] [MainThread]: Command end result
[0m22:11:35.876945 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:11:35.887817 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:11:35.906082 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:11:35.907505 [info ] [MainThread]: 
[0m22:11:35.908915 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:11:35.910398 [info ] [MainThread]: 
[0m22:11:35.911768 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m22:11:35.913162 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m22:11:35.914409 [info ] [MainThread]: 
[0m22:11:35.915847 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m22:11:35.917511 [info ] [MainThread]: 
[0m22:11:35.919218 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m22:11:35.922510 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 27.916092, "process_in_blocks": "0", "process_kernel_time": 0.744725, "process_mem_max_rss": "255852", "process_out_blocks": "0", "process_user_time": 7.347693}
[0m22:11:35.924234 [debug] [MainThread]: Command `dbt test` failed at 22:11:35.924081 after 27.92 seconds
[0m22:11:35.925763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73da8e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73d953d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cf73cf2a8c0>]}
[0m22:11:35.927244 [debug] [MainThread]: Flushing usage events
[0m22:11:36.379777 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:16:50.359760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e80ffe470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e7fe242b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e7fe24250>]}


============================== 22:16:50.370418 | 11e199a0-3775-4eea-b76b-83d7ecd900e6 ==============================
[0m22:16:50.370418 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:16:50.373072 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'partial_parse': 'True', 'write_json': 'True', 'no_print': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'quiet': 'False', 'fail_fast': 'False', 'cache_selected_only': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'printer_width': '80', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'version_check': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'log_cache_events': 'False', 'warn_error': 'None'}
[0m22:16:51.721801 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:16:51.724021 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:16:51.726280 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:16:53.003749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e7de28040>]}
[0m22:16:53.159196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e7e079b10>]}
[0m22:16:53.161682 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:16:53.415914 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:16:53.997694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:16:53.999461 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:16:54.017574 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:16:54.146968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e80554b50>]}
[0m22:16:54.446438 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:16:54.456081 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:16:54.487732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e59f20310>]}
[0m22:16:54.489518 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:16:54.491372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e5a9eba00>]}
[0m22:16:54.495446 [info ] [MainThread]: 
[0m22:16:54.497143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:16:54.498798 [info ] [MainThread]: 
[0m22:16:54.500644 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:16:54.502067 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:16:54.515193 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:16:54.517084 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:16:54.535955 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:16:54.538057 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:16:54.539672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:55.125977 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bd69-10d2-937d-70f91881895b) - Created
[0m22:16:55.672757 [debug] [ThreadPool]: SQL status: OK in 1.130 seconds
[0m22:16:55.684204 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-bd69-10d2-937d-70f91881895b, command-id=01f0ecdf-bd7e-1bbf-8823-953b72c9c08d) - Closing
[0m22:16:55.687207 [debug] [ThreadPool]: On list_workspace: Close
[0m22:16:55.689444 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bd69-10d2-937d-70f91881895b) - Closing
[0m22:16:55.850454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:16:55.852347 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:16:55.864406 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:16:55.866480 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:16:55.868163 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:56.291465 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-be1a-12ad-a9e3-8f21d2fb703a) - Created
[0m22:16:56.815087 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m22:16:56.819717 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-be1a-12ad-a9e3-8f21d2fb703a, command-id=01f0ecdf-be2f-1403-9f75-617c2399eff8) - Closing
[0m22:16:56.821929 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:16:56.823461 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-be1a-12ad-a9e3-8f21d2fb703a) - Closing
[0m22:16:56.988162 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:16:56.997852 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:16:57.019551 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:16:57.021803 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:16:57.023676 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:57.411956 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bec8-1080-abf7-bc1bc1bb4ed7) - Created
[0m22:16:57.962730 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m22:16:57.969776 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-bec8-1080-abf7-bc1bc1bb4ed7, command-id=01f0ecdf-bed7-1f05-8507-efe8a3328a13) - Closing
[0m22:16:57.972777 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:16:57.975545 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bec8-1080-abf7-bc1bc1bb4ed7) - Closing
[0m22:16:58.133225 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:16:58.136369 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:16:58.147537 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:16:58.149415 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:16:58.151113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:58.562363 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bf77-1154-8baa-981bf824b6ae) - Created
[0m22:16:59.016388 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m22:16:59.020260 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-bf77-1154-8baa-981bf824b6ae, command-id=01f0ecdf-bf88-1786-9788-17b53fa05a91) - Closing
[0m22:16:59.022247 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:16:59.024072 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-bf77-1154-8baa-981bf824b6ae) - Closing
[0m22:16:59.155294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e82b9b820>]}
[0m22:16:59.165173 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m22:16:59.167115 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m22:16:59.169098 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m22:16:59.170655 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m22:16:59.172205 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m22:16:59.173755 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m22:16:59.183864 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:16:59.185722 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e58e3dc30>]}
[0m22:16:59.246121 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:16:59.248728 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e58e5e800>]}
[0m22:16:59.285434 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:16:59.287869 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:16:59.289896 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:16:59.668556 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecdf-c020-171f-bba3-7dd1df4231e5) - Created
[0m22:17:01.552246 [debug] [Thread-5 (]: SQL status: OK in 2.260 seconds
[0m22:17:01.557779 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecdf-c020-171f-bba3-7dd1df4231e5, command-id=01f0ecdf-c035-1761-ab02-a6545ea19235) - Closing
[0m22:17:01.589554 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:17:01.591617 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m22:17:03.455577 [debug] [Thread-5 (]: SQL status: OK in 1.860 seconds
[0m22:17:03.460674 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecdf-c020-171f-bba3-7dd1df4231e5, command-id=01f0ecdf-c156-131f-811b-957aef2ec5aa) - Closing
[0m22:17:03.473730 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m22:17:03.501719 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m22:17:03.503362 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecdf-c020-171f-bba3-7dd1df4231e5) - Closing
[0m22:17:03.664919 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e59f38d60>]}
[0m22:17:03.669326 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 4.49s]
[0m22:17:03.673065 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m22:17:03.675928 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m22:17:03.679175 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m22:17:03.681664 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m22:17:03.683644 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m22:17:03.685654 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m22:17:03.687719 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m22:17:03.705594 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:17:03.707194 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:17:03.708930 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:17:04.158548 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecdf-c2ca-13a0-b2b5-a7621e2e0a7b) - Created
[0m22:17:05.614510 [debug] [Thread-5 (]: SQL status: OK in 1.910 seconds
[0m22:17:05.623953 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecdf-c2ca-13a0-b2b5-a7621e2e0a7b, command-id=01f0ecdf-c2df-1789-96b1-b89f457bd8a6) - Closing
[0m22:17:05.636230 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:17:05.640706 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m22:17:07.533637 [debug] [Thread-5 (]: SQL status: OK in 1.890 seconds
[0m22:17:07.535422 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecdf-c2ca-13a0-b2b5-a7621e2e0a7b, command-id=01f0ecdf-c3c0-1d89-8618-3e474c570cc8) - Closing
[0m22:17:07.537294 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m22:17:07.552337 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m22:17:07.554055 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecdf-c2ca-13a0-b2b5-a7621e2e0a7b) - Closing
[0m22:17:07.679556 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11e199a0-3775-4eea-b76b-83d7ecd900e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e5a8c0610>]}
[0m22:17:07.681788 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 4.00s]
[0m22:17:07.683803 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m22:17:07.687011 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:17:07.688792 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:17:07.690699 [info ] [MainThread]: 
[0m22:17:07.692550 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 13.19 seconds (13.19s).
[0m22:17:07.695181 [debug] [MainThread]: Command end result
[0m22:17:07.790278 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:17:07.800777 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:17:07.816660 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:17:07.818065 [info ] [MainThread]: 
[0m22:17:07.819479 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:17:07.820962 [info ] [MainThread]: 
[0m22:17:07.822398 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:17:07.824856 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 17.648712, "process_in_blocks": "0", "process_kernel_time": 0.880511, "process_mem_max_rss": "246604", "process_out_blocks": "712", "process_user_time": 7.836549}
[0m22:17:07.827292 [debug] [MainThread]: Command `dbt seed` succeeded at 22:17:07.827066 after 17.65 seconds
[0m22:17:07.828813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e80ffe470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e80f6f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e5aabf490>]}
[0m22:17:07.830204 [debug] [MainThread]: Flushing usage events
[0m22:17:08.242943 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:17:13.689317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2b59e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2a368280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2a368220>]}


============================== 22:17:13.694760 | 1c13bf5d-3e7e-4259-8275-55f83015d630 ==============================
[0m22:17:13.694760 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:17:13.696482 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'indirect_selection': 'eager', 'empty': 'False', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'debug': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'printer_width': '80', 'warn_error': 'None', 'quiet': 'False', 'static_parser': 'True', 'log_format': 'default', 'write_json': 'True', 'introspect': 'True'}
[0m22:17:14.828167 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:17:14.829836 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:17:14.831381 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:17:15.915568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2d05d0c0>]}
[0m22:17:16.036154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2cc7e980>]}
[0m22:17:16.038359 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:17:16.230440 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:17:16.739704 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:17:16.741671 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:17:16.758136 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:17:16.861513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2aa83e50>]}
[0m22:17:17.117383 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:17:17.125996 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:17:17.147136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2aa81c30>]}
[0m22:17:17.148582 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:17:17.149893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2aa819f0>]}
[0m22:17:17.152536 [info ] [MainThread]: 
[0m22:17:17.153944 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:17:17.155279 [info ] [MainThread]: 
[0m22:17:17.156750 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:17:17.158745 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:17:17.170057 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:17:17.172033 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:17:17.188115 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:17:17.190588 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:17:17.193257 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:17.699958 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cade-15a3-9ee3-1255b6b488c2) - Created
[0m22:17:18.027259 [debug] [ThreadPool]: SQL status: OK in 0.830 seconds
[0m22:17:18.036027 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cade-15a3-9ee3-1255b6b488c2, command-id=01f0ecdf-caf0-1e78-9762-686dbb0932fd) - Closing
[0m22:17:18.038428 [debug] [ThreadPool]: On list_workspace: Close
[0m22:17:18.040387 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cade-15a3-9ee3-1255b6b488c2) - Closing
[0m22:17:18.168156 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:17:18.172270 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:17:18.185518 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:17:18.188534 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:17:18.191005 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:18.566996 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cb64-1178-a402-3760936495dd) - Created
[0m22:17:18.911095 [debug] [ThreadPool]: SQL status: OK in 0.720 seconds
[0m22:17:18.914617 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cb64-1178-a402-3760936495dd, command-id=01f0ecdf-cb75-15ee-a688-97038b8bee25) - Closing
[0m22:17:18.916472 [debug] [ThreadPool]: On list_workspace: Close
[0m22:17:18.918142 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cb64-1178-a402-3760936495dd) - Closing
[0m22:17:19.052434 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:17:19.053678 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:17:19.065559 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:17:19.066993 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:17:19.068432 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:19.432497 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cbe7-18eb-9689-0c4d96acc2d9) - Created
[0m22:17:19.785269 [debug] [ThreadPool]: SQL status: OK in 0.720 seconds
[0m22:17:19.793739 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cbe7-18eb-9689-0c4d96acc2d9, command-id=01f0ecdf-cbf8-1af8-bb63-ded0f476f3b7) - Closing
[0m22:17:19.796658 [debug] [ThreadPool]: On list_workspace: Close
[0m22:17:19.798960 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cbe7-18eb-9689-0c4d96acc2d9) - Closing
[0m22:17:19.923293 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:17:19.924898 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:17:19.935958 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:17:19.937597 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:17:19.938874 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:20.326386 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cc70-12ce-85c3-589c914eb507) - Created
[0m22:17:20.868492 [debug] [ThreadPool]: SQL status: OK in 0.930 seconds
[0m22:17:20.872678 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cc70-12ce-85c3-589c914eb507, command-id=01f0ecdf-cc81-13b3-983a-1faa89757ac9) - Closing
[0m22:17:20.874699 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:17:20.876827 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cc70-12ce-85c3-589c914eb507) - Closing
[0m22:17:21.004411 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:17:21.006287 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:17:21.011606 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:17:21.013239 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:17:21.014597 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:21.388251 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cd12-13ae-bf92-c300ec1b2182) - Created
[0m22:17:21.762258 [debug] [ThreadPool]: SQL status: OK in 0.750 seconds
[0m22:17:21.769320 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cd12-13ae-bf92-c300ec1b2182, command-id=01f0ecdf-cd22-1fbf-9774-45e05d31da55) - Closing
[0m22:17:21.772931 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:17:21.776301 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cd12-13ae-bf92-c300ec1b2182) - Closing
[0m22:17:21.917071 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:17:21.919535 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:17:21.930234 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:17:21.932513 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:17:21.934283 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:22.327408 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cda1-14aa-bc2c-fdf1423fbf98) - Created
[0m22:17:22.748494 [debug] [ThreadPool]: SQL status: OK in 0.810 seconds
[0m22:17:22.753843 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-cda1-14aa-bc2c-fdf1423fbf98, command-id=01f0ecdf-cdb1-1ac5-b692-561887c64780) - Closing
[0m22:17:22.756356 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:17:22.758326 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-cda1-14aa-bc2c-fdf1423fbf98) - Closing
[0m22:17:22.898995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2832e560>]}
[0m22:17:22.906566 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m22:17:22.909475 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m22:17:22.912286 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m22:17:22.914137 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m22:17:22.915670 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m22:17:22.931135 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m22:17:22.943899 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m22:17:22.966988 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:17:22.970954 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:17:22.972777 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c083a3d30>]}
[0m22:17:22.992620 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m22:17:23.007773 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m22:17:23.023372 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m22:17:23.026019 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m22:17:23.028177 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:17:23.411945 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-ce46-1d03-9cce-896bc238df32) - Created
[0m22:17:24.131996 [debug] [Thread-7 (]: SQL status: OK in 1.100 seconds
[0m22:17:24.134226 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecdf-ce46-1d03-9cce-896bc238df32, command-id=01f0ecdf-ce57-1b0e-9038-75434741f55b) - Closing
[0m22:17:24.148426 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:17:24.152163 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m22:17:24.153735 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-ce46-1d03-9cce-896bc238df32) - Closing
[0m22:17:24.284973 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c0836cc70>]}
[0m22:17:24.288444 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.37s]
[0m22:17:24.292370 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m22:17:24.295821 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m22:17:24.298971 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m22:17:24.302169 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m22:17:24.304592 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m22:17:24.306730 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m22:17:24.316304 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m22:17:24.331445 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m22:17:24.336434 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:17:24.339035 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m22:17:24.340848 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m22:17:24.353301 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m22:17:24.354685 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m22:17:24.355946 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:17:24.727168 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-cf0f-1b24-9516-c82bd8c9a63e) - Created
[0m22:17:25.654106 [debug] [Thread-7 (]: SQL status: OK in 1.300 seconds
[0m22:17:25.656174 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecdf-cf0f-1b24-9516-c82bd8c9a63e, command-id=01f0ecdf-cf20-18cb-a156-cc97d887c640) - Closing
[0m22:17:25.657917 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:17:25.661347 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m22:17:25.662791 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-cf0f-1b24-9516-c82bd8c9a63e) - Closing
[0m22:17:25.826388 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c083a7b50>]}
[0m22:17:25.830617 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.52s]
[0m22:17:25.833918 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m22:17:25.837019 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m22:17:25.839400 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m22:17:25.842310 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m22:17:25.844856 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m22:17:25.846932 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m22:17:25.859515 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m22:17:25.873914 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m22:17:25.879136 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:17:25.881332 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m22:17:25.882801 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m22:17:25.893230 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m22:17:25.894679 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m22:17:25.895662 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:17:26.451790 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-d016-1976-91f7-f17fa400afe5) - Created
[0m22:17:27.184162 [debug] [Thread-7 (]: SQL status: OK in 1.290 seconds
[0m22:17:27.186377 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecdf-d016-1976-91f7-f17fa400afe5, command-id=01f0ecdf-d028-1909-957e-0bae36ba4aff) - Closing
[0m22:17:27.188303 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:17:27.190577 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m22:17:27.192155 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-d016-1976-91f7-f17fa400afe5) - Closing
[0m22:17:27.316744 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2aa78520>]}
[0m22:17:27.319234 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.47s]
[0m22:17:27.321477 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m22:17:27.324316 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m22:17:27.327383 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m22:17:27.330003 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m22:17:27.332242 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m22:17:27.333916 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m22:17:27.350919 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m22:17:27.366607 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m22:17:27.398289 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m22:17:27.417737 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:17:27.419569 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c083f3ac0>]}
[0m22:17:27.454445 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m22:17:27.467103 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m22:17:27.468782 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m22:17:27.470384 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:17:27.831208 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-d0ea-174c-97e6-9daf2e92113d) - Created
[0m22:17:31.485905 [debug] [Thread-7 (]: SQL status: OK in 4.020 seconds
[0m22:17:31.491264 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ecdf-d0ea-174c-97e6-9daf2e92113d, command-id=01f0ecdf-d0fa-10b4-8b1f-7c8c866bdd7e) - Closing
[0m22:17:31.504092 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:17:31.533235 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m22:17:31.535113 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ecdf-d0ea-174c-97e6-9daf2e92113d) - Closing
[0m22:17:31.691412 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c13bf5d-3e7e-4259-8275-55f83015d630', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c08ae4280>]}
[0m22:17:31.698931 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 4.36s]
[0m22:17:31.705556 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m22:17:31.717672 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:17:31.721562 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:17:31.726188 [info ] [MainThread]: 
[0m22:17:31.731007 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 14.57 seconds (14.57s).
[0m22:17:31.737544 [debug] [MainThread]: Command end result
[0m22:17:31.834338 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:17:31.840455 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:17:31.853286 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:17:31.854436 [info ] [MainThread]: 
[0m22:17:31.855557 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:17:31.856704 [info ] [MainThread]: 
[0m22:17:31.857949 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m22:17:31.860745 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.298367, "process_in_blocks": "0", "process_kernel_time": 0.597528, "process_mem_max_rss": "246200", "process_out_blocks": "0", "process_user_time": 6.6724}
[0m22:17:31.862105 [debug] [MainThread]: Command `dbt run` succeeded at 22:17:31.861976 after 18.30 seconds
[0m22:17:31.863333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2b59e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2cc7e980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708c2b460e20>]}
[0m22:17:31.864524 [debug] [MainThread]: Flushing usage events
[0m22:17:32.339345 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:17:38.270771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81432de230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d814208c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d814208c250>]}


============================== 22:17:38.276137 | efaed147-d7cf-4e2b-bc61-076774643a4d ==============================
[0m22:17:38.276137 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:17:38.279059 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'empty': 'None', 'quiet': 'False', 'warn_error': 'None', 'no_print': 'None', 'indirect_selection': 'eager', 'log_format': 'default', 'cache_selected_only': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'log_path': '/opt/dagster/app/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'printer_width': '80', 'use_colors': 'True', 'introspect': 'True', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m22:17:39.272185 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:17:39.274270 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:17:39.276311 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:17:40.348162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d814006b310>]}
[0m22:17:40.472913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d8142066da0>]}
[0m22:17:40.475356 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:17:40.663995 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:17:41.163860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:17:41.165767 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:17:41.181892 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:17:41.282076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d812016ca60>]}
[0m22:17:41.536115 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:17:41.545150 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:17:41.575300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81427a85b0>]}
[0m22:17:41.577271 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:17:41.578951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81427a9e70>]}
[0m22:17:41.582210 [info ] [MainThread]: 
[0m22:17:41.584147 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:17:41.585672 [info ] [MainThread]: 
[0m22:17:41.587692 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:17:41.589124 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:17:41.601372 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:17:41.603336 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:17:41.625136 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:17:41.627452 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:17:41.629426 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:42.021446 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-d95f-1f54-b2ab-6c8afc5bb4d0) - Created
[0m22:17:42.553931 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m22:17:42.565009 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-d95f-1f54-b2ab-6c8afc5bb4d0, command-id=01f0ecdf-d96e-18c3-9a4f-e4cc9227b9d3) - Closing
[0m22:17:42.567578 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:17:42.569563 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-d95f-1f54-b2ab-6c8afc5bb4d0) - Closing
[0m22:17:42.814677 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:17:42.819363 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:17:42.832039 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:17:42.835577 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:17:42.838611 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:43.759923 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-da5b-1773-b8b1-811c9666234e) - Created
[0m22:17:44.149362 [debug] [ThreadPool]: SQL status: OK in 1.310 seconds
[0m22:17:44.152823 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-da5b-1773-b8b1-811c9666234e, command-id=01f0ecdf-da79-1318-a4fa-a74083d0ba51) - Closing
[0m22:17:44.154587 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:17:44.155905 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-da5b-1773-b8b1-811c9666234e) - Closing
[0m22:17:44.304972 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:17:44.306689 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:17:44.313774 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:17:44.315392 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:17:44.316740 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:44.701138 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-daf7-1da2-b9d8-52d1f37f2e36) - Created
[0m22:17:45.227085 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m22:17:45.236021 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecdf-daf7-1da2-b9d8-52d1f37f2e36, command-id=01f0ecdf-db08-1800-a401-5edbc974410a) - Closing
[0m22:17:45.241453 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:17:45.246141 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecdf-daf7-1da2-b9d8-52d1f37f2e36) - Closing
[0m22:17:45.392093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efaed147-d7cf-4e2b-bc61-076774643a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d8144ba1ba0>]}
[0m22:17:45.417153 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:17:45.423666 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m22:17:45.432265 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m22:17:45.436735 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m22:17:45.441220 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:17:45.482001 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:17:45.497992 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:17:45.522787 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:17:45.536005 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:17:45.537275 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m22:17:45.538454 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:45.924293 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dbb1-17ad-bbb5-2b027d7f90f9) - Created
[0m22:17:46.472716 [debug] [Thread-4 (]: SQL status: OK in 0.930 seconds
[0m22:17:46.478135 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-dbb1-17ad-bbb5-2b027d7f90f9, command-id=01f0ecdf-dbc3-14f8-922e-2d4f24703cab) - Closing
[0m22:17:46.484113 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m22:17:46.485822 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dbb1-17ad-bbb5-2b027d7f90f9) - Closing
[0m22:17:46.623929 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.19s]
[0m22:17:46.629345 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:17:46.632577 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:17:46.635561 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m22:17:46.639661 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m22:17:46.643845 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m22:17:46.646377 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:17:46.654668 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:17:46.672915 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:17:46.678540 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:17:46.695049 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:17:46.696548 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m22:17:46.698058 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:47.084520 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dc63-16e2-8f2a-a35a6a50c7ef) - Created
[0m22:17:47.587250 [debug] [Thread-4 (]: SQL status: OK in 0.890 seconds
[0m22:17:47.591079 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-dc63-16e2-8f2a-a35a6a50c7ef, command-id=01f0ecdf-dc75-1534-8e07-06fe4a5597e1) - Closing
[0m22:17:47.593733 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m22:17:47.595661 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dc63-16e2-8f2a-a35a6a50c7ef) - Closing
[0m22:17:47.721752 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.08s]
[0m22:17:47.724113 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:17:47.726091 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:17:47.728197 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m22:17:47.730598 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m22:17:47.732035 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m22:17:47.733492 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:17:47.744026 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:17:47.756787 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:17:47.770812 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:17:47.789225 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:17:47.790860 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m22:17:47.792617 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:48.164575 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dd09-1af6-9405-b7be48e5e0fc) - Created
[0m22:17:48.731727 [debug] [Thread-4 (]: SQL status: OK in 0.940 seconds
[0m22:17:48.735299 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-dd09-1af6-9405-b7be48e5e0fc, command-id=01f0ecdf-dd18-1652-a74c-7b0e9d8cc86d) - Closing
[0m22:17:48.737220 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m22:17:48.738690 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dd09-1af6-9405-b7be48e5e0fc) - Closing
[0m22:17:48.878093 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.15s]
[0m22:17:48.881966 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:17:48.884415 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:17:48.886956 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m22:17:48.891639 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m22:17:48.896419 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m22:17:48.898573 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:17:48.911713 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:17:48.929121 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:17:48.937606 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:17:48.955372 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:17:48.956796 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m22:17:48.958087 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:49.322065 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-ddba-18c6-a1d1-d9e46bf65712) - Created
[0m22:17:49.950521 [debug] [Thread-4 (]: SQL status: OK in 0.990 seconds
[0m22:17:49.955224 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-ddba-18c6-a1d1-d9e46bf65712, command-id=01f0ecdf-ddc9-1d6f-b4fc-e6743833acb8) - Closing
[0m22:17:49.957780 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m22:17:49.960292 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-ddba-18c6-a1d1-d9e46bf65712) - Closing
[0m22:17:50.099927 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.21s]
[0m22:17:50.105022 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:17:50.108467 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:17:50.113317 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m22:17:50.117273 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m22:17:50.119426 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m22:17:50.121384 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:17:50.133045 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:17:50.147845 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:17:50.154802 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:17:50.171520 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:17:50.173125 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m22:17:50.174430 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:50.553329 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-de75-10ff-9e83-4bcc47dee079) - Created
[0m22:17:51.150852 [debug] [Thread-4 (]: SQL status: OK in 0.980 seconds
[0m22:17:51.155006 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-de75-10ff-9e83-4bcc47dee079, command-id=01f0ecdf-de85-13c7-b6ab-f5a2b83a8f01) - Closing
[0m22:17:51.156991 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m22:17:51.158589 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-de75-10ff-9e83-4bcc47dee079) - Closing
[0m22:17:51.276779 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.16s]
[0m22:17:51.279289 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:17:51.280799 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:17:51.282217 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m22:17:51.284431 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m22:17:51.285991 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m22:17:51.287426 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:17:51.299855 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:17:51.314686 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:17:51.321100 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:17:51.340411 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:17:51.341871 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m22:17:51.343783 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:51.715670 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-df25-1ba4-b0f2-de489de98eb8) - Created
[0m22:17:52.171757 [debug] [Thread-4 (]: SQL status: OK in 0.830 seconds
[0m22:17:52.176480 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-df25-1ba4-b0f2-de489de98eb8, command-id=01f0ecdf-df36-1988-9c88-dc5a90322637) - Closing
[0m22:17:52.179052 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m22:17:52.180823 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-df25-1ba4-b0f2-de489de98eb8) - Closing
[0m22:17:52.312031 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.03s]
[0m22:17:52.314840 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:17:52.317074 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:17:52.319402 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m22:17:52.322248 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m22:17:52.324037 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m22:17:52.326294 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:17:52.338611 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:17:52.357592 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:17:52.365438 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:17:52.386393 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:17:52.387973 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m22:17:52.389342 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:52.777886 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dfc6-18e2-8399-bf618d621cf0) - Created
[0m22:17:53.313172 [debug] [Thread-4 (]: SQL status: OK in 0.920 seconds
[0m22:17:53.319767 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-dfc6-18e2-8399-bf618d621cf0, command-id=01f0ecdf-dfd8-1e93-a4b3-00221002f17e) - Closing
[0m22:17:53.322651 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m22:17:53.324810 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-dfc6-18e2-8399-bf618d621cf0) - Closing
[0m22:17:53.452906 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.13s]
[0m22:17:53.455733 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:17:53.457580 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:17:53.459896 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m22:17:53.463182 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m22:17:53.464949 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m22:17:53.466835 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:17:53.480358 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:17:53.493515 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:17:53.500445 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:17:53.516895 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:17:53.519490 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:17:53.520977 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:53.907533 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e073-18b3-88be-2b5679e0fc32) - Created
[0m22:17:54.522826 [debug] [Thread-4 (]: SQL status: OK in 1.000 seconds
[0m22:17:54.528031 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e073-18b3-88be-2b5679e0fc32, command-id=01f0ecdf-e08c-10a9-9089-523cb254bcba) - Closing
[0m22:17:54.530674 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m22:17:54.532527 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e073-18b3-88be-2b5679e0fc32) - Closing
[0m22:17:54.660250 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.20s]
[0m22:17:54.664209 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:17:54.666156 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:17:54.667827 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m22:17:54.670797 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m22:17:54.672935 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m22:17:54.674967 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:17:54.694188 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:17:54.714369 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:17:54.722472 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:17:54.741516 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:17:54.746905 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:17:54.750900 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:55.280291 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e144-1c58-a4d7-f77c1e5ce2d3) - Created
[0m22:17:55.874460 [debug] [Thread-4 (]: SQL status: OK in 1.120 seconds
[0m22:17:55.882322 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e144-1c58-a4d7-f77c1e5ce2d3, command-id=01f0ecdf-e157-1944-a87b-30c92630b5ff) - Closing
[0m22:17:55.885300 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m22:17:55.887391 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e144-1c58-a4d7-f77c1e5ce2d3) - Closing
[0m22:17:56.036613 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.36s]
[0m22:17:56.055744 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:17:56.066843 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:17:56.069845 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m22:17:56.073760 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m22:17:56.076274 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m22:17:56.079192 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:17:56.081765 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:17:56.157951 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:56.159795 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:17:56.163179 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:56.592259 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c) - Created
[0m22:17:56.907223 [debug] [Thread-4 (]: SQL status: OK in 0.740 seconds
[0m22:17:56.912736 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c, command-id=01f0ecdf-e21e-1a49-8f2a-9ed7193548c5) - Closing
[0m22:17:56.936478 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:56.966036 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:57.034728 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:57.036700 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:17:57.418965 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m22:17:57.440155 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c, command-id=01f0ecdf-e265-1ef7-a876-db24a1e7de34) - Closing
[0m22:17:57.454744 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:57.456692 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m22:17:57.710597 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:17:57.723375 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c, command-id=01f0ecdf-e2a2-1d24-a9ed-be11df96a3c5) - Closing
[0m22:17:57.750867 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:57.768735 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:57.770062 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:17:58.134287 [debug] [Thread-4 (]: SQL status: OK in 0.360 seconds
[0m22:17:58.138893 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c, command-id=01f0ecdf-e2d2-14ff-bd37-4f01effeb3ea) - Closing
[0m22:17:58.149789 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:17:58.166070 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:17:58.168880 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:17:58.420546 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:17:58.432589 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c, command-id=01f0ecdf-e30e-1f3d-9bfc-265f38a23139) - Closing
[0m22:17:58.459287 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m22:17:58.463640 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e20f-16fc-a661-80f1f8b8547c) - Closing
[0m22:17:58.608008 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.53s]
[0m22:17:58.618758 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:17:58.626183 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:17:58.636751 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m22:17:58.645020 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m22:17:58.649348 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m22:17:58.652574 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:17:58.656447 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:17:58.679816 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:17:58.681641 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:17:58.683250 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:17:59.088952 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0) - Created
[0m22:17:59.458776 [debug] [Thread-4 (]: SQL status: OK in 0.780 seconds
[0m22:17:59.465710 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0, command-id=01f0ecdf-e39b-1a0f-9d51-56d6566a00d1) - Closing
[0m22:17:59.474647 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:17:59.521694 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:17:59.546869 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:17:59.548870 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:17:59.965499 [debug] [Thread-4 (]: SQL status: OK in 0.410 seconds
[0m22:17:59.970766 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0, command-id=01f0ecdf-e3e1-1faa-a80a-2618ac63d31d) - Closing
[0m22:17:59.979499 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:17:59.981785 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m22:18:00.229529 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:18:00.232672 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0, command-id=01f0ecdf-e424-1034-9158-7a0fd9482870) - Closing
[0m22:18:00.236032 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:18:00.254346 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:18:00.255845 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:18:00.624892 [debug] [Thread-4 (]: SQL status: OK in 0.370 seconds
[0m22:18:00.631870 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0, command-id=01f0ecdf-e44d-1cc5-b0d0-1b0d58ed6982) - Closing
[0m22:18:00.638532 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:18:00.640548 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:18:00.642242 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:18:00.873634 [debug] [Thread-4 (]: SQL status: OK in 0.230 seconds
[0m22:18:00.876344 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0, command-id=01f0ecdf-e489-1451-8b27-12f4574c10ad) - Closing
[0m22:18:00.880808 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m22:18:00.882778 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e38b-1dbe-a9d1-dd81f98ad1f0) - Closing
[0m22:18:01.019003 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.37s]
[0m22:18:01.024299 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:18:01.027838 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:18:01.031261 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m22:18:01.034789 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m22:18:01.036911 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m22:18:01.038839 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:18:01.040642 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:18:01.057669 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:01.060651 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:18:01.062686 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:18:01.441359 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77) - Created
[0m22:18:01.755851 [debug] [Thread-4 (]: SQL status: OK in 0.690 seconds
[0m22:18:01.765054 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e503-19a0-96d3-94e1cdc3d2c1) - Closing
[0m22:18:01.773232 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:01.801638 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:01.828882 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:01.830611 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:18:02.098861 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m22:18:02.103818 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e53d-1f77-a1c1-30890546772d) - Closing
[0m22:18:02.107242 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:02.109200 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:18:02.425803 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m22:18:02.432492 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e569-1003-a34d-1821c1f614ae) - Closing
[0m22:18:02.439907 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:02.442369 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m22:18:02.651818 [debug] [Thread-4 (]: SQL status: OK in 0.210 seconds
[0m22:18:02.654831 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e59c-1b30-94ec-ce71670a2bbc) - Closing
[0m22:18:02.656907 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:02.671176 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:02.672611 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:18:03.009206 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m22:18:03.016325 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e5c5-113c-9c76-b75815c54902) - Closing
[0m22:18:03.022855 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m22:18:03.025159 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:18:03.027371 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m22:18:03.285033 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m22:18:03.289756 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77, command-id=01f0ecdf-e5f4-18c8-a4f6-d5f8e287371b) - Closing
[0m22:18:03.299362 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m22:18:03.301679 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecdf-e4f1-1a81-8b62-0b5ea6b84d77) - Closing
[0m22:18:03.430697 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.40s]
[0m22:18:03.433104 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:18:03.435740 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:18:03.437177 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:18:03.438875 [info ] [MainThread]: 
[0m22:18:03.440577 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 21.85 seconds (21.85s).
[0m22:18:03.445851 [debug] [MainThread]: Command end result
[0m22:18:03.664783 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:18:03.672145 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:18:03.687946 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:18:03.689084 [info ] [MainThread]: 
[0m22:18:03.690826 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:18:03.692551 [info ] [MainThread]: 
[0m22:18:03.695184 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m22:18:03.697421 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m22:18:03.699207 [info ] [MainThread]: 
[0m22:18:03.701135 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m22:18:03.702991 [info ] [MainThread]: 
[0m22:18:03.704687 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m22:18:03.707082 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 25.567883, "process_in_blocks": "0", "process_kernel_time": 0.681207, "process_mem_max_rss": "255844", "process_out_blocks": "0", "process_user_time": 7.962111}
[0m22:18:03.709009 [debug] [MainThread]: Command `dbt test` failed at 22:18:03.708757 after 25.57 seconds
[0m22:18:03.710882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81432de230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81428cae00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d81428ca710>]}
[0m22:18:03.713022 [debug] [MainThread]: Flushing usage events
[0m22:18:04.157886 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:18:09.463392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45daf5e2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45d9d402b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45d9d40250>]}


============================== 22:18:09.467071 | dcbd266a-6c9c-42ae-882b-ca3371284b91 ==============================
[0m22:18:09.467071 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:18:09.468802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'use_colors': 'True', 'log_format': 'default', 'partial_parse': 'True', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'static_parser': 'True', 'quiet': 'False', 'write_json': 'True', 'introspect': 'True', 'version_check': 'True', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --args {"model_name": "account_interest_summary"} --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'target_path': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'use_experimental_parser': 'False'}
[0m22:18:10.413668 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:18:10.415464 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:18:10.417013 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:18:11.555525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dcbd266a-6c9c-42ae-882b-ca3371284b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45d9f1bac0>]}
[0m22:18:11.683257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dcbd266a-6c9c-42ae-882b-ca3371284b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da099f30>]}
[0m22:18:11.685374 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:18:11.881535 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:18:12.329286 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:18:12.331068 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:18:12.344500 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:18:12.439718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dcbd266a-6c9c-42ae-882b-ca3371284b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da423ee0>]}
[0m22:18:12.691794 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:18:12.701377 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:18:12.719635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dcbd266a-6c9c-42ae-882b-ca3371284b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da73e650>]}
[0m22:18:12.721235 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:18:12.722615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcbd266a-6c9c-42ae-882b-ca3371284b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da40bac0>]}
[0m22:18:12.724266 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m22:18:12.725634 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m22:18:12.735868 [info ] [MainThread]: Exporting model: account_interest_summary
[0m22:18:12.745486 [debug] [MainThread]: Using databricks connection "macro_export_to_dbfs_csv"
[0m22:18:12.747269 [debug] [MainThread]: On macro_export_to_dbfs_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
[0m22:18:12.748617 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:18:13.180332 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecdf-ebf1-1bd9-9252-c305c6607dcf) - Created
[0m22:18:13.461677 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:791)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:782)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:570)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0ecdf-ec01-189b-a60f-ed74f64b5809
[0m22:18:13.471318 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  
[0m22:18:13.474789 [debug] [MainThread]: On macro_export_to_dbfs_csv: Close
[0m22:18:13.477807 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ecdf-ebf1-1bd9-9252-c305c6607dcf) - Closing
[0m22:18:13.632663 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    
[0m22:18:13.646073 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 944, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 66, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    

[0m22:18:13.668788 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m22:18:13.671025 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.338392, "process_in_blocks": "0", "process_kernel_time": 0.517719, "process_mem_max_rss": "237572", "process_out_blocks": "0", "process_user_time": 5.530365}
[0m22:18:13.672346 [debug] [MainThread]: Command `dbt run-operation` failed at 22:18:13.672225 after 4.34 seconds
[0m22:18:13.673850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45daf5e2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da73e650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45da409a80>]}
[0m22:18:13.675130 [debug] [MainThread]: Flushing usage events
[0m22:18:14.061716 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:24:25.171054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dd00e4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dbe602b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dbe60250>]}


============================== 22:24:25.184990 | 21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3 ==============================
[0m22:24:25.184990 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:24:25.187068 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --args {"model_name": "account_interest_summary"}', 'partial_parse': 'True', 'target_path': 'None', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'introspect': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'fail_fast': 'False', 'version_check': 'True', 'debug': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'write_json': 'True', 'quiet': 'False', 'static_parser': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs'}
[0m22:24:26.169609 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:24:26.171550 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:24:26.173058 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:24:27.259924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4d9dfe8f0>]}
[0m22:24:27.377283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dc7006d0>]}
[0m22:24:27.379665 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:24:27.568309 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m22:24:27.862017 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m22:24:27.864272 [debug] [MainThread]: previous checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, current checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9
[0m22:24:27.866321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dc699630>]}
[0m22:24:30.308981 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:30.331960 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:30.338837 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:30.344774 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:30.616394 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:30.622467 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.069834 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.075285 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.079805 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.084119 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.088322 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.092665 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.098280 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.321561 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.364365 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:24:31.595603 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:24:31.614133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4b4ba0df0>]}
[0m22:24:31.845678 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:24:31.855901 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:24:31.875061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dc866680>]}
[0m22:24:31.876941 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:24:31.878552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21e96aa1-cd5f-46c9-8852-ebd7db6d1eb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4b4d5c280>]}
[0m22:24:31.880796 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m22:24:31.882181 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m22:24:31.890833 [info ] [MainThread]: Exporting model: account_interest_summary
[0m22:24:31.901004 [debug] [MainThread]: Using databricks connection "macro_export_to_dbfs_csv"
[0m22:24:31.902902 [debug] [MainThread]: On macro_export_to_dbfs_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
[0m22:24:31.904571 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:32.483608 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ece0-ce04-1f43-a441-d04965b6f523) - Created
[0m22:24:32.786057 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE
    
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:791)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:782)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:570)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0ece0-ce1b-1174-88bd-d775b6fc2e04
[0m22:24:32.789156 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  
[0m22:24:32.791148 [debug] [MainThread]: On macro_export_to_dbfs_csv: Close
[0m22:24:32.792754 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ece0-ce04-1f43-a441-d04965b6f523) - Closing
[0m22:24:32.923404 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    
[0m22:24:32.928253 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 944, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
        COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
------------------^^^
        FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
        FILEFORMAT = CSV
        FORMAT_OPTIONS ('header'='true')
        OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 66, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
          COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
  ------------------^^^
          FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
          FILEFORMAT = CSV
          FORMAT_OPTIONS ('header'='true')
          OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''dbfs:/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 18)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
            COPY INTO 'dbfs:/tmp/banking_pipeline/account_summary'
    ------------------^^^
            FROM (SELECT * FROM `workspace`.`marts`.`account_interest_summary`)
            FILEFORMAT = CSV
            FORMAT_OPTIONS ('header'='true')
            OVERWRITE = TRUE
    

[0m22:24:32.949396 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m22:24:32.951617 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 8.023867, "process_in_blocks": "0", "process_kernel_time": 0.68838, "process_mem_max_rss": "248868", "process_out_blocks": "736", "process_user_time": 8.76885}
[0m22:24:32.953229 [debug] [MainThread]: Command `dbt run-operation` failed at 22:24:32.953075 after 8.03 seconds
[0m22:24:32.954994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dd00e4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4d9dfe8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79f4dd31f340>]}
[0m22:24:32.956342 [debug] [MainThread]: Flushing usage events
[0m22:24:33.379161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:31:18.679190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0eb1e4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0d9702b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0d970250>]}


============================== 22:31:18.686713 | 96f12766-0d7f-42fb-88bd-38b52ccb07c7 ==============================
[0m22:31:18.686713 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:31:18.688545 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'use_experimental_parser': 'False', 'static_parser': 'True', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run-operation export_to_dbfs_csv --args {"model_name": "account_interest_summary"}', 'printer_width': '80', 'fail_fast': 'False', 'write_json': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'quiet': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'log_path': '/opt/dagster/app/dbt/logs', 'version_check': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'use_colors': 'True', 'debug': 'False', 'log_format': 'default'}
[0m22:31:19.557035 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:31:19.559067 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:31:19.560721 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:31:20.613914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '96f12766-0d7f-42fb-88bd-38b52ccb07c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0b8ba8f0>]}
[0m22:31:20.723636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '96f12766-0d7f-42fb-88bd-38b52ccb07c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0e2106d0>]}
[0m22:31:20.725751 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:31:20.894754 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m22:31:21.309015 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:31:21.311079 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://macros/export_to_dbfs_csv.sql
[0m22:31:21.508924 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:31:21.539045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96f12766-0d7f-42fb-88bd-38b52ccb07c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de7a320b0>]}
[0m22:31:21.757826 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:31:21.770289 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:31:21.792789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96f12766-0d7f-42fb-88bd-38b52ccb07c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de7a02a10>]}
[0m22:31:21.795049 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:31:21.797606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96f12766-0d7f-42fb-88bd-38b52ccb07c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de7a02800>]}
[0m22:31:21.800411 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_export_to_dbfs_csv) - Creating connection
[0m22:31:21.803865 [debug] [MainThread]: Acquiring new databricks connection 'macro_export_to_dbfs_csv'
[0m22:31:21.821203 [info ] [MainThread]: Exporting account_interest_summary to DBFS
[0m22:31:21.840083 [debug] [MainThread]: Using databricks connection "macro_export_to_dbfs_csv"
[0m22:31:21.842429 [debug] [MainThread]: On macro_export_to_dbfs_csv: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE

  
[0m22:31:21.850725 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:22.430478 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ece1-c25d-155c-b1e2-a8cca287ab5c) - Created
[0m22:31:22.725996 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE

  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
----------^^^
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
----------^^^
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
----------^^^
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:791)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:782)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:570)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0ece1-c273-1c20-b254-c43b7c675664
[0m22:31:22.728525 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
macro export_to_dbfs_csv
: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
  COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
  ----------^^^
  FROM (
      SELECT * FROM `workspace`.`marts`.`account_interest_summary`
  )
  FILEFORMAT = CSV
  FORMAT_OPTIONS ('header'='true')
  OVERWRITE = TRUE
  
[0m22:31:22.730072 [debug] [MainThread]: On macro_export_to_dbfs_csv: Close
[0m22:31:22.731396 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0ece1-c25d-155c-b1e2-a8cca287ab5c) - Closing
[0m22:31:22.893987 [error] [MainThread]: Encountered an error while running operation: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
    COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
    ----------^^^
    FROM (
        SELECT * FROM `workspace`.`marts`.`account_interest_summary`
    )
    FILEFORMAT = CSV
    FORMAT_OPTIONS ('header'='true')
    OVERWRITE = TRUE
    
[0m22:31:22.899196 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 341, in add_query
    cursor = handle.execute(sql, bindings)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 157, in execute
    return self._safe_execute(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 251, in _safe_execute
    self._cursor = CursorWrapper(f(self._conn.cursor()))
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/handle.py", line 158, in <lambda>
    lambda cursor: cursor.execute(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/telemetry/latency_logger.py", line 175, in wrapper
    result = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/client.py", line 954, in execute
    self.active_result_set = self.backend.execute_command(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1058, in execute_command
    execute_response, has_more_rows = self._handle_execute_response(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 1265, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 944, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.10/site-packages/databricks/sql/backend/thrift_backend.py", line 635, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */

    
COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
----------^^^
FROM (
    SELECT * FROM `workspace`.`marts`.`account_interest_summary`
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header'='true')
OVERWRITE = TRUE


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 232, in exception_handler
    yield
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1310, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 427, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/clients/jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/usr/local/lib/python3.10/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/impl.py", line 401, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt_common/record.py", line 538, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 461, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 367, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 323, in add_query
    with self.exception_handler(sql):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 236, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
  
      
  COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
  ----------^^^
  FROM (
      SELECT * FROM `workspace`.`marts`.`account_interest_summary`
  )
  FILEFORMAT = CSV
  FORMAT_OPTIONS ('header'='true')
  OVERWRITE = TRUE
  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.10/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1309, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/local/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 248, in exception_handler
    raise DbtDatabaseError(str(exc)) from exc
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Database Error
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near ''/dbfs/tmp/banking_pipeline/account_summary''. SQLSTATE: 42601 (line 4, pos 10)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "macro_export_to_dbfs_csv"} */
    
        
    COPY INTO '/dbfs/tmp/banking_pipeline/account_summary'
    ----------^^^
    FROM (
        SELECT * FROM `workspace`.`marts`.`account_interest_summary`
    )
    FILEFORMAT = CSV
    FORMAT_OPTIONS ('header'='true')
    OVERWRITE = TRUE
    

[0m22:31:22.920782 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /opt/dagster/app/dbt/target/run_results.json
[0m22:31:22.923133 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.3578267, "process_in_blocks": "0", "process_kernel_time": 0.442495, "process_mem_max_rss": "237700", "process_out_blocks": "736", "process_user_time": 5.228758}
[0m22:31:22.925266 [debug] [MainThread]: Command `dbt run-operation` failed at 22:31:22.924967 after 4.36 seconds
[0m22:31:22.927359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0eb1e4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8dec1b02e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8e0ee36e60>]}
[0m22:31:22.928952 [debug] [MainThread]: Flushing usage events
[0m22:31:23.352513 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:54:47.827059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714640c6a290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71463fc44310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71463fc442b0>]}


============================== 22:54:47.850481 | 24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c ==============================
[0m22:54:47.850481 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:54:47.854705 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'fail_fast': 'False', 'debug': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'printer_width': '80', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'warn_error': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'profiles_dir': '/opt/dagster/app/dbt'}
[0m22:54:47.891505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757c962350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757b78c280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757b78c220>]}


============================== 22:54:47.916151 | 762101fd-f1e2-4f69-b40d-562c34d40c9b ==============================
[0m22:54:47.916151 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:54:47.922344 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'indirect_selection': 'eager', 'static_parser': 'True', 'empty': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'fail_fast': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'debug': 'False', 'partial_parse': 'True', 'printer_width': '80', 'log_path': '/opt/dagster/app/dbt/logs', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'quiet': 'False'}
[0m22:54:48.081987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5c0ffe410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5bfe402b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5bfe40250>]}


============================== 22:54:48.110861 | deb39faa-5742-4b3c-8a5b-1dfbe53a498d ==============================
[0m22:54:48.110861 [info ] [MainThread]: Running with dbt=1.10.18
[0m22:54:48.115752 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/opt/dagster/app/dbt', 'log_format': 'default', 'use_colors': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'static_parser': 'True', 'target_path': 'None', 'fail_fast': 'False', 'empty': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'no_print': 'None', 'version_check': 'True', 'printer_width': '80', 'warn_error': 'None', 'write_json': 'True', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'quiet': 'False', 'partial_parse': 'True'}
[0m22:54:52.373162 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:54:52.376281 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:54:52.380895 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:54:52.427013 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:54:52.430944 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:54:52.433741 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:54:52.498599 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:54:52.500887 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:54:52.502751 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:54:55.737844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757e4691e0>]}
[0m22:54:55.738367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5c0023790>]}
[0m22:54:55.782355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71463fe6dcf0>]}
[0m22:54:55.986096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5bff8ac50>]}
[0m22:54:55.991626 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:54:56.000971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7556504190>]}
[0m22:54:56.003017 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:54:56.035069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71463fc161d0>]}
[0m22:54:56.037902 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m22:54:56.322326 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:54:56.373720 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:54:56.419961 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m22:54:56.772328 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m22:54:56.774135 [debug] [MainThread]: previous checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, current checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91
[0m22:54:56.775633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757c18f340>]}
[0m22:54:56.822405 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m22:54:56.824224 [debug] [MainThread]: previous checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, current checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91
[0m22:54:56.825936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714640645e40>]}
[0m22:54:56.827034 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m22:54:56.831788 [debug] [MainThread]: previous checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, current checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91
[0m22:54:56.833700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5c067a0e0>]}
[0m22:55:00.982569 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.009876 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.019894 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.025428 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.026897 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.055954 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.063780 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.070845 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.072824 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.101297 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.107514 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.116495 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.424711 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.431681 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.509943 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.520192 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.527585 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:01.535452 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.233499 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.233768 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.238238 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.238532 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.242932 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.243079 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.250577 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.250627 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.255323 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.255328 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.260488 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.260392 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.266738 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.266717 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.403101 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.407880 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.414511 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.421648 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.426555 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.434806 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.440903 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.555700 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.620446 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.623502 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.630804 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.688343 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:02.688627 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m22:55:03.024068 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:55:03.030370 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:55:03.047568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d75544d3e80>]}
[0m22:55:03.054566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598b7ce80>]}
[0m22:55:03.068248 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m22:55:03.097268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714618990e80>]}
[0m22:55:03.418617 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:03.427018 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:03.433020 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:03.441768 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:03.449225 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:03.459527 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:03.478845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598a2f550>]}
[0m22:55:03.482829 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:55:03.483275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d755421e6b0>]}
[0m22:55:03.484954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598a2f310>]}
[0m22:55:03.485312 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:55:03.486334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7146189a9a80>]}
[0m22:55:03.486930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d755421f7f0>]}
[0m22:55:03.488171 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m22:55:03.489095 [info ] [MainThread]: 
[0m22:55:03.489844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714618991bd0>]}
[0m22:55:03.490690 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:55:03.491432 [info ] [MainThread]: 
[0m22:55:03.492282 [info ] [MainThread]: 
[0m22:55:03.492972 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:55:03.493724 [info ] [MainThread]: 
[0m22:55:03.494065 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:03.494297 [info ] [MainThread]: 
[0m22:55:03.495458 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:55:03.495604 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:03.497806 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:03.498546 [info ] [MainThread]: 
[0m22:55:03.499784 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:03.500699 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:03.502123 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:03.511336 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:55:03.514507 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:55:03.517360 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:55:03.518986 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:55:03.519308 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:55:03.521080 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:55:03.540086 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:55:03.542184 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:55:03.543888 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:03.545574 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:55:03.549380 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:55:03.551321 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:03.552810 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:55:03.554466 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:55:03.555875 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:04.090100 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bd-1ba3-9041-1a5a5a827a78) - Created
[0m22:55:04.093279 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bc-1b21-91ba-7130268b9691) - Created
[0m22:55:04.093944 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bc-1887-971e-cd90b743ebd0) - Created
[0m22:55:07.905060 [debug] [ThreadPool]: SQL status: OK in 4.350 seconds
[0m22:55:07.906018 [debug] [ThreadPool]: SQL status: OK in 4.360 seconds
[0m22:55:07.917228 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-11bd-1ba3-9041-1a5a5a827a78, command-id=01f0ece5-11d2-11e8-8f30-95546334f25c) - Closing
[0m22:55:07.917488 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-11bc-1b21-91ba-7130268b9691, command-id=01f0ece5-11d4-1bad-ad9d-db9d807d971e) - Closing
[0m22:55:07.918875 [debug] [ThreadPool]: On list_workspace: Close
[0m22:55:07.919322 [debug] [ThreadPool]: On list_workspace: Close
[0m22:55:07.920162 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bd-1ba3-9041-1a5a5a827a78) - Closing
[0m22:55:07.920449 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bc-1b21-91ba-7130268b9691) - Closing
[0m22:55:08.041161 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:55:08.043628 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:55:08.053724 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:55:08.055245 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:55:08.056731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:08.059050 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:55:08.060526 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:55:08.081326 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:55:08.083715 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:55:08.085701 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:08.465180 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-145c-16a6-82aa-0b6ac9c2863f) - Created
[0m22:55:08.483472 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-145f-1be7-bed8-de5a78b63e3a) - Created
[0m22:55:09.632779 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m22:55:09.635923 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-145c-16a6-82aa-0b6ac9c2863f, command-id=01f0ece5-146d-1365-89a5-494a6e36d1d6) - Closing
[0m22:55:09.637447 [debug] [ThreadPool]: On list_workspace: Close
[0m22:55:09.638741 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-145c-16a6-82aa-0b6ac9c2863f) - Closing
[0m22:55:09.768804 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m22:55:09.770436 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:55:09.776867 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:55:09.779889 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m22:55:09.781946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:09.899936 [debug] [ThreadPool]: SQL status: OK in 6.340 seconds
[0m22:55:09.907199 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-11bc-1887-971e-cd90b743ebd0, command-id=01f0ece5-11d3-1186-b304-787297c072c9) - Closing
[0m22:55:09.909276 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:55:09.910897 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-11bc-1887-971e-cd90b743ebd0) - Closing
[0m22:55:10.044818 [debug] [ThreadPool]: SQL status: OK in 1.960 seconds
[0m22:55:10.051837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-145f-1be7-bed8-de5a78b63e3a, command-id=01f0ece5-1470-1021-b87f-057c98becb18) - Closing
[0m22:55:10.051208 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:55:10.054564 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:55:10.054437 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:55:10.056638 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-145f-1be7-bed8-de5a78b63e3a) - Closing
[0m22:55:10.060627 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:55:10.063471 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:55:10.065243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:10.158414 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-155f-16fe-acdc-187c4f52a134) - Created
[0m22:55:10.186133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:55:10.187782 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:55:10.196600 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:55:10.198556 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:55:10.200701 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:10.481143 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-158c-10d0-8e14-6c117e772010) - Created
[0m22:55:10.580736 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-159f-1374-9830-4c8273f0d9b2) - Created
[0m22:55:10.738851 [debug] [ThreadPool]: SQL status: OK in 0.960 seconds
[0m22:55:10.742560 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-155f-16fe-acdc-187c4f52a134, command-id=01f0ece5-156f-1054-b7fc-9c5bd239f8a6) - Closing
[0m22:55:10.744935 [debug] [ThreadPool]: On list_workspace: Close
[0m22:55:10.748306 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-155f-16fe-acdc-187c4f52a134) - Closing
[0m22:55:10.876305 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:55:10.877928 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:55:10.892137 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:55:10.893605 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:55:10.895122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:11.285107 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-160b-1988-b5ab-6b040dba739f) - Created
[0m22:55:11.431619 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m22:55:11.435908 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-158c-10d0-8e14-6c117e772010, command-id=01f0ece5-15a0-1a41-aaf4-67de38ccb902) - Closing
[0m22:55:11.437952 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:55:11.439675 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-158c-10d0-8e14-6c117e772010) - Closing
[0m22:55:11.474200 [debug] [ThreadPool]: SQL status: OK in 1.270 seconds
[0m22:55:11.477664 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-159f-1374-9830-4c8273f0d9b2, command-id=01f0ece5-15af-17df-bd5a-6668bce7263b) - Closing
[0m22:55:11.481098 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:55:11.482736 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-159f-1374-9830-4c8273f0d9b2) - Closing
[0m22:55:11.572074 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:55:11.574059 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:55:11.579582 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:55:11.581665 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:55:11.583277 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:11.627557 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m22:55:11.630721 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m22:55:11.635356 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m22:55:11.636874 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m22:55:11.638182 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:11.959418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1672-145b-b515-fb343bb31d96) - Created
[0m22:55:12.014058 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1679-15ec-8815-2ecf3805f0bc) - Created
[0m22:55:12.791457 [debug] [ThreadPool]: SQL status: OK in 1.900 seconds
[0m22:55:12.798465 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-160b-1988-b5ab-6b040dba739f, command-id=01f0ece5-161b-1986-a5ce-a9c09998714c) - Closing
[0m22:55:12.801768 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:55:12.803799 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-160b-1988-b5ab-6b040dba739f) - Closing
[0m22:55:12.952197 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m22:55:12.955440 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m22:55:12.970168 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m22:55:12.972442 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m22:55:12.974275 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:13.269352 [debug] [ThreadPool]: SQL status: OK in 1.690 seconds
[0m22:55:13.274577 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-1672-145b-b515-fb343bb31d96, command-id=01f0ece5-1682-1c94-93a0-c56c63651cca) - Closing
[0m22:55:13.277201 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:55:13.279544 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1672-145b-b515-fb343bb31d96) - Closing
[0m22:55:13.354138 [debug] [ThreadPool]: SQL status: OK in 1.720 seconds
[0m22:55:13.359941 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-1679-15ec-8815-2ecf3805f0bc, command-id=01f0ece5-168a-1171-b8d7-a8db6eb5d437) - Closing
[0m22:55:13.360468 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1747-17ee-9a3a-d3860771df6d) - Created
[0m22:55:13.363113 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m22:55:13.365477 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1679-15ec-8815-2ecf3805f0bc) - Closing
[0m22:55:13.406782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '762101fd-f1e2-4f69-b40d-562c34d40c9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757973a530>]}
[0m22:55:13.417495 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:55:13.423098 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m22:55:13.425365 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m22:55:13.427140 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m22:55:13.429787 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:55:13.481113 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:55:13.501346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7146419ed2a0>]}
[0m22:55:13.502215 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:55:13.507565 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m22:55:13.509628 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m22:55:13.514791 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m22:55:13.516909 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m22:55:13.519077 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m22:55:13.521717 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m22:55:13.541116 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:55:13.543354 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714618755030>]}
[0m22:55:13.551459 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:55:13.574773 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m22:55:13.577018 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m22:55:13.579739 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:13.670577 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:55:13.672343 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71461878dcf0>]}
[0m22:55:13.715779 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:55:13.717727 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:55:13.719218 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:55:14.004301 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-17a7-17cc-947a-591f922c3316) - Created
[0m22:55:14.132921 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece5-17bd-13cd-a403-f7cc0feab319) - Created
[0m22:55:14.206795 [debug] [ThreadPool]: SQL status: OK in 1.230 seconds
[0m22:55:14.212214 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-1747-17ee-9a3a-d3860771df6d, command-id=01f0ece5-1757-1c07-8007-78db0683c112) - Closing
[0m22:55:14.214472 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m22:55:14.216030 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-1747-17ee-9a3a-d3860771df6d) - Closing
[0m22:55:14.344833 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m22:55:14.347597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m22:55:14.355395 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m22:55:14.357419 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m22:55:14.359080 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:14.749958 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-181a-1bb1-a981-e5ae9749255b) - Created
[0m22:55:16.173746 [debug] [Thread-4 (]: SQL status: OK in 2.590 seconds
[0m22:55:16.184460 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-17a7-17cc-947a-591f922c3316, command-id=01f0ece5-17ba-17d0-8cc4-a91a16e0ac7f) - Closing
[0m22:55:16.191870 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m22:55:16.193789 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-17a7-17cc-947a-591f922c3316) - Closing
[0m22:55:16.330854 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 2.90s]
[0m22:55:16.334278 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m22:55:16.337038 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:55:16.339661 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m22:55:16.342810 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m22:55:16.347616 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m22:55:16.349950 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:55:16.358038 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:55:16.372174 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:55:16.377000 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:55:16.389600 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m22:55:16.391005 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m22:55:16.391973 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:16.658498 [debug] [ThreadPool]: SQL status: OK in 2.300 seconds
[0m22:55:16.663553 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece5-181a-1bb1-a981-e5ae9749255b, command-id=01f0ece5-182c-1b3c-a36c-bb055a22bf0d) - Closing
[0m22:55:16.665720 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m22:55:16.667654 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece5-181a-1bb1-a981-e5ae9749255b) - Closing
[0m22:55:16.766845 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-194e-1d05-bd8b-9c11714427a6) - Created
[0m22:55:16.814313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598d31120>]}
[0m22:55:16.818979 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m22:55:16.820827 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m22:55:16.822917 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m22:55:16.824542 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m22:55:16.826128 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m22:55:16.837471 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m22:55:16.850852 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m22:55:16.876042 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:55:16.882083 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:55:16.884321 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa59814e980>]}
[0m22:55:16.905854 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m22:55:16.918707 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m22:55:16.933728 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m22:55:16.935131 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m22:55:16.936274 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:55:17.336400 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-19a6-150c-a365-d2ce6c1cba05) - Created
[0m22:55:17.657324 [debug] [Thread-4 (]: SQL status: OK in 1.270 seconds
[0m22:55:17.663445 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-194e-1d05-bd8b-9c11714427a6, command-id=01f0ece5-195f-1eba-915a-21970d2b5d1d) - Closing
[0m22:55:17.666328 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m22:55:17.668144 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-194e-1d05-bd8b-9c11714427a6) - Closing
[0m22:55:17.796715 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.45s]
[0m22:55:17.798956 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m22:55:17.800700 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:55:17.802305 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m22:55:17.804214 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m22:55:17.805685 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m22:55:17.807147 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:55:17.820102 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:55:17.833983 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:55:17.838568 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:55:17.852507 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m22:55:17.854144 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m22:55:17.855685 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:18.231678 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1a2e-1c67-8e59-29c54528f628) - Created
[0m22:55:18.975876 [debug] [Thread-5 (]: SQL status: OK in 5.260 seconds
[0m22:55:18.984031 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece5-17bd-13cd-a403-f7cc0feab319, command-id=01f0ece5-17cd-1d58-8769-b7a7b07011fe) - Closing
[0m22:55:19.023228 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m22:55:19.025167 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m22:55:19.134889 [debug] [Thread-7 (]: SQL status: OK in 2.200 seconds
[0m22:55:19.137193 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece5-19a6-150c-a365-d2ce6c1cba05, command-id=01f0ece5-19b7-1821-9050-a4419f1295e2) - Closing
[0m22:55:19.155720 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:55:19.160349 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m22:55:19.162747 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-19a6-150c-a365-d2ce6c1cba05) - Closing
[0m22:55:19.300380 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598ad3f70>]}
[0m22:55:19.302407 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.48s]
[0m22:55:19.304198 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m22:55:19.305593 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m22:55:19.307148 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m22:55:19.308854 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m22:55:19.310284 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m22:55:19.311769 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m22:55:19.317814 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m22:55:19.332512 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m22:55:19.340210 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:55:19.342530 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m22:55:19.344435 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m22:55:19.357788 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m22:55:19.359195 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m22:55:19.360556 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:55:19.726172 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1b12-1f38-ac8f-056c85c13445) - Created
[0m22:55:21.623700 [debug] [Thread-4 (]: SQL status: OK in 3.770 seconds
[0m22:55:21.631057 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-1a2e-1c67-8e59-29c54528f628, command-id=01f0ece5-1a3f-1ce1-a16f-cb19a5802d9a) - Closing
[0m22:55:21.634615 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m22:55:21.637466 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1a2e-1c67-8e59-29c54528f628) - Closing
[0m22:55:21.779359 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 3.97s]
[0m22:55:21.783908 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m22:55:21.786507 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:55:21.789032 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m22:55:21.791868 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m22:55:21.794338 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m22:55:21.797336 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:55:21.809820 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:55:21.822015 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:55:21.826076 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:55:21.839742 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m22:55:21.840813 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m22:55:21.841782 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:22.243167 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1c92-13c1-bf65-2d309eeda45c) - Created
[0m22:55:22.280686 [debug] [Thread-7 (]: SQL status: OK in 2.920 seconds
[0m22:55:22.284380 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece5-1b12-1f38-ac8f-056c85c13445, command-id=01f0ece5-1b23-12f7-a25d-64ba07bacaf7) - Closing
[0m22:55:22.287590 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:55:22.290937 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m22:55:22.293366 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1b12-1f38-ac8f-056c85c13445) - Closing
[0m22:55:22.423296 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5c1301240>]}
[0m22:55:22.425945 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 3.11s]
[0m22:55:22.428773 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m22:55:22.432766 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m22:55:22.435250 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m22:55:22.438154 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m22:55:22.440847 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m22:55:22.442754 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m22:55:22.452884 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m22:55:22.469295 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m22:55:22.473545 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m22:55:22.475463 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m22:55:22.477220 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m22:55:22.491765 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m22:55:22.493058 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m22:55:22.494155 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:55:22.881017 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1cf4-1863-8c56-5c754b24cbb5) - Created
[0m22:55:23.531344 [debug] [Thread-4 (]: SQL status: OK in 1.690 seconds
[0m22:55:23.536770 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-1c92-13c1-bf65-2d309eeda45c, command-id=01f0ece5-1ca4-1ec2-bd1f-0761319a20c6) - Closing
[0m22:55:23.539210 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m22:55:23.541228 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1c92-13c1-bf65-2d309eeda45c) - Closing
[0m22:55:23.678077 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.89s]
[0m22:55:23.682512 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m22:55:23.685125 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:55:23.686841 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m22:55:23.688780 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m22:55:23.690190 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m22:55:23.691619 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:55:23.703067 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:55:23.720062 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:55:23.725789 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:55:23.742927 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:55:23.744481 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m22:55:23.746006 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:24.195609 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1dbc-1b6e-8e4a-7322873aaa2f) - Created
[0m22:55:24.821909 [debug] [Thread-7 (]: SQL status: OK in 2.330 seconds
[0m22:55:24.824687 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece5-1cf4-1863-8c56-5c754b24cbb5, command-id=01f0ece5-1d04-1a64-b4fd-2883a606ceac) - Closing
[0m22:55:24.827065 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:55:24.830092 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m22:55:24.832050 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1cf4-1863-8c56-5c754b24cbb5) - Closing
[0m22:55:24.962966 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598123d60>]}
[0m22:55:24.965192 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 2.53s]
[0m22:55:24.966893 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m22:55:24.969188 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m22:55:24.971108 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m22:55:24.972896 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m22:55:24.974286 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m22:55:24.975691 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m22:55:24.980824 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m22:55:24.992825 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m22:55:25.022342 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m22:55:25.038097 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m22:55:25.040108 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598a31c90>]}
[0m22:55:25.076705 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m22:55:25.091219 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m22:55:25.092335 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m22:55:25.093296 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m22:55:25.486608 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1e82-1406-8e56-34b1bcd7cf2c) - Created
[0m22:55:25.655056 [debug] [Thread-5 (]: SQL status: OK in 6.630 seconds
[0m22:55:25.656774 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece5-17bd-13cd-a403-f7cc0feab319, command-id=01f0ece5-1ab8-17e1-afba-4da39a972b1a) - Closing
[0m22:55:25.808098 [debug] [Thread-4 (]: SQL status: OK in 2.060 seconds
[0m22:55:25.812114 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-1dbc-1b6e-8e4a-7322873aaa2f, command-id=01f0ece5-1dcd-1b9a-a94f-718d8cc7c1b5) - Closing
[0m22:55:25.814783 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m22:55:25.816901 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1dbc-1b6e-8e4a-7322873aaa2f) - Closing
[0m22:55:25.833842 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m22:55:25.865317 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m22:55:25.867124 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece5-17bd-13cd-a403-f7cc0feab319) - Closing
[0m22:55:25.952330 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.26s]
[0m22:55:25.954204 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:55:25.955690 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:55:25.957033 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m22:55:25.958673 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m22:55:25.960427 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m22:55:25.962127 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:55:25.971209 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:55:25.987629 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:55:25.992731 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:55:26.009536 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7146187220b0>]}
[0m22:55:26.009869 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m22:55:26.012081 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m22:55:26.012233 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 12.49s]
[0m22:55:26.015573 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:26.016753 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m22:55:26.021492 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m22:55:26.023669 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m22:55:26.025772 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m22:55:26.027452 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m22:55:26.029415 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m22:55:26.031633 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m22:55:26.052690 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:55:26.055650 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m22:55:26.057444 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m22:55:26.398337 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1f0c-1b4c-b5ad-f24def047f9f) - Created
[0m22:55:26.442863 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece5-1f12-1de9-a653-75f09c17ae82) - Created
[0m22:55:27.587303 [debug] [Thread-4 (]: SQL status: OK in 1.570 seconds
[0m22:55:27.591075 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-1f0c-1b4c-b5ad-f24def047f9f, command-id=01f0ece5-1f1e-137c-870e-22d5b259bade) - Closing
[0m22:55:27.593254 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m22:55:27.594889 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-1f0c-1b4c-b5ad-f24def047f9f) - Closing
[0m22:55:27.719320 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.76s]
[0m22:55:27.721513 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m22:55:27.723359 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:55:27.725166 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m22:55:27.727369 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m22:55:27.729213 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m22:55:27.731489 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:55:27.742463 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:55:27.760404 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:55:27.767597 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:55:27.781839 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m22:55:27.783534 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m22:55:27.784930 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:28.171247 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-201b-1ecc-a2a7-1fc4a511b134) - Created
[0m22:55:29.639834 [debug] [Thread-5 (]: SQL status: OK in 3.580 seconds
[0m22:55:29.642794 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece5-1f12-1de9-a653-75f09c17ae82, command-id=01f0ece5-1f24-12d1-a753-32dee7e580ef) - Closing
[0m22:55:29.648177 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m22:55:29.650319 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m22:55:29.783282 [debug] [Thread-4 (]: SQL status: OK in 2.000 seconds
[0m22:55:29.786876 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-201b-1ecc-a2a7-1fc4a511b134, command-id=01f0ece5-202c-12cb-be1c-ab3cd6bc9439) - Closing
[0m22:55:29.789208 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m22:55:29.790810 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-201b-1ecc-a2a7-1fc4a511b134) - Closing
[0m22:55:29.913825 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 2.19s]
[0m22:55:29.916560 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m22:55:29.918947 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:55:29.920771 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m22:55:29.923032 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m22:55:29.924726 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m22:55:29.926088 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:55:29.940159 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:55:29.953601 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:55:29.958017 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:55:29.971724 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m22:55:29.973049 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:55:29.974200 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:30.342629 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-2166-1722-978f-9b1fb8a15dd9) - Created
[0m22:55:32.643363 [debug] [Thread-4 (]: SQL status: OK in 2.670 seconds
[0m22:55:32.647351 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-2166-1722-978f-9b1fb8a15dd9, command-id=01f0ece5-2177-1104-98c4-dac561b48052) - Closing
[0m22:55:32.650616 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m22:55:32.652255 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-2166-1722-978f-9b1fb8a15dd9) - Closing
[0m22:55:32.795656 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 2.87s]
[0m22:55:32.799377 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m22:55:32.801894 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:55:32.804105 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m22:55:32.806361 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m22:55:32.808190 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m22:55:32.810353 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:55:32.821476 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:55:32.835873 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:55:32.840328 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:55:32.857442 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m22:55:32.858964 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:55:32.860208 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:33.283046 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-2327-178b-a94b-a3b365605d4e) - Created
[0m22:55:33.368834 [debug] [Thread-5 (]: SQL status: OK in 3.720 seconds
[0m22:55:33.370423 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece5-1f12-1de9-a653-75f09c17ae82, command-id=01f0ece5-210c-1ec3-b9df-7f99855ad8c8) - Closing
[0m22:55:33.372366 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m22:55:33.388166 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m22:55:33.389606 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece5-1f12-1de9-a653-75f09c17ae82) - Closing
[0m22:55:33.533111 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24dcb4b8-30e6-47a0-aa2f-bd1b8c9f762c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71464112a8f0>]}
[0m22:55:33.535577 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 7.51s]
[0m22:55:33.538065 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m22:55:33.541458 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:33.543307 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:33.544932 [info ] [MainThread]: 
[0m22:55:33.546463 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 30.04 seconds (30.04s).
[0m22:55:33.549432 [debug] [MainThread]: Command end result
[0m22:55:33.667784 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:33.677307 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:33.694128 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:55:33.695567 [info ] [MainThread]: 
[0m22:55:33.697239 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:55:33.701520 [info ] [MainThread]: 
[0m22:55:33.703152 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:55:33.705111 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 46.40263, "process_in_blocks": "0", "process_kernel_time": 1.601833, "process_mem_max_rss": "258424", "process_out_blocks": "184", "process_user_time": 19.76731}
[0m22:55:33.706490 [debug] [MainThread]: Command `dbt seed` succeeded at 22:55:33.706368 after 46.40 seconds
[0m22:55:33.707930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714640c6a290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714618937910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71463fe6cd60>]}
[0m22:55:33.709279 [debug] [MainThread]: Flushing usage events
[0m22:55:33.826260 [debug] [Thread-7 (]: SQL status: OK in 8.730 seconds
[0m22:55:33.830393 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece5-1e82-1406-8e56-34b1bcd7cf2c, command-id=01f0ece5-1e92-148e-bc77-c7f89d53db9a) - Closing
[0m22:55:34.010934 [debug] [Thread-7 (]: Applying tags to relation None
[0m22:55:34.094554 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m22:55:34.097347 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece5-1e82-1406-8e56-34b1bcd7cf2c) - Closing
[0m22:55:34.169956 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:55:34.216867 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'deb39faa-5742-4b3c-8a5b-1dfbe53a498d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa598b97d60>]}
[0m22:55:34.219685 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 9.24s]
[0m22:55:34.221817 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m22:55:34.225936 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:34.227711 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:34.230340 [info ] [MainThread]: 
[0m22:55:34.232377 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 30.74 seconds (30.74s).
[0m22:55:34.235279 [debug] [MainThread]: Command end result
[0m22:55:34.339732 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:34.348738 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:34.365891 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:55:34.367303 [info ] [MainThread]: 
[0m22:55:34.368756 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:55:34.370024 [info ] [MainThread]: 
[0m22:55:34.371523 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m22:55:34.373743 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 47.037186, "process_in_blocks": "0", "process_kernel_time": 1.551736, "process_mem_max_rss": "259112", "process_out_blocks": "304", "process_user_time": 20.584682}
[0m22:55:34.375473 [debug] [MainThread]: Command `dbt run` succeeded at 22:55:34.375271 after 47.04 seconds
[0m22:55:34.376892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5c0ffe410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa5bff8ac50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aa59a6f3880>]}
[0m22:55:34.378397 [debug] [MainThread]: Flushing usage events
[0m22:55:34.886430 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:55:35.620097 [debug] [Thread-4 (]: SQL status: OK in 2.760 seconds
[0m22:55:35.624827 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-2327-178b-a94b-a3b365605d4e, command-id=01f0ece5-2337-1c63-a04b-4d1c6d1c8af5) - Closing
[0m22:55:35.627358 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m22:55:35.629141 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-2327-178b-a94b-a3b365605d4e) - Closing
[0m22:55:35.760420 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.95s]
[0m22:55:35.762986 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m22:55:35.764943 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:55:35.767011 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m22:55:35.769552 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m22:55:35.771539 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m22:55:35.773171 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:55:35.774753 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:55:35.842097 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:35.844054 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:55:35.849537 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:36.235034 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8) - Created
[0m22:55:36.981214 [debug] [Thread-4 (]: SQL status: OK in 1.130 seconds
[0m22:55:36.984663 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8, command-id=01f0ece5-24fa-1524-8c9f-a2ba98424293) - Closing
[0m22:55:37.014211 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.044248 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.125643 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.128880 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:55:37.519823 [debug] [Thread-4 (]: SQL status: OK in 0.390 seconds
[0m22:55:37.523212 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8, command-id=01f0ece5-2584-1170-aa67-15f315ad668d) - Closing
[0m22:55:37.528536 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.530581 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m22:55:37.809569 [debug] [Thread-4 (]: SQL status: OK in 0.280 seconds
[0m22:55:37.817516 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8, command-id=01f0ece5-25bf-1c6c-98fb-ae0b8d48447a) - Closing
[0m22:55:37.828359 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.842918 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:37.844277 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:55:38.295765 [debug] [Thread-4 (]: SQL status: OK in 0.450 seconds
[0m22:55:38.303132 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8, command-id=01f0ece5-25f0-1451-9c74-92eec7f293ec) - Closing
[0m22:55:38.322501 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:55:38.333442 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m22:55:38.336124 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m22:55:38.615338 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m22:55:38.621789 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8, command-id=01f0ece5-263b-1bba-a07f-627698000517) - Closing
[0m22:55:38.635501 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m22:55:38.637664 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-24e9-1cf8-8044-7b45a6ab57c8) - Closing
[0m22:55:38.793387 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 3.02s]
[0m22:55:38.796680 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m22:55:38.799412 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:55:38.801642 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m22:55:38.804193 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m22:55:38.806117 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m22:55:38.807977 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:55:38.809724 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:55:38.824700 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:38.826718 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:55:38.828329 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:39.215460 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2) - Created
[0m22:55:39.760898 [debug] [Thread-4 (]: SQL status: OK in 0.930 seconds
[0m22:55:39.767638 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2, command-id=01f0ece5-26c1-17ac-9f46-437fd5910ade) - Closing
[0m22:55:39.775875 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:39.805792 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:39.830918 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:39.832813 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:55:40.122744 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m22:55:40.127137 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2, command-id=01f0ece5-2720-192e-838e-d086582528da) - Closing
[0m22:55:40.135288 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:40.138091 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m22:55:40.432092 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m22:55:40.436766 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2, command-id=01f0ece5-274e-19dc-a5b3-1245c914d6f8) - Closing
[0m22:55:40.441297 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:40.467738 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:40.470329 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:55:40.814848 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m22:55:40.820183 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2, command-id=01f0ece5-2781-15fa-9255-a57b7e80a219) - Closing
[0m22:55:40.826118 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:55:40.828461 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m22:55:40.830355 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m22:55:41.119771 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m22:55:41.122260 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2, command-id=01f0ece5-27b8-1b57-888b-9b73af77f4ad) - Closing
[0m22:55:41.125726 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m22:55:41.127465 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-26b1-1409-9af9-bf1d1998fdc2) - Closing
[0m22:55:41.261292 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.46s]
[0m22:55:41.263256 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m22:55:41.264827 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:55:41.266854 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m22:55:41.269380 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m22:55:41.270942 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m22:55:41.272412 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:55:41.273762 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:55:41.291836 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:41.293468 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m22:55:41.294865 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:55:41.686616 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634) - Created
[0m22:55:42.375691 [debug] [Thread-4 (]: SQL status: OK in 1.080 seconds
[0m22:55:42.379744 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-283a-174d-90be-5334b9d806db) - Closing
[0m22:55:42.385786 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:42.408756 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:42.432666 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:42.434674 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:55:42.694425 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m22:55:42.700493 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-28ac-1ca0-87f1-8ef13b0a5d0f) - Closing
[0m22:55:42.704279 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:42.706533 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m22:55:43.034236 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m22:55:43.036943 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-28d7-123d-9afc-a4ea7f0e227c) - Closing
[0m22:55:43.041128 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:43.042589 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m22:55:43.269128 [debug] [Thread-4 (]: SQL status: OK in 0.220 seconds
[0m22:55:43.272316 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-2908-1f94-81cc-b3dd233bc56c) - Closing
[0m22:55:43.274664 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:43.292259 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:43.293842 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m22:55:43.669291 [debug] [Thread-4 (]: SQL status: OK in 0.370 seconds
[0m22:55:43.676349 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-292f-18f7-9b4b-5bb2aee0b1d1) - Closing
[0m22:55:43.681136 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m22:55:43.682842 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m22:55:43.684309 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m22:55:43.933082 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m22:55:43.938257 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634, command-id=01f0ece5-296b-13f4-af81-78ed66006a31) - Closing
[0m22:55:43.953297 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m22:55:43.955548 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece5-282a-12f5-bc86-58a4021ee634) - Closing
[0m22:55:44.098576 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.83s]
[0m22:55:44.107405 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m22:55:44.120219 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m22:55:44.126775 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:55:44.134570 [info ] [MainThread]: 
[0m22:55:44.143579 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 40.64 seconds (40.64s).
[0m22:55:44.156940 [debug] [MainThread]: Command end result
[0m22:55:44.380287 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m22:55:44.388466 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m22:55:44.405576 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m22:55:44.407053 [info ] [MainThread]: 
[0m22:55:44.408472 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:55:44.409815 [info ] [MainThread]: 
[0m22:55:44.411335 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m22:55:44.412815 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m22:55:44.414102 [info ] [MainThread]: 
[0m22:55:44.415413 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m22:55:44.416610 [info ] [MainThread]: 
[0m22:55:44.418000 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m22:55:44.419995 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 57.098335, "process_in_blocks": "0", "process_kernel_time": 1.690167, "process_mem_max_rss": "265968", "process_out_blocks": "344", "process_user_time": 21.244818}
[0m22:55:44.421432 [debug] [MainThread]: Command `dbt test` failed at 22:55:44.421308 after 57.10 seconds
[0m22:55:44.422719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757c962350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d757cac0c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d75541d5c60>]}
[0m22:55:44.424113 [debug] [MainThread]: Flushing usage events
[0m22:55:44.795085 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:07:03.814579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5b8d2440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5a6f82b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5a6f8250>]}


============================== 23:07:03.823002 | 1c358530-5a3f-4be7-86c0-ee2855b92c0a ==============================
[0m23:07:03.823002 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:07:03.825008 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'no_print': 'None', 'static_parser': 'True', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'profiles_dir': '/opt/dagster/app/dbt', 'write_json': 'True', 'target_path': 'None', 'partial_parse': 'True', 'warn_error': 'None', 'empty': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'debug': 'False', 'use_colors': 'True', 'version_check': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'introspect': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True'}
[0m23:07:04.846011 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:07:04.848073 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:07:04.849698 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:07:06.557760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5d3faf50>]}
[0m23:07:06.682799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5a87dcc0>]}
[0m23:07:06.685097 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:07:06.881705 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:07:07.454078 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:07:07.456062 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:07:07.471673 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:07:07.581172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e387ccb20>]}
[0m23:07:07.862181 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:07:07.871217 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:07:07.900108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e3882f820>]}
[0m23:07:07.901544 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:07:07.902829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e3882f760>]}
[0m23:07:07.905559 [info ] [MainThread]: 
[0m23:07:07.906904 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:07:07.908112 [info ] [MainThread]: 
[0m23:07:07.909868 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:07:07.912217 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:07:07.922324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:07:07.924034 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:07:07.940392 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:07:07.942223 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:07:07.943898 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:08.762546 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-c1a1-1fe7-8557-3d6f1a3cd31d) - Created
[0m23:07:28.039123 [debug] [ThreadPool]: SQL status: OK in 20.100 seconds
[0m23:07:28.046671 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-c1a1-1fe7-8557-3d6f1a3cd31d, command-id=01f0ece6-c1c6-1d0b-b216-f802856e15ab) - Closing
[0m23:07:28.288162 [debug] [ThreadPool]: On list_workspace: Close
[0m23:07:28.290796 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-c1a1-1fe7-8557-3d6f1a3cd31d) - Closing
[0m23:07:28.425636 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:07:28.426887 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:07:28.440526 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:07:28.441992 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:07:28.443374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:28.888560 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-cdac-19dd-a9de-64f76e6431c3) - Created
[0m23:07:32.817567 [debug] [ThreadPool]: SQL status: OK in 4.370 seconds
[0m23:07:32.825451 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-cdac-19dd-a9de-64f76e6431c3, command-id=01f0ece6-cdc2-1902-86e6-2f734872987b) - Closing
[0m23:07:32.828343 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:07:32.830806 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-cdac-19dd-a9de-64f76e6431c3) - Closing
[0m23:07:32.986888 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:07:32.990672 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:07:33.008319 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:07:33.010433 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:07:33.012118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:33.391410 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-d060-1acd-9baf-6046a3257e48) - Created
[0m23:07:34.325824 [debug] [ThreadPool]: SQL status: OK in 1.310 seconds
[0m23:07:34.328775 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-d060-1acd-9baf-6046a3257e48, command-id=01f0ece6-d070-1b27-a43b-c29a9a81aa10) - Closing
[0m23:07:34.330533 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:07:34.331834 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-d060-1acd-9baf-6046a3257e48) - Closing
[0m23:07:34.470040 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:07:34.472819 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:07:34.478996 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:07:34.480554 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:07:34.481985 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:34.911435 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-d144-14ec-ac8f-828c46116139) - Created
[0m23:07:35.545699 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m23:07:35.556305 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-d144-14ec-ac8f-828c46116139, command-id=01f0ece6-d158-16d6-86aa-8876e1b11e86) - Closing
[0m23:07:35.561079 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:07:35.564837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-d144-14ec-ac8f-828c46116139) - Closing
[0m23:07:35.719058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5d1fb940>]}
[0m23:07:35.757939 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m23:07:35.766119 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m23:07:35.776492 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m23:07:35.783010 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m23:07:35.790473 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m23:07:35.797323 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m23:07:35.835512 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:07:35.839198 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e3874dba0>]}
[0m23:07:35.906780 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:07:35.908798 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e38768730>]}
[0m23:07:35.933689 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:07:35.935389 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:07:35.937740 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:07:36.370767 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece6-d222-182f-bdfe-960c5b3dae0d) - Created
[0m23:07:41.323584 [debug] [Thread-5 (]: SQL status: OK in 5.390 seconds
[0m23:07:41.330437 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece6-d222-182f-bdfe-960c5b3dae0d, command-id=01f0ece6-d237-1acb-8bd2-1e058f51cb8c) - Closing
[0m23:07:41.360877 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:07:41.362636 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m23:07:47.362012 [debug] [Thread-5 (]: SQL status: OK in 6.000 seconds
[0m23:07:47.364419 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece6-d222-182f-bdfe-960c5b3dae0d, command-id=01f0ece6-d531-1721-896b-09062be093c1) - Closing
[0m23:07:47.850145 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m23:07:47.896948 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m23:07:47.898566 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece6-d222-182f-bdfe-960c5b3dae0d) - Closing
[0m23:07:48.037956 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e3884ab00>]}
[0m23:07:48.039834 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 12.26s]
[0m23:07:48.041538 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m23:07:48.043186 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m23:07:48.044951 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m23:07:48.046817 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m23:07:48.048284 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m23:07:48.049726 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m23:07:48.051618 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m23:07:48.069596 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:07:48.072314 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:07:48.074054 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:07:48.457502 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece6-d95a-11c9-b04d-3cd83222b7b7) - Created
[0m23:07:51.175256 [debug] [Thread-5 (]: SQL status: OK in 3.100 seconds
[0m23:07:51.177538 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece6-d95a-11c9-b04d-3cd83222b7b7, command-id=01f0ece6-d96c-12fe-b4b8-137e711789e2) - Closing
[0m23:07:51.181366 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:07:51.182985 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m23:07:54.178037 [debug] [Thread-5 (]: SQL status: OK in 2.990 seconds
[0m23:07:54.183387 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ece6-d95a-11c9-b04d-3cd83222b7b7, command-id=01f0ece6-db0b-1b5a-bbf0-49247f69a59a) - Closing
[0m23:07:54.188795 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m23:07:54.221978 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m23:07:54.224729 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ece6-d95a-11c9-b04d-3cd83222b7b7) - Closing
[0m23:07:54.346576 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c358530-5a3f-4be7-86c0-ee2855b92c0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e390d1180>]}
[0m23:07:54.348077 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 6.30s]
[0m23:07:54.349400 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m23:07:54.351837 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:07:54.353973 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:07:54.356069 [info ] [MainThread]: 
[0m23:07:54.357461 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 46.45 seconds (46.45s).
[0m23:07:54.359191 [debug] [MainThread]: Command end result
[0m23:07:54.457433 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:07:54.466397 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:07:54.483169 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:07:54.484740 [info ] [MainThread]: 
[0m23:07:54.488326 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:07:54.490099 [info ] [MainThread]: 
[0m23:07:54.491376 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:07:54.493522 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 50.81317, "process_in_blocks": "0", "process_kernel_time": 0.80507, "process_mem_max_rss": "246612", "process_out_blocks": "712", "process_user_time": 6.895432}
[0m23:07:54.494929 [debug] [MainThread]: Command `dbt seed` succeeded at 23:07:54.494756 after 50.81 seconds
[0m23:07:54.496189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5b8d2440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5b83b3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0e5adb53c0>]}
[0m23:07:54.497992 [debug] [MainThread]: Flushing usage events
[0m23:07:54.917348 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:08:01.210987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4ebb42200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eab182b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eab18250>]}


============================== 23:08:01.214434 | 66a55f91-6892-42fc-a20a-21b521d700fa ==============================
[0m23:08:01.214434 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:08:01.215846 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'introspect': 'True', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'debug': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'no_print': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'quiet': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'target_path': 'None'}
[0m23:08:02.171745 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:08:02.174073 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:08:02.175868 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:08:03.249370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4ed7f9180>]}
[0m23:08:03.373665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eaada380>]}
[0m23:08:03.375780 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:08:03.552808 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:08:04.008896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:08:04.010778 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:08:04.025698 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:08:04.118034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb193fd0>]}
[0m23:08:04.374624 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:08:04.382558 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:08:04.401983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb191840>]}
[0m23:08:04.403825 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:08:04.406258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb191600>]}
[0m23:08:04.409346 [info ] [MainThread]: 
[0m23:08:04.410609 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:08:04.411677 [info ] [MainThread]: 
[0m23:08:04.413073 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:08:04.414073 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:08:04.426014 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:08:04.427858 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:08:04.445787 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:08:04.447632 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:08:04.449248 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:04.961188 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e32c-1799-ae17-1de12c4e7f8b) - Created
[0m23:08:05.352146 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m23:08:05.367243 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e32c-1799-ae17-1de12c4e7f8b, command-id=01f0ece6-e341-1468-a5ca-ebabe5b313a6) - Closing
[0m23:08:05.369531 [debug] [ThreadPool]: On list_workspace: Close
[0m23:08:05.371672 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e32c-1799-ae17-1de12c4e7f8b) - Closing
[0m23:08:05.508634 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:08:05.512417 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:08:05.523913 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:08:05.527125 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:08:05.530023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:05.964920 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e3c9-1c5b-b638-4f0668bbff20) - Created
[0m23:08:06.358686 [debug] [ThreadPool]: SQL status: OK in 0.830 seconds
[0m23:08:06.380207 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e3c9-1c5b-b638-4f0668bbff20, command-id=01f0ece6-e3db-1cf7-954b-11e8cb00e2c6) - Closing
[0m23:08:06.384849 [debug] [ThreadPool]: On list_workspace: Close
[0m23:08:06.388031 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e3c9-1c5b-b638-4f0668bbff20) - Closing
[0m23:08:06.530893 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:08:06.535275 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:08:06.567721 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:08:06.572497 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:08:06.577573 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:07.015175 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e469-11b6-b68c-09b7217bd12e) - Created
[0m23:08:07.415284 [debug] [ThreadPool]: SQL status: OK in 0.840 seconds
[0m23:08:07.423321 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e469-11b6-b68c-09b7217bd12e, command-id=01f0ece6-e47c-11cf-80fe-4470acec5661) - Closing
[0m23:08:07.426590 [debug] [ThreadPool]: On list_workspace: Close
[0m23:08:07.428627 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e469-11b6-b68c-09b7217bd12e) - Closing
[0m23:08:07.560917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:08:07.562582 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:08:07.576386 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:08:07.578317 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:08:07.579899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:07.974449 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e4fb-1d5f-8df5-5c634f0b8a7c) - Created
[0m23:08:08.527570 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m23:08:08.532138 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e4fb-1d5f-8df5-5c634f0b8a7c, command-id=01f0ece6-e50d-1f10-95ea-ad05be6fffa3) - Closing
[0m23:08:08.533975 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:08:08.535380 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e4fb-1d5f-8df5-5c634f0b8a7c) - Closing
[0m23:08:08.667255 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:08:08.669521 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:08:08.674734 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:08:08.676297 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:08:08.677777 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:09.068706 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e5a3-1da7-a243-715434c22e51) - Created
[0m23:08:09.724821 [debug] [ThreadPool]: SQL status: OK in 1.050 seconds
[0m23:08:09.728839 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e5a3-1da7-a243-715434c22e51, command-id=01f0ece6-e5b5-1533-b30e-5effc1660fe3) - Closing
[0m23:08:09.730942 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:08:09.732712 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e5a3-1da7-a243-715434c22e51) - Closing
[0m23:08:09.872569 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:08:09.877141 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:08:09.892015 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:08:09.894495 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:08:09.896690 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:10.285733 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e65c-1a7e-8c18-c9c89f31ea0e) - Created
[0m23:08:10.844491 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m23:08:10.847800 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-e65c-1a7e-8c18-c9c89f31ea0e, command-id=01f0ece6-e66f-1d66-afcd-77df69b6d613) - Closing
[0m23:08:10.849806 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:08:10.852659 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-e65c-1a7e-8c18-c9c89f31ea0e) - Closing
[0m23:08:10.987300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4ed605b70>]}
[0m23:08:11.009014 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_accounts
[0m23:08:11.012738 [info ] [Thread-7 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m23:08:11.016314 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m23:08:11.020475 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m23:08:11.023133 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m23:08:11.041796 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m23:08:11.054612 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_accounts
[0m23:08:11.088826 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m23:08:11.093754 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:08:11.095465 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb205c90>]}
[0m23:08:11.134289 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m23:08:11.162022 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m23:08:11.181572 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m23:08:11.184082 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m23:08:11.187931 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m23:08:11.639662 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e72a-1de2-a685-9e04e42d85c8) - Created
[0m23:08:12.916625 [debug] [Thread-7 (]: SQL status: OK in 1.730 seconds
[0m23:08:12.922358 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece6-e72a-1de2-a685-9e04e42d85c8, command-id=01f0ece6-e73e-1c19-9424-8e7502baea43) - Closing
[0m23:08:12.944199 [debug] [Thread-7 (]: Applying tags to relation None
[0m23:08:12.948120 [debug] [Thread-7 (]: On model.banking_pipeline.stg_accounts: Close
[0m23:08:12.949993 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e72a-1de2-a685-9e04e42d85c8) - Closing
[0m23:08:13.093885 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4c4db0af0>]}
[0m23:08:13.097730 [info ] [Thread-7 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 2.07s]
[0m23:08:13.102448 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_accounts
[0m23:08:13.106039 [debug] [Thread-7 (]: Began running node model.banking_pipeline.stg_customers
[0m23:08:13.108763 [info ] [Thread-7 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m23:08:13.111456 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m23:08:13.113412 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m23:08:13.115251 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.stg_customers
[0m23:08:13.121825 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m23:08:13.135210 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.stg_customers
[0m23:08:13.139962 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m23:08:13.142258 [debug] [Thread-7 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m23:08:13.144103 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m23:08:13.159025 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m23:08:13.160263 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m23:08:13.161318 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m23:08:13.541297 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e84e-1937-8202-c6b49a8b9258) - Created
[0m23:08:14.584262 [debug] [Thread-7 (]: SQL status: OK in 1.420 seconds
[0m23:08:14.588065 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece6-e84e-1937-8202-c6b49a8b9258, command-id=01f0ece6-e85e-1fad-9aa4-ebbfa45d8f91) - Closing
[0m23:08:14.590003 [debug] [Thread-7 (]: Applying tags to relation None
[0m23:08:14.592252 [debug] [Thread-7 (]: On model.banking_pipeline.stg_customers: Close
[0m23:08:14.593541 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e84e-1937-8202-c6b49a8b9258) - Closing
[0m23:08:14.728920 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4c5737850>]}
[0m23:08:14.731366 [info ] [Thread-7 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.62s]
[0m23:08:14.733469 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.stg_customers
[0m23:08:14.737332 [debug] [Thread-7 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m23:08:14.739889 [info ] [Thread-7 (]: 3 of 4 START sql view model raw.int_customer_accounts .......................... [RUN]
[0m23:08:14.742452 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m23:08:14.744603 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m23:08:14.750213 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m23:08:14.762311 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m23:08:14.776819 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m23:08:14.780618 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m23:08:14.782706 [debug] [Thread-7 (]: Creating view `workspace`.`raw`.`int_customer_accounts`
[0m23:08:14.787551 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m23:08:14.799100 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m23:08:14.801051 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`raw`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m23:08:14.804112 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m23:08:15.204929 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e94b-1df7-be63-ac8a0b4e7754) - Created
[0m23:08:16.031171 [debug] [Thread-7 (]: SQL status: OK in 1.230 seconds
[0m23:08:16.043516 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece6-e94b-1df7-be63-ac8a0b4e7754, command-id=01f0ece6-e95d-1810-9d83-787ddf393fe7) - Closing
[0m23:08:16.051621 [debug] [Thread-7 (]: Applying tags to relation None
[0m23:08:16.057516 [debug] [Thread-7 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m23:08:16.060437 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-e94b-1df7-be63-ac8a0b4e7754) - Closing
[0m23:08:16.203572 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb1de350>]}
[0m23:08:16.208309 [info ] [Thread-7 (]: 3 of 4 OK created sql view model raw.int_customer_accounts ..................... [[32mOK[0m in 1.46s]
[0m23:08:16.211648 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m23:08:16.214819 [debug] [Thread-7 (]: Began running node model.banking_pipeline.account_interest_summary
[0m23:08:16.217214 [info ] [Thread-7 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m23:08:16.220759 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m23:08:16.222874 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m23:08:16.224731 [debug] [Thread-7 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m23:08:16.232997 [debug] [Thread-7 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m23:08:16.248477 [debug] [Thread-7 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m23:08:16.271988 [debug] [Thread-7 (]: MATERIALIZING TABLE
[0m23:08:16.286463 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:08:16.288207 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4c443b8b0>]}
[0m23:08:16.321915 [debug] [Thread-7 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m23:08:16.333830 [debug] [Thread-7 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m23:08:16.336342 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`raw`.`int_customer_accounts`
  
[0m23:08:16.337707 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m23:08:16.739103 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-ea35-1d78-9b17-6aa31f611a3b) - Created
[0m23:08:22.513829 [debug] [Thread-7 (]: SQL status: OK in 6.180 seconds
[0m23:08:22.515909 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0ece6-ea35-1d78-9b17-6aa31f611a3b, command-id=01f0ece6-ea47-16c7-88f3-ea34b196add6) - Closing
[0m23:08:22.677318 [debug] [Thread-7 (]: Applying tags to relation None
[0m23:08:22.700757 [debug] [Thread-7 (]: On model.banking_pipeline.account_interest_summary: Close
[0m23:08:22.702837 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0ece6-ea35-1d78-9b17-6aa31f611a3b) - Closing
[0m23:08:22.833544 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66a55f91-6892-42fc-a20a-21b521d700fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4c5677700>]}
[0m23:08:22.836269 [info ] [Thread-7 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 6.61s]
[0m23:08:22.839678 [debug] [Thread-7 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m23:08:22.842721 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:08:22.844211 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:08:22.845871 [info ] [MainThread]: 
[0m23:08:22.847334 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 18.43 seconds (18.43s).
[0m23:08:22.849758 [debug] [MainThread]: Command end result
[0m23:08:22.941409 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:08:22.949857 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:08:22.967611 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:08:22.969556 [info ] [MainThread]: 
[0m23:08:22.972536 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:08:22.974217 [info ] [MainThread]: 
[0m23:08:22.975972 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m23:08:22.978601 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 21.89984, "process_in_blocks": "0", "process_kernel_time": 0.634695, "process_mem_max_rss": "246536", "process_out_blocks": "0", "process_user_time": 6.632171}
[0m23:08:22.980145 [debug] [MainThread]: Command `dbt run` succeeded at 23:08:22.979976 after 21.90 seconds
[0m23:08:22.981422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4ebb42200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4ed7f9180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eb4eb1d1cc0>]}
[0m23:08:22.982916 [debug] [MainThread]: Flushing usage events
[0m23:08:23.467915 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:08:29.537381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d0e621d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1cfc102e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1cfc10280>]}


============================== 23:08:29.544084 | dc84984a-e1df-42fd-89b9-c1ce5d997adf ==============================
[0m23:08:29.544084 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:08:29.545773 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'introspect': 'True', 'fail_fast': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'partial_parse': 'True', 'empty': 'None', 'debug': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/opt/dagster/app/dbt', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'cache_selected_only': 'False', 'use_colors': 'True', 'quiet': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'use_experimental_parser': 'False'}
[0m23:08:30.592866 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:08:30.594431 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:08:30.596340 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:08:31.692341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1cdfafd30>]}
[0m23:08:31.825267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d04df130>]}
[0m23:08:31.827294 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:08:32.015072 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:08:32.492054 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:08:32.493840 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:08:32.510057 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:08:32.609850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1a9ce7e50>]}
[0m23:08:32.881130 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:08:32.890254 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:08:32.922005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d0329540>]}
[0m23:08:32.924166 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:08:32.925620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d032b4f0>]}
[0m23:08:32.928709 [info ] [MainThread]: 
[0m23:08:32.930166 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:08:32.931547 [info ] [MainThread]: 
[0m23:08:32.932933 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:08:32.934325 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:08:32.945814 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:08:32.947584 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:08:32.967122 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:08:32.969073 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:08:32.970991 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:33.410340 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f425-1f64-9207-41d1bdbf84da) - Created
[0m23:08:34.051432 [debug] [ThreadPool]: SQL status: OK in 1.080 seconds
[0m23:08:34.063202 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-f425-1f64-9207-41d1bdbf84da, command-id=01f0ece6-f437-1c9e-8ca7-78f3b5cab238) - Closing
[0m23:08:34.065836 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:08:34.067967 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f425-1f64-9207-41d1bdbf84da) - Closing
[0m23:08:34.231101 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:08:34.234899 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:08:34.246009 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:08:34.249280 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:08:34.252324 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:34.656831 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f4e2-1e9b-8ff1-86dcaadac792) - Created
[0m23:08:35.173591 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m23:08:35.179237 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-f4e2-1e9b-8ff1-86dcaadac792, command-id=01f0ece6-f4f6-192b-b80f-ed19af51a484) - Closing
[0m23:08:35.181696 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:08:35.183446 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f4e2-1e9b-8ff1-86dcaadac792) - Closing
[0m23:08:35.330670 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:08:35.333469 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:08:35.346440 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:08:35.349853 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:08:35.351989 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:08:35.853700 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f599-11c3-aa72-996c3c5800e8) - Created
[0m23:08:36.487554 [debug] [ThreadPool]: SQL status: OK in 1.130 seconds
[0m23:08:36.503290 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece6-f599-11c3-aa72-996c3c5800e8, command-id=01f0ece6-f5ad-14c2-b109-e7f25f98a122) - Closing
[0m23:08:36.510859 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:08:36.514465 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece6-f599-11c3-aa72-996c3c5800e8) - Closing
[0m23:08:36.664674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc84984a-e1df-42fd-89b9-c1ce5d997adf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d27204f0>]}
[0m23:08:36.684360 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:08:36.689887 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m23:08:36.693652 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m23:08:36.695991 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m23:08:36.698267 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:08:36.729926 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:08:36.746591 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:08:36.772891 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:08:36.786452 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:08:36.788433 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m23:08:36.790962 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:37.177001 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f664-101e-aa11-d5f75d8c63f4) - Created
[0m23:08:38.347805 [debug] [Thread-4 (]: SQL status: OK in 1.560 seconds
[0m23:08:38.355169 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-f664-101e-aa11-d5f75d8c63f4, command-id=01f0ece6-f675-1a41-b6e9-f790a06b2d19) - Closing
[0m23:08:38.362852 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m23:08:38.365033 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f664-101e-aa11-d5f75d8c63f4) - Closing
[0m23:08:38.509118 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.82s]
[0m23:08:38.511272 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:08:38.512948 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:08:38.514839 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m23:08:38.517115 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m23:08:38.519107 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m23:08:38.520970 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:08:38.529612 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:08:38.546252 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:08:38.550692 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:08:38.563682 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:08:38.564661 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m23:08:38.565677 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:38.951006 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f773-17fe-a297-30566701e0cd) - Created
[0m23:08:39.654911 [debug] [Thread-4 (]: SQL status: OK in 1.090 seconds
[0m23:08:39.659054 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-f773-17fe-a297-30566701e0cd, command-id=01f0ece6-f784-1f31-8460-d6bd4c550954) - Closing
[0m23:08:39.661150 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m23:08:39.662625 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f773-17fe-a297-30566701e0cd) - Closing
[0m23:08:39.783912 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.27s]
[0m23:08:39.786813 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:08:39.789844 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:08:39.791795 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m23:08:39.794055 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m23:08:39.795890 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m23:08:39.797632 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:08:39.812166 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:08:39.829097 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:08:39.839358 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:08:39.851482 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:08:39.853046 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m23:08:39.855556 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:40.252981 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f83a-1bd2-9ec4-d31833cd0183) - Created
[0m23:08:41.087302 [debug] [Thread-4 (]: SQL status: OK in 1.230 seconds
[0m23:08:41.097597 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-f83a-1bd2-9ec4-d31833cd0183, command-id=01f0ece6-f84b-15d1-9536-f70864c9996e) - Closing
[0m23:08:41.101366 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m23:08:41.105214 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f83a-1bd2-9ec4-d31833cd0183) - Closing
[0m23:08:41.241032 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.45s]
[0m23:08:41.243040 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:08:41.244558 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:08:41.245815 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m23:08:41.247556 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m23:08:41.249099 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m23:08:41.250455 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:08:41.260793 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:08:41.275253 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:08:41.279754 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:08:41.294117 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:08:41.295388 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m23:08:41.296402 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:41.672569 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f912-19b3-9611-60f6b37da341) - Created
[0m23:08:42.317854 [debug] [Thread-4 (]: SQL status: OK in 1.020 seconds
[0m23:08:42.326973 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-f912-19b3-9611-60f6b37da341, command-id=01f0ece6-f924-1076-9c34-43790f539945) - Closing
[0m23:08:42.331480 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m23:08:42.335101 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f912-19b3-9611-60f6b37da341) - Closing
[0m23:08:42.462675 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.21s]
[0m23:08:42.466821 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:08:42.470510 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:08:42.475685 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m23:08:42.479957 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m23:08:42.483305 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m23:08:42.486127 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:08:42.499870 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:08:42.515319 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:08:42.519814 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:08:42.534184 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:08:42.535284 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:08:42.536221 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:42.946176 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f9d4-19e3-bc8d-650d241681a8) - Created
[0m23:08:43.524716 [debug] [Thread-4 (]: SQL status: OK in 0.990 seconds
[0m23:08:43.528070 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-f9d4-19e3-bc8d-650d241681a8, command-id=01f0ece6-f9e7-1765-bfd1-ab4e85691f5d) - Closing
[0m23:08:43.529814 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m23:08:43.531248 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-f9d4-19e3-bc8d-650d241681a8) - Closing
[0m23:08:43.670242 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.19s]
[0m23:08:43.673623 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:08:43.675860 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:08:43.677899 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m23:08:43.680571 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m23:08:43.682876 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m23:08:43.684919 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:08:43.698510 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:08:43.716353 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:08:43.723512 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:08:43.738757 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:08:43.740692 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m23:08:43.742100 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:44.112605 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fa86-137c-ac98-f5f9177e162a) - Created
[0m23:08:44.796035 [debug] [Thread-4 (]: SQL status: OK in 1.050 seconds
[0m23:08:44.799995 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fa86-137c-ac98-f5f9177e162a, command-id=01f0ece6-fa98-1ed8-998a-389dc2ba58e7) - Closing
[0m23:08:44.802523 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m23:08:44.805491 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fa86-137c-ac98-f5f9177e162a) - Closing
[0m23:08:44.942961 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.26s]
[0m23:08:44.946003 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:08:44.948437 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:08:44.950781 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m23:08:44.953855 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m23:08:44.957436 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m23:08:44.959606 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:08:44.970082 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:08:44.984879 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:08:44.991026 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:08:45.001951 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:08:45.004068 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m23:08:45.007249 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:45.386333 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fb48-152b-8fd0-358c103b7ef4) - Created
[0m23:08:46.041175 [debug] [Thread-4 (]: SQL status: OK in 1.030 seconds
[0m23:08:46.060808 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fb48-152b-8fd0-358c103b7ef4, command-id=01f0ece6-fb5c-1943-bddf-c4b3d28a3d9f) - Closing
[0m23:08:46.069217 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m23:08:46.077300 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fb48-152b-8fd0-358c103b7ef4) - Closing
[0m23:08:46.231866 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.28s]
[0m23:08:46.240459 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:08:46.247145 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:08:46.252650 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m23:08:46.261154 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m23:08:46.267473 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m23:08:46.274636 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:08:46.309376 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:08:46.340562 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:08:46.351406 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:08:46.366329 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:08:46.367431 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:08:46.368345 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:46.749033 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fc18-1e38-813e-623b577e0d1f) - Created
[0m23:08:47.529917 [debug] [Thread-4 (]: SQL status: OK in 1.160 seconds
[0m23:08:47.539261 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fc18-1e38-813e-623b577e0d1f, command-id=01f0ece6-fc2a-13c3-a1f8-79949817d764) - Closing
[0m23:08:47.543449 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m23:08:47.545928 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fc18-1e38-813e-623b577e0d1f) - Closing
[0m23:08:47.675354 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.42s]
[0m23:08:47.677335 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:08:47.678851 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:08:47.680298 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m23:08:47.682015 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m23:08:47.683461 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m23:08:47.684778 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:08:47.695546 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:08:47.710863 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:08:47.715421 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:08:47.730224 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:08:47.731550 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:08:47.732740 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:48.100489 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fce7-1d44-874e-35b479f9807e) - Created
[0m23:08:48.852649 [debug] [Thread-4 (]: SQL status: OK in 1.120 seconds
[0m23:08:48.857822 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fce7-1d44-874e-35b479f9807e, command-id=01f0ece6-fcf9-18d3-9d68-38a2be6b728b) - Closing
[0m23:08:48.860308 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m23:08:48.862090 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fce7-1d44-874e-35b479f9807e) - Closing
[0m23:08:48.990759 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.31s]
[0m23:08:48.994664 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:08:48.997890 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:08:49.001215 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m23:08:49.004335 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m23:08:49.008477 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m23:08:49.010839 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:08:49.012743 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:08:49.065644 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:49.067482 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:08:49.069041 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:49.448500 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd) - Created
[0m23:08:50.164686 [debug] [Thread-4 (]: SQL status: OK in 1.100 seconds
[0m23:08:50.176165 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd, command-id=01f0ece6-fdc7-19ef-ab60-493fb2fed580) - Closing
[0m23:08:50.208203 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.233650 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.299770 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.301974 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:08:50.657695 [debug] [Thread-4 (]: SQL status: OK in 0.350 seconds
[0m23:08:50.662152 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd, command-id=01f0ece6-fe49-1191-900a-7d7556656255) - Closing
[0m23:08:50.670611 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.674577 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m23:08:50.931147 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m23:08:50.936759 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd, command-id=01f0ece6-fe81-1725-85af-84de37784e9f) - Closing
[0m23:08:50.961362 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.978835 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:50.980089 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:08:51.440055 [debug] [Thread-4 (]: SQL status: OK in 0.460 seconds
[0m23:08:51.448504 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd, command-id=01f0ece6-feb0-1128-89f9-2f0d2fdf3e07) - Closing
[0m23:08:51.465128 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:08:51.476674 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:08:51.478533 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:08:51.766783 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m23:08:51.770487 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd, command-id=01f0ece6-fefb-1ccd-8eca-df1b5c74e148) - Closing
[0m23:08:51.782307 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m23:08:51.784532 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-fdb3-1ef5-9289-bd69396cb8cd) - Closing
[0m23:08:51.960126 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.96s]
[0m23:08:51.963061 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:08:51.965369 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:08:51.967670 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m23:08:51.970434 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m23:08:51.973755 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m23:08:51.976539 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:08:51.978812 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:08:52.000654 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:52.002508 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:08:52.003982 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:52.600574 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-ff8b-1245-83a7-879d93013578) - Created
[0m23:08:53.014986 [debug] [Thread-4 (]: SQL status: OK in 1.010 seconds
[0m23:08:53.030287 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-ff8b-1245-83a7-879d93013578, command-id=01f0ece6-ffa8-1ca9-8487-22ef8e377659) - Closing
[0m23:08:53.053296 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.087193 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.111397 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.113109 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:08:53.460569 [debug] [Thread-4 (]: SQL status: OK in 0.350 seconds
[0m23:08:53.468161 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-ff8b-1245-83a7-879d93013578, command-id=01f0ece6-fff5-1a86-b328-3660813afcd0) - Closing
[0m23:08:53.482557 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.486199 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m23:08:53.931273 [debug] [Thread-4 (]: SQL status: OK in 0.440 seconds
[0m23:08:53.935361 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-ff8b-1245-83a7-879d93013578, command-id=01f0ece7-002f-1063-b0f1-120356529b0a) - Closing
[0m23:08:53.940929 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.959362 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:53.960727 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:08:54.293122 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m23:08:54.299939 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-ff8b-1245-83a7-879d93013578, command-id=01f0ece7-0077-168e-8c96-9c09e37550ee) - Closing
[0m23:08:54.308310 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:08:54.310573 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:08:54.312301 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:08:54.608349 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m23:08:54.610703 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece6-ff8b-1245-83a7-879d93013578, command-id=01f0ece7-00ad-1084-a024-bea3f25ca83a) - Closing
[0m23:08:54.613882 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m23:08:54.615414 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece6-ff8b-1245-83a7-879d93013578) - Closing
[0m23:08:54.784125 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.81s]
[0m23:08:54.785952 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:08:54.787489 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:08:54.789626 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m23:08:54.792392 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m23:08:54.794012 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m23:08:54.795589 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:08:54.796943 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:08:54.812739 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:54.814644 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:08:54.816125 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:08:55.315510 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505) - Created
[0m23:08:55.808869 [debug] [Thread-4 (]: SQL status: OK in 0.990 seconds
[0m23:08:55.815895 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-0146-1f33-946d-f419dddaa5d9) - Closing
[0m23:08:55.825059 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:55.855649 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:55.885574 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:55.887353 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m23:08:56.154091 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m23:08:56.159042 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-019c-19f3-b36f-b6ab71441258) - Closing
[0m23:08:56.161951 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:56.163528 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:08:56.474825 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m23:08:56.478770 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-01c8-108c-a35b-4aedd5c06739) - Closing
[0m23:08:56.484793 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:56.487029 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m23:08:56.870249 [debug] [Thread-4 (]: SQL status: OK in 0.380 seconds
[0m23:08:56.876310 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-01f7-1e46-8c0a-e9958188886b) - Closing
[0m23:08:56.879870 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:56.900226 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:56.901621 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:08:57.371809 [debug] [Thread-4 (]: SQL status: OK in 0.470 seconds
[0m23:08:57.386329 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-0237-1bfb-9ce2-05fde9bf0cdf) - Closing
[0m23:08:57.394757 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m23:08:57.397354 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:08:57.399164 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m23:08:57.744806 [debug] [Thread-4 (]: SQL status: OK in 0.340 seconds
[0m23:08:57.754116 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505, command-id=01f0ece7-0287-1960-844f-510ebad542d0) - Closing
[0m23:08:57.776588 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m23:08:57.781282 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-0132-17e2-92d9-234e22d9e505) - Closing
[0m23:08:57.901166 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 3.11s]
[0m23:08:57.908666 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:08:57.912611 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:08:57.914357 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:08:57.916274 [info ] [MainThread]: 
[0m23:08:57.918192 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 24.98 seconds (24.98s).
[0m23:08:57.924611 [debug] [MainThread]: Command end result
[0m23:08:58.175885 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:08:58.187069 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:08:58.209599 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:08:58.210866 [info ] [MainThread]: 
[0m23:08:58.212147 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:08:58.213392 [info ] [MainThread]: 
[0m23:08:58.214656 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m23:08:58.215741 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m23:08:58.216729 [info ] [MainThread]: 
[0m23:08:58.217802 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m23:08:58.218839 [info ] [MainThread]: 
[0m23:08:58.219880 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m23:08:58.221855 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 28.853746, "process_in_blocks": "0", "process_kernel_time": 0.632301, "process_mem_max_rss": "255792", "process_out_blocks": "0", "process_user_time": 7.995818}
[0m23:08:58.224636 [debug] [MainThread]: Command `dbt test` failed at 23:08:58.224312 after 28.86 seconds
[0m23:08:58.226296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d0e621d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d0342170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1d0343250>]}
[0m23:08:58.227802 [debug] [MainThread]: Flushing usage events
[0m23:08:58.624444 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:13:49.122011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ff9ee6530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ff8d382b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ff8d38250>]}


============================== 23:13:49.129782 | 3780c4e8-8f1e-4a50-96d6-bd9c1be39c45 ==============================
[0m23:13:49.129782 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:13:49.131673 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'write_json': 'True', 'target_path': 'None', 'debug': 'False', 'use_colors': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'static_parser': 'True', 'version_check': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'partial_parse': 'True', 'introspect': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'fail_fast': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'cache_selected_only': 'False', 'indirect_selection': 'eager'}
[0m23:13:50.013316 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:13:50.015225 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:13:50.016756 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:13:51.017540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fd37905b0>]}
[0m23:13:51.125295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ff8d04f70>]}
[0m23:13:51.127546 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:13:51.303038 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:13:51.704632 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:13:51.706438 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:13:51.719664 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:13:51.803178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fd2d34d30>]}
[0m23:13:52.029053 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:13:52.038537 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:13:52.074872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fd2c78f10>]}
[0m23:13:52.076565 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:13:52.077864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fd36727a0>]}
[0m23:13:52.081996 [info ] [MainThread]: 
[0m23:13:52.084350 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:13:52.087493 [info ] [MainThread]: 
[0m23:13:52.089487 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:13:52.090979 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:13:52.103127 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:13:52.104837 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:13:52.122287 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:13:52.124218 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:13:52.125817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:13:52.632717 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b26b-1a0a-9f58-424615046241) - Created
[0m23:13:53.349069 [debug] [ThreadPool]: SQL status: OK in 1.220 seconds
[0m23:13:53.354446 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece7-b26b-1a0a-9f58-424615046241, command-id=01f0ece7-b27b-1b15-b81f-c79d30450654) - Closing
[0m23:13:53.356400 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:13:53.357837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b26b-1a0a-9f58-424615046241) - Closing
[0m23:13:53.511287 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:13:53.513733 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:13:53.519191 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:13:53.521134 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:13:53.523099 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:13:53.967221 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b334-1e71-9b3d-76f31206f0f5) - Created
[0m23:13:54.440015 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m23:13:54.444087 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece7-b334-1e71-9b3d-76f31206f0f5, command-id=01f0ece7-b348-1954-a60a-007eef433b79) - Closing
[0m23:13:54.446041 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:13:54.447489 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b334-1e71-9b3d-76f31206f0f5) - Closing
[0m23:13:54.611156 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:13:54.617522 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:13:54.629981 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:13:54.632384 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:13:54.634296 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:13:56.067514 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b476-14a2-be17-509271add0a0) - Created
[0m23:13:56.519542 [debug] [ThreadPool]: SQL status: OK in 1.890 seconds
[0m23:13:56.526395 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ece7-b476-14a2-be17-509271add0a0, command-id=01f0ece7-b48b-163f-804a-369ec296b19e) - Closing
[0m23:13:56.529296 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:13:56.531502 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ece7-b476-14a2-be17-509271add0a0) - Closing
[0m23:13:56.673727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3780c4e8-8f1e-4a50-96d6-bd9c1be39c45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fd36727a0>]}
[0m23:13:56.686886 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:13:56.689155 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m23:13:56.691715 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m23:13:56.693633 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m23:13:56.695548 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:13:56.720661 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:13:56.734006 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:13:56.757241 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:13:56.770699 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:13:56.772037 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m23:13:56.773162 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:13:57.149068 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b51d-15da-8b2b-34f54797c6b6) - Created
[0m23:13:57.452498 [debug] [Thread-4 (]: SQL status: OK in 0.680 seconds
[0m23:13:57.456487 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b51d-15da-8b2b-34f54797c6b6, command-id=01f0ece7-b530-166b-b96d-f70a434d5630) - Closing
[0m23:13:57.461445 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m23:13:57.463008 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b51d-15da-8b2b-34f54797c6b6) - Closing
[0m23:13:57.595631 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 0.90s]
[0m23:13:57.597975 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:13:57.600616 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:13:57.602749 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m23:13:57.606358 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m23:13:57.607812 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m23:13:57.609090 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:13:57.615382 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:13:57.632633 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:13:57.638477 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:13:57.655787 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:13:57.657370 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m23:13:57.658602 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:13:58.055890 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b5a5-1cdb-9acf-abc887c80d23) - Created
[0m23:13:58.293982 [debug] [Thread-4 (]: SQL status: OK in 0.640 seconds
[0m23:13:58.297403 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b5a5-1cdb-9acf-abc887c80d23, command-id=01f0ece7-b5b8-12b0-b1fd-0fe21e6bd74d) - Closing
[0m23:13:58.299415 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m23:13:58.301379 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b5a5-1cdb-9acf-abc887c80d23) - Closing
[0m23:13:58.435373 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 0.83s]
[0m23:13:58.441821 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:13:58.444303 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:13:58.446738 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m23:13:58.450105 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m23:13:58.451676 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m23:13:58.453536 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:13:58.466377 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:13:58.480600 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:13:58.489889 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:13:58.502364 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:13:58.504734 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m23:13:58.506349 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:13:58.887259 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b625-1ed4-b672-a81abc9418ed) - Created
[0m23:13:59.151733 [debug] [Thread-4 (]: SQL status: OK in 0.640 seconds
[0m23:13:59.160335 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b625-1ed4-b672-a81abc9418ed, command-id=01f0ece7-b636-1acf-a80d-ac679988b7ac) - Closing
[0m23:13:59.163962 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m23:13:59.166697 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b625-1ed4-b672-a81abc9418ed) - Closing
[0m23:13:59.314940 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 0.86s]
[0m23:13:59.317152 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:13:59.319543 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:13:59.321947 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m23:13:59.325221 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m23:13:59.326922 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m23:13:59.328401 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:13:59.340361 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:13:59.354673 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:13:59.359585 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:13:59.373554 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:13:59.374830 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m23:13:59.375948 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:13:59.756906 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b6aa-1eef-bece-ccad2a1815ff) - Created
[0m23:14:00.076964 [debug] [Thread-4 (]: SQL status: OK in 0.700 seconds
[0m23:14:00.081053 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b6aa-1eef-bece-ccad2a1815ff, command-id=01f0ece7-b6bb-13c2-abbf-cc8ded254473) - Closing
[0m23:14:00.083346 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m23:14:00.084973 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b6aa-1eef-bece-ccad2a1815ff) - Closing
[0m23:14:00.207342 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 0.88s]
[0m23:14:00.209852 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:14:00.212178 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:14:00.214249 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m23:14:00.217406 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m23:14:00.219003 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m23:14:00.220972 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:14:00.232800 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:14:00.250782 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:14:00.257141 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:14:00.271720 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:14:00.273467 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:14:00.275236 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:00.666993 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b735-1cd3-aa64-78e98d93dfef) - Created
[0m23:14:00.972188 [debug] [Thread-4 (]: SQL status: OK in 0.700 seconds
[0m23:14:00.980500 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b735-1cd3-aa64-78e98d93dfef, command-id=01f0ece7-b745-1ab6-913e-06466b2749b6) - Closing
[0m23:14:00.983772 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m23:14:00.986120 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b735-1cd3-aa64-78e98d93dfef) - Closing
[0m23:14:01.116996 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 0.90s]
[0m23:14:01.119265 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:14:01.121195 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:14:01.123128 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m23:14:01.125916 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m23:14:01.127492 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m23:14:01.128921 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:14:01.139146 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:14:01.153748 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:14:01.159530 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:14:01.176503 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:14:01.177871 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m23:14:01.179176 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:01.589958 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b7c1-1c00-bf8a-ffd08e0a6116) - Created
[0m23:14:01.870330 [debug] [Thread-4 (]: SQL status: OK in 0.690 seconds
[0m23:14:01.875115 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b7c1-1c00-bf8a-ffd08e0a6116, command-id=01f0ece7-b7d2-165d-9b7d-e9a4105ea828) - Closing
[0m23:14:01.877694 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m23:14:01.879617 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b7c1-1c00-bf8a-ffd08e0a6116) - Closing
[0m23:14:02.026744 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 0.90s]
[0m23:14:02.028982 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:14:02.031140 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:14:02.033422 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m23:14:02.036778 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m23:14:02.038938 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m23:14:02.041057 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:14:02.051211 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:14:02.066226 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:14:02.071830 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:14:02.088747 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:14:02.089970 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m23:14:02.091047 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:02.453636 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b844-19e1-bb55-cc072b7e621d) - Created
[0m23:14:03.019857 [debug] [Thread-4 (]: SQL status: OK in 0.930 seconds
[0m23:14:03.023825 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b844-19e1-bb55-cc072b7e621d, command-id=01f0ece7-b857-18a2-9c99-bf9d923ea642) - Closing
[0m23:14:03.026066 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m23:14:03.027735 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b844-19e1-bb55-cc072b7e621d) - Closing
[0m23:14:03.155753 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.12s]
[0m23:14:03.158005 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:14:03.160439 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:14:03.162802 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m23:14:03.166476 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m23:14:03.168316 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m23:14:03.169900 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:14:03.181009 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:14:03.198549 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:14:03.205761 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:14:03.222107 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:14:03.223709 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:14:03.225169 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:03.595859 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b8f5-101a-b592-3e829a193efa) - Created
[0m23:14:03.899695 [debug] [Thread-4 (]: SQL status: OK in 0.670 seconds
[0m23:14:03.914062 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b8f5-101a-b592-3e829a193efa, command-id=01f0ece7-b904-15ac-88e2-07a10640fd88) - Closing
[0m23:14:03.921691 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m23:14:03.926585 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b8f5-101a-b592-3e829a193efa) - Closing
[0m23:14:04.052018 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 0.89s]
[0m23:14:04.055023 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:14:04.057440 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:14:04.060351 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m23:14:04.063279 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m23:14:04.064829 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m23:14:04.066285 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:14:04.074802 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:14:04.086215 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:14:04.091326 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:14:04.106309 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:14:04.108624 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:14:04.109905 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:04.527672 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b981-1396-b06f-30a83c9445b2) - Created
[0m23:14:05.094727 [debug] [Thread-4 (]: SQL status: OK in 0.980 seconds
[0m23:14:05.098212 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-b981-1396-b06f-30a83c9445b2, command-id=01f0ece7-b994-1af5-b44b-a99df2663c4e) - Closing
[0m23:14:05.100449 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m23:14:05.101998 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-b981-1396-b06f-30a83c9445b2) - Closing
[0m23:14:05.236082 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.17s]
[0m23:14:05.238653 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:14:05.240581 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:14:05.242618 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m23:14:05.246131 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m23:14:05.249113 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m23:14:05.250978 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:14:05.253046 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:14:05.335098 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:05.336847 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:14:05.338909 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:05.717056 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb) - Created
[0m23:14:06.080390 [debug] [Thread-4 (]: SQL status: OK in 0.740 seconds
[0m23:14:06.085893 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb, command-id=01f0ece7-ba48-1cde-af79-13cc9c87fdf4) - Closing
[0m23:14:06.116821 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.140963 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.198178 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.205258 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:14:06.509322 [debug] [Thread-4 (]: SQL status: OK in 0.300 seconds
[0m23:14:06.515504 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb, command-id=01f0ece7-ba93-1b3f-b919-ab47d2276ce8) - Closing
[0m23:14:06.525096 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.527816 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m23:14:06.773493 [debug] [Thread-4 (]: SQL status: OK in 0.240 seconds
[0m23:14:06.777362 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb, command-id=01f0ece7-bac4-14a8-9569-b727be4969a6) - Closing
[0m23:14:06.789663 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.808246 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:06.809747 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:14:07.160920 [debug] [Thread-4 (]: SQL status: OK in 0.350 seconds
[0m23:14:07.167850 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb, command-id=01f0ece7-baef-15a2-a703-833acd300e2d) - Closing
[0m23:14:07.182140 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:14:07.189574 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:14:07.191528 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:14:07.451000 [debug] [Thread-4 (]: SQL status: OK in 0.260 seconds
[0m23:14:07.453077 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb, command-id=01f0ece7-bb29-1d92-ba1b-e63a9055e28d) - Closing
[0m23:14:07.458082 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m23:14:07.459607 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-ba37-15e1-9e72-1a8cf9e94ceb) - Closing
[0m23:14:07.593487 [info ] [Thread-4 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.35s]
[0m23:14:07.598271 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:14:07.601176 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:14:07.603992 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m23:14:07.607526 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m23:14:07.610419 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m23:14:07.612457 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:14:07.614326 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:14:07.632955 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:07.634454 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:14:07.635931 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:08.017861 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a) - Created
[0m23:14:08.322095 [debug] [Thread-4 (]: SQL status: OK in 0.690 seconds
[0m23:14:08.325984 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a, command-id=01f0ece7-bba6-18bb-af9e-aae7b9989543) - Closing
[0m23:14:08.331101 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:08.356302 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:08.380247 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:08.381923 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:14:08.751654 [debug] [Thread-4 (]: SQL status: OK in 0.370 seconds
[0m23:14:08.755946 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a, command-id=01f0ece7-bbe0-1c29-89d7-af96e8aab215) - Closing
[0m23:14:08.763461 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:08.766048 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m23:14:09.003535 [debug] [Thread-4 (]: SQL status: OK in 0.230 seconds
[0m23:14:09.014080 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a, command-id=01f0ece7-bc1a-15a0-a132-010ffb49f3ca) - Closing
[0m23:14:09.022374 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:09.057715 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:09.060795 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:14:09.388574 [debug] [Thread-4 (]: SQL status: OK in 0.330 seconds
[0m23:14:09.393351 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a, command-id=01f0ece7-bc46-12cc-aa8a-37e003f424da) - Closing
[0m23:14:09.399362 [debug] [Thread-4 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:14:09.401270 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:14:09.402805 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:14:09.675822 [debug] [Thread-4 (]: SQL status: OK in 0.270 seconds
[0m23:14:09.678004 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a, command-id=01f0ece7-bc7a-1dbb-a83d-1f200612cf3e) - Closing
[0m23:14:09.681207 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m23:14:09.682853 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-bb96-1e73-b48f-801d33d8080a) - Closing
[0m23:14:09.823434 [info ] [Thread-4 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.22s]
[0m23:14:09.826035 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:14:09.828259 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:14:09.830517 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m23:14:09.833669 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m23:14:09.835429 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m23:14:09.837090 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:14:09.838628 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:14:09.855350 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:09.857204 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:14:09.858698 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:14:10.289100 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e) - Created
[0m23:14:10.578030 [debug] [Thread-4 (]: SQL status: OK in 0.720 seconds
[0m23:14:10.581338 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-bd03-135e-b07b-5b2adbf59b84) - Closing
[0m23:14:10.586336 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:10.610126 [debug] [Thread-4 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:10.632476 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:10.634437 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

    
[0m23:14:10.948297 [debug] [Thread-4 (]: SQL status: OK in 0.310 seconds
[0m23:14:10.955046 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-bd3a-1206-bb8a-dcd354870d93) - Closing
[0m23:14:10.958943 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:10.961030 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:14:11.248225 [debug] [Thread-4 (]: SQL status: OK in 0.290 seconds
[0m23:14:11.250773 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-bd6b-1f67-8f41-fa8c1e4df08f) - Closing
[0m23:14:11.254911 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:11.256516 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m23:14:11.469563 [debug] [Thread-4 (]: SQL status: OK in 0.210 seconds
[0m23:14:11.473337 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-bd95-1ea6-b9df-a91f2f51dc82) - Closing
[0m23:14:11.476290 [debug] [Thread-4 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:11.493210 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:11.494539 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:14:12.056444 [debug] [Thread-4 (]: SQL status: OK in 0.560 seconds
[0m23:14:12.060639 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-bdbb-196c-9363-259ee57efb48) - Closing
[0m23:14:12.065565 [debug] [Thread-4 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m23:14:12.067461 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:14:12.068989 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m23:14:12.321369 [debug] [Thread-4 (]: SQL status: OK in 0.250 seconds
[0m23:14:12.324103 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e, command-id=01f0ece7-be12-11a6-b7ff-d2f8f9fd756e) - Closing
[0m23:14:12.328785 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m23:14:12.330629 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ece7-bcf0-1261-b27a-c04aa61c219e) - Closing
[0m23:14:12.473926 [error] [Thread-4 (]: 12 of 12 FAIL 1 account_interest_summary::non_savings_account_filtered_out ..... [[31mFAIL 1[0m in 2.64s]
[0m23:14:12.476568 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:14:12.480486 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:14:12.482911 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:14:12.485034 [info ] [MainThread]: 
[0m23:14:12.486837 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 20.40 seconds (20.40s).
[0m23:14:12.490663 [debug] [MainThread]: Command end result
[0m23:14:12.692034 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:14:12.698205 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:14:12.711494 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:14:12.712689 [info ] [MainThread]: 
[0m23:14:12.714049 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:14:12.716078 [info ] [MainThread]: 
[0m23:14:12.718248 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m23:14:12.720189 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mcustomer_id[0m[33m,[0m[0;1maccount_id[0m[33m,[0m[0;1moriginal_balance[0m[33m,[0m[0;1minterest_rate[0m[33m,[0m[0;1mannual_interest_amount[0m[33m,[0m[0;1mnew_balance[0m
[32;1m+++[0m[33m,[0m[32;1m203[0m        [33m,[0m[32;1mUT003[0m     [33m,[0m[32;1mDecimal('20000.00')[0m        [33m,[0m[32;1mDecimal('0.020')[0m        [33m,[0m[32;1mDecimal('400.00000')[0m             [33m,[0m[32;1mDecimal('20400.00000')[0m


[0m23:14:12.723570 [info ] [MainThread]: 
[0m23:14:12.726066 [info ] [MainThread]:   compiled code at target/compiled/banking_pipeline/tests/unit/test_interest_calculation.yml/tests/unit/non_savings_account_filtered_out.sql
[0m23:14:12.728483 [info ] [MainThread]: 
[0m23:14:12.731891 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m23:14:12.735741 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 23.729952, "process_in_blocks": "0", "process_kernel_time": 0.648501, "process_mem_max_rss": "255956", "process_out_blocks": "808", "process_user_time": 6.961386}
[0m23:14:12.737714 [debug] [MainThread]: Command `dbt test` failed at 23:14:12.737415 after 23.73 seconds
[0m23:14:12.739340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ff9ee6530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fc02729b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fc02736a0>]}
[0m23:14:12.740944 [debug] [MainThread]: Flushing usage events
[0m23:14:13.169639 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:33:43.965524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33b6be4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33a4f4280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33a4f4220>]}


============================== 23:33:43.972868 | a264fcc3-75a5-488f-a64b-d36b8ddeb8ed ==============================
[0m23:33:43.972868 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:33:43.974623 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'printer_width': '80', 'use_colors': 'True', 'version_check': 'True', 'quiet': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'no_print': 'None', 'log_format': 'default', 'empty': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'log_path': '/opt/dagster/app/dbt/logs', 'fail_fast': 'False', 'static_parser': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'log_cache_events': 'False'}
[0m23:33:44.907626 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:33:44.909684 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:33:44.911358 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:33:46.019784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33d0fda20>]}
[0m23:33:46.138425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d3384d9090>]}
[0m23:33:46.140642 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:33:46.332863 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:33:46.854468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:33:46.857000 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://models/marts/account_interest_summary.sql
[0m23:33:47.226911 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m23:33:47.526403 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m23:33:47.531996 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m23:33:47.701649 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:33:47.732176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d31316d570>]}
[0m23:33:47.989184 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:33:47.998111 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:33:48.140766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d3185d2350>]}
[0m23:33:48.142436 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:33:48.145911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d3185d3790>]}
[0m23:33:48.150255 [info ] [MainThread]: 
[0m23:33:48.153599 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:33:48.155251 [info ] [MainThread]: 
[0m23:33:48.156880 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:33:48.158369 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:33:48.173572 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:33:48.175492 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:33:48.194296 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:33:48.196454 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:33:48.198366 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:33:48.906806 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-7b62-1738-b45f-8561d827db97) - Created
[0m23:34:07.010815 [debug] [ThreadPool]: SQL status: OK in 18.810 seconds
[0m23:34:07.022862 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecea-7b62-1738-b45f-8561d827db97, command-id=01f0ecea-7b87-14d7-8a65-a7ba99964dda) - Closing
[0m23:34:07.391834 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:34:07.393213 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-7b62-1738-b45f-8561d827db97) - Closing
[0m23:34:07.527261 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:34:07.531886 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:34:07.541245 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:34:07.543334 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:34:07.546014 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:34:07.961259 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-86ce-10ff-81a4-f5803cca0d3f) - Created
[0m23:34:10.585599 [debug] [ThreadPool]: SQL status: OK in 3.040 seconds
[0m23:34:10.602317 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecea-86ce-10ff-81a4-f5803cca0d3f, command-id=01f0ecea-86e1-18d2-b985-052eae3e5448) - Closing
[0m23:34:10.605728 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:34:10.617834 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-86ce-10ff-81a4-f5803cca0d3f) - Closing
[0m23:34:10.778582 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:34:10.781199 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:34:10.791364 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:34:10.794182 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:34:10.796590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:34:11.299128 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-88ce-1331-8241-1994b5ed1e61) - Created
[0m23:34:12.749516 [debug] [ThreadPool]: SQL status: OK in 1.950 seconds
[0m23:34:12.764542 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecea-88ce-1331-8241-1994b5ed1e61, command-id=01f0ecea-88df-1a20-88a6-787fe34b5b20) - Closing
[0m23:34:12.769716 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:34:12.771667 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecea-88ce-1331-8241-1994b5ed1e61) - Closing
[0m23:34:12.915161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a264fcc3-75a5-488f-a64b-d36b8ddeb8ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d3185d0c40>]}
[0m23:34:12.929938 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:34:12.939370 [info ] [Thread-4 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m23:34:12.943231 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m23:34:12.947283 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m23:34:12.952103 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:34:12.997917 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:34:13.024160 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:34:13.093033 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:34:13.115455 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:34:13.118930 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m23:34:13.121833 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:13.607114 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8a2a-1f63-9ea3-eed057248863) - Created
[0m23:34:18.096312 [debug] [Thread-4 (]: SQL status: OK in 4.970 seconds
[0m23:34:18.100707 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-8a2a-1f63-9ea3-eed057248863, command-id=01f0ecea-8a3f-12be-8406-c043efa335e5) - Closing
[0m23:34:18.107166 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m23:34:18.109062 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8a2a-1f63-9ea3-eed057248863) - Closing
[0m23:34:18.247465 [info ] [Thread-4 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 5.30s]
[0m23:34:18.252490 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:34:18.258523 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:34:18.263341 [info ] [Thread-4 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m23:34:18.269739 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m23:34:18.273543 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m23:34:18.276410 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:34:18.288472 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:34:18.302689 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:34:18.307043 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:34:18.317671 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:34:18.319004 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m23:34:18.320168 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:18.697771 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8d37-14d6-886e-02baf7cfe5a8) - Created
[0m23:34:19.515675 [debug] [Thread-4 (]: SQL status: OK in 1.190 seconds
[0m23:34:19.530978 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-8d37-14d6-886e-02baf7cfe5a8, command-id=01f0ecea-8d47-1f65-8af6-533ed28a8178) - Closing
[0m23:34:19.539287 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m23:34:19.545827 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8d37-14d6-886e-02baf7cfe5a8) - Closing
[0m23:34:19.706110 [info ] [Thread-4 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.43s]
[0m23:34:19.715077 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:34:19.722226 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:34:19.730322 [info ] [Thread-4 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m23:34:19.739515 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m23:34:19.747320 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m23:34:19.754452 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:34:19.793507 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:34:19.816941 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:34:19.821766 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:34:19.834401 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:34:19.835524 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m23:34:19.836793 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:20.237266 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8e22-1f9c-afb9-a131b241c04f) - Created
[0m23:34:22.046547 [debug] [Thread-4 (]: SQL status: OK in 2.210 seconds
[0m23:34:22.050910 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-8e22-1f9c-afb9-a131b241c04f, command-id=01f0ecea-8e32-1d60-8574-3e1ba26027e8) - Closing
[0m23:34:22.053389 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m23:34:22.055326 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8e22-1f9c-afb9-a131b241c04f) - Closing
[0m23:34:22.183339 [info ] [Thread-4 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 2.45s]
[0m23:34:22.186205 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:34:22.188748 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:34:22.192549 [info ] [Thread-4 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m23:34:22.195877 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m23:34:22.198134 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m23:34:22.200179 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:34:22.219203 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:34:22.235565 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:34:22.241171 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:34:22.254557 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:34:22.256221 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m23:34:22.257803 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:22.710410 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8f99-14f9-8e65-c4a71c5b5bc9) - Created
[0m23:34:23.468882 [debug] [Thread-4 (]: SQL status: OK in 1.210 seconds
[0m23:34:23.473470 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-8f99-14f9-8e65-c4a71c5b5bc9, command-id=01f0ecea-8fab-1eca-ac9f-114719baa332) - Closing
[0m23:34:23.475936 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m23:34:23.477732 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-8f99-14f9-8e65-c4a71c5b5bc9) - Closing
[0m23:34:23.607373 [info ] [Thread-4 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.41s]
[0m23:34:23.609855 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:34:23.611947 [debug] [Thread-4 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:34:23.614161 [info ] [Thread-4 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m23:34:23.617406 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m23:34:23.619088 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m23:34:23.620671 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:34:23.632837 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:34:23.649444 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:34:23.654785 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:34:23.669261 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:34:23.670585 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:34:23.671704 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:24.042177 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9066-199b-ad73-a0dc8b4a5d7a) - Created
[0m23:34:25.479827 [debug] [Thread-4 (]: SQL status: OK in 1.810 seconds
[0m23:34:25.489482 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-9066-199b-ad73-a0dc8b4a5d7a, command-id=01f0ecea-9076-1ea3-9711-cf4aab319131) - Closing
[0m23:34:25.493409 [debug] [Thread-4 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m23:34:25.496206 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9066-199b-ad73-a0dc8b4a5d7a) - Closing
[0m23:34:25.637689 [info ] [Thread-4 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.02s]
[0m23:34:25.639972 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:34:25.642115 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:34:25.644129 [info ] [Thread-4 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m23:34:25.647069 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m23:34:25.648634 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m23:34:25.650088 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:34:25.660271 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:34:25.673816 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:34:25.679327 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:34:25.693613 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:34:25.694988 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m23:34:25.696252 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:26.096671 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-919f-189e-a008-5ac6e838c78c) - Created
[0m23:34:26.769473 [debug] [Thread-4 (]: SQL status: OK in 1.070 seconds
[0m23:34:26.774102 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-919f-189e-a008-5ac6e838c78c, command-id=01f0ecea-91b2-10ac-b19b-9dc0f8326efd) - Closing
[0m23:34:26.776763 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m23:34:26.778875 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-919f-189e-a008-5ac6e838c78c) - Closing
[0m23:34:26.928350 [info ] [Thread-4 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.28s]
[0m23:34:26.935144 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:34:26.939313 [debug] [Thread-4 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:34:26.943321 [info ] [Thread-4 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m23:34:26.946355 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m23:34:26.948679 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m23:34:26.951154 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:34:26.962232 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:34:26.975294 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:34:26.979296 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:34:26.992778 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:34:26.994013 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m23:34:26.995013 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:27.362195 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9260-1de3-9853-141855b2e3e8) - Created
[0m23:34:28.103508 [debug] [Thread-4 (]: SQL status: OK in 1.110 seconds
[0m23:34:28.111869 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-9260-1de3-9853-141855b2e3e8, command-id=01f0ecea-9271-183f-a5c8-c85c7993fd68) - Closing
[0m23:34:28.115964 [debug] [Thread-4 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m23:34:28.120573 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9260-1de3-9853-141855b2e3e8) - Closing
[0m23:34:28.247455 [info ] [Thread-4 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.30s]
[0m23:34:28.249897 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:34:28.251889 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:34:28.254240 [info ] [Thread-4 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m23:34:28.257694 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m23:34:28.259533 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m23:34:28.261196 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:34:28.275467 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:34:28.290754 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:34:28.297789 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:34:28.310139 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:34:28.311383 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:34:28.312436 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:28.688714 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-932a-1b7b-944f-4f77aba655d5) - Created
[0m23:34:29.449462 [debug] [Thread-4 (]: SQL status: OK in 1.140 seconds
[0m23:34:29.453379 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-932a-1b7b-944f-4f77aba655d5, command-id=01f0ecea-933c-18ca-8e7a-5c4f975fe0cb) - Closing
[0m23:34:29.455475 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m23:34:29.457330 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-932a-1b7b-944f-4f77aba655d5) - Closing
[0m23:34:29.609484 [info ] [Thread-4 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.35s]
[0m23:34:29.617563 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:34:29.624665 [debug] [Thread-4 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:34:29.632734 [info ] [Thread-4 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m23:34:29.641538 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m23:34:29.648318 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m23:34:29.656052 [debug] [Thread-4 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:34:29.686262 [debug] [Thread-4 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:34:29.702947 [debug] [Thread-4 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:34:29.707626 [debug] [Thread-4 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:34:29.720600 [debug] [Thread-4 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:34:29.722095 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:34:29.723550 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:30.117851 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9404-1753-8ad0-04db097c0a1f) - Created
[0m23:34:30.933052 [debug] [Thread-4 (]: SQL status: OK in 1.210 seconds
[0m23:34:30.936651 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0ecea-9404-1753-8ad0-04db097c0a1f, command-id=01f0ecea-9417-1479-85d4-6cffb24d2c8d) - Closing
[0m23:34:30.938623 [debug] [Thread-4 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m23:34:30.940133 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9404-1753-8ad0-04db097c0a1f) - Closing
[0m23:34:31.056454 [info ] [Thread-4 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.42s]
[0m23:34:31.058353 [debug] [Thread-4 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:34:31.060304 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:34:31.062282 [info ] [Thread-4 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m23:34:31.065840 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m23:34:31.067657 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m23:34:31.069107 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:34:31.070683 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:34:31.127553 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:34:31.129805 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:34:31.131888 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:31.520876 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-94dc-1244-a565-96d3b13f9bab) - Created
[0m23:34:31.991055 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0ecea-94ed-123e-98a4-0068dd4e0888
[0m23:34:31.993549 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_comments_as_json
: Database Error
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
[0m23:34:31.996809 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m23:34:31.998730 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-94dc-1244-a565-96d3b13f9bab) - Closing
[0m23:34:32.126822 [debug] [Thread-4 (]: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:32.128586 [error] [Thread-4 (]: 10 of 12 ERROR account_interest_summary::interest_rate_with_loan_mid_balance ... [[31mERROR[0m in 1.06s]
[0m23:34:32.130378 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:34:32.132459 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:34:32.133518 [debug] [Thread-7 (]: Marking all children of 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance' to be skipped because of status 'error'.  Reason: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml).
[0m23:34:32.135286 [info ] [Thread-4 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m23:34:32.142679 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m23:34:32.144107 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m23:34:32.145837 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:34:32.148256 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:34:32.162271 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:34:32.164171 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:34:32.165658 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:32.579012 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-957b-1161-8dda-ad8137de646f) - Created
[0m23:34:32.896488 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0ecea-958e-109f-a205-bbe9353ae57f
[0m23:34:32.900459 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_comments_as_json
: Database Error
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
[0m23:34:32.904487 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m23:34:32.907491 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-957b-1161-8dda-ad8137de646f) - Closing
[0m23:34:33.044167 [debug] [Thread-4 (]: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:33.048034 [error] [Thread-4 (]: 11 of 12 ERROR account_interest_summary::interest_rate_without_loan_low_balance  [[31mERROR[0m in 0.91s]
[0m23:34:33.052679 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:34:33.057201 [debug] [Thread-4 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:34:33.059764 [debug] [Thread-7 (]: Marking all children of 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance' to be skipped because of status 'error'.  Reason: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml).
[0m23:34:33.062511 [info ] [Thread-4 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m23:34:33.067787 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m23:34:33.070340 [debug] [Thread-4 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m23:34:33.073184 [debug] [Thread-4 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:34:33.075134 [debug] [Thread-4 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:34:33.095131 [debug] [Thread-4 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:34:33.096879 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
[0m23:34:33.098718 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m23:34:33.508307 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9609-1234-8be4-e682e38c93b3) - Created
[0m23:34:33.817659 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`raw`.`int_customer_accounts` AS JSON

  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0ecea-961c-1a34-b66a-fecb50fbc6a6
[0m23:34:33.819930 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_comments_as_json
: Database Error
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`int_customer_accounts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 4 pos 26
[0m23:34:33.822412 [debug] [Thread-4 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m23:34:33.824090 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0ecea-9609-1234-8be4-e682e38c93b3) - Closing
[0m23:34:33.971244 [debug] [Thread-4 (]: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:33.973578 [error] [Thread-4 (]: 12 of 12 ERROR account_interest_summary::non_savings_account_filtered_out ...... [[31mERROR[0m in 0.91s]
[0m23:34:33.976009 [debug] [Thread-4 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:34:33.978361 [debug] [Thread-7 (]: Marking all children of 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out' to be skipped because of status 'error'.  Reason: Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml).
[0m23:34:33.982759 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:34:33.984518 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:34:33.986546 [info ] [MainThread]: 
[0m23:34:33.988216 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 45.83 seconds (45.83s).
[0m23:34:33.992604 [debug] [MainThread]: Command end result
[0m23:34:34.119349 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:34:34.127348 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:34:34.145887 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:34:34.147671 [info ] [MainThread]: 
[0m23:34:34.154051 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m23:34:34.157516 [info ] [MainThread]: 
[0m23:34:34.160443 [error] [MainThread]: [31mFailure in unit_test interest_rate_with_loan_mid_balance (tests/unit/test_interest_calculation.yml)[0m
[0m23:34:34.163496 [error] [MainThread]:   Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:34.168304 [info ] [MainThread]: 
[0m23:34:34.171899 [error] [MainThread]: [31mFailure in unit_test interest_rate_without_loan_low_balance (tests/unit/test_interest_calculation.yml)[0m
[0m23:34:34.174255 [error] [MainThread]:   Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:34.176079 [info ] [MainThread]: 
[0m23:34:34.178070 [error] [MainThread]: [31mFailure in unit_test non_savings_account_filtered_out (tests/unit/test_interest_calculation.yml)[0m
[0m23:34:34.181022 [error] [MainThread]:   Compilation Error in model int_customer_accounts (tests/unit/test_interest_calculation.yml)
  Not able to get columns for unit test 'int_customer_accounts' from relation `workspace`.`raw`.`int_customer_accounts` because the relation doesn't exist
  
  > in macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model int_customer_accounts (tests/unit/test_interest_calculation.yml)
[0m23:34:34.183423 [info ] [MainThread]: 
[0m23:34:34.185653 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=12
[0m23:34:34.189491 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 50.348103, "process_in_blocks": "0", "process_kernel_time": 0.735664, "process_mem_max_rss": "258972", "process_out_blocks": "808", "process_user_time": 7.86999}
[0m23:34:34.191312 [debug] [MainThread]: Command `dbt test` failed at 23:34:34.191006 after 50.35 seconds
[0m23:34:34.192811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33b6be4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33d0fda20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76d33b5a0c70>]}
[0m23:34:34.194873 [debug] [MainThread]: Flushing usage events
[0m23:34:34.622128 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:44:58.552956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1b1c22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc19f902b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc19f90250>]}


============================== 23:44:58.557589 | 00017ba8-af48-4784-8d09-bbdccc3ebb3d ==============================
[0m23:44:58.557589 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:44:58.559332 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'log_path': '/opt/dagster/app/dbt/logs', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'profiles_dir': '/opt/dagster/app/dbt', 'cache_selected_only': 'False', 'write_json': 'True', 'printer_width': '80', 'version_check': 'True', 'debug': 'False', 'warn_error': 'None', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None'}
[0m23:44:59.576373 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:44:59.578767 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:44:59.580812 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:45:00.707547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edbf8bfad40>]}
[0m23:45:00.838972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc19f6d6c0>]}
[0m23:45:00.841000 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:45:01.050876 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:45:01.525707 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:45:01.527802 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:45:01.541208 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:45:01.637047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1a683e50>]}
[0m23:45:01.882033 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:45:01.890133 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:45:01.912871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1a6a3be0>]}
[0m23:45:01.914520 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:45:01.915845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1a6a11e0>]}
[0m23:45:01.919396 [info ] [MainThread]: 
[0m23:45:01.921422 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:45:01.923493 [info ] [MainThread]: 
[0m23:45:01.926144 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:45:01.927726 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:01.940940 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:45:01.942702 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:45:01.961770 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:45:01.963763 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:45:01.965468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:02.663124 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-0cf9-1977-bce0-816648ca8041) - Created
[0m23:45:24.367375 [debug] [ThreadPool]: SQL status: OK in 22.400 seconds
[0m23:45:24.373466 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecec-0cf9-1977-bce0-816648ca8041, command-id=01f0ecec-0d1e-1608-b818-250223809e4c) - Closing
[0m23:45:24.584665 [debug] [ThreadPool]: On list_workspace: Close
[0m23:45:24.591028 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-0cf9-1977-bce0-816648ca8041) - Closing
[0m23:45:24.722254 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:45:24.723918 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:45:24.734339 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:45:24.736083 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:45:24.737649 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:25.225437 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1a80-10b8-9a1d-ea6fdea619b8) - Created
[0m23:45:29.443702 [debug] [ThreadPool]: SQL status: OK in 4.710 seconds
[0m23:45:29.507033 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecec-1a80-10b8-9a1d-ea6fdea619b8, command-id=01f0ecec-1a91-170b-9958-731482a58810) - Closing
[0m23:45:29.527945 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:45:29.530205 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1a80-10b8-9a1d-ea6fdea619b8) - Closing
[0m23:45:29.660488 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:45:29.662612 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:45:29.679479 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:45:29.681011 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:45:29.682517 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:30.113864 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1d6a-11fc-81e6-2c4568614b32) - Created
[0m23:45:30.857673 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m23:45:30.862619 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecec-1d6a-11fc-81e6-2c4568614b32, command-id=01f0ecec-1d7c-1247-988c-32b4f93a14ca) - Closing
[0m23:45:30.864493 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:45:30.865774 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1d6a-11fc-81e6-2c4568614b32) - Closing
[0m23:45:30.981797 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:45:30.983828 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:45:30.991823 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:45:30.994132 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:45:30.996062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:31.393360 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1e2a-1edd-8d17-6087bb608457) - Created
[0m23:45:32.018723 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m23:45:32.021862 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0ecec-1e2a-1edd-8d17-6087bb608457, command-id=01f0ecec-1e3f-197c-a9a5-91ba35547aba) - Closing
[0m23:45:32.023410 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:45:32.024818 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0ecec-1e2a-1edd-8d17-6087bb608457) - Closing
[0m23:45:32.154787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1cce1210>]}
[0m23:45:32.162792 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.accounts
[0m23:45:32.165651 [info ] [Thread-5 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m23:45:32.169318 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m23:45:32.172306 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m23:45:32.175783 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.accounts
[0m23:45:32.178468 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.accounts
[0m23:45:32.191695 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:45:32.193699 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edbf2739c30>]}
[0m23:45:32.249325 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:45:32.251829 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edbf2758730>]}
[0m23:45:32.287148 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:45:32.289231 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:45:32.290771 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:45:32.694184 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecec-1ef4-11ae-9493-344eba1de8f7) - Created
[0m23:45:38.917651 [debug] [Thread-5 (]: SQL status: OK in 6.630 seconds
[0m23:45:38.920039 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecec-1ef4-11ae-9493-344eba1de8f7, command-id=01f0ecec-1f04-1f15-b392-42027b0b2b6d) - Closing
[0m23:45:39.105775 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:45:39.107867 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m23:45:45.016274 [debug] [Thread-5 (]: SQL status: OK in 5.910 seconds
[0m23:45:45.018259 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecec-1ef4-11ae-9493-344eba1de8f7, command-id=01f0ecec-22d7-160e-8268-8f3e441d5cd8) - Closing
[0m23:45:45.187973 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m23:45:45.215081 [debug] [Thread-5 (]: On seed.banking_pipeline.accounts: Close
[0m23:45:45.217190 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecec-1ef4-11ae-9493-344eba1de8f7) - Closing
[0m23:45:45.345732 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1cc89b10>]}
[0m23:45:45.348117 [info ] [Thread-5 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 13.17s]
[0m23:45:45.350903 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.accounts
[0m23:45:45.353562 [debug] [Thread-5 (]: Began running node seed.banking_pipeline.customers
[0m23:45:45.356693 [info ] [Thread-5 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m23:45:45.360011 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m23:45:45.362787 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m23:45:45.364797 [debug] [Thread-5 (]: Began compiling node seed.banking_pipeline.customers
[0m23:45:45.368127 [debug] [Thread-5 (]: Began executing node seed.banking_pipeline.customers
[0m23:45:45.393258 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:45:45.395600 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:45:45.397455 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:45:45.775606 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecec-26c0-1387-a70f-0dc69bcc6821) - Created
[0m23:45:48.132928 [debug] [Thread-5 (]: SQL status: OK in 2.730 seconds
[0m23:45:48.136657 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecec-26c0-1387-a70f-0dc69bcc6821, command-id=01f0ecec-26d0-1b4d-be0f-122d6989d589) - Closing
[0m23:45:48.143458 [debug] [Thread-5 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:45:48.145331 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m23:45:50.599944 [debug] [Thread-5 (]: SQL status: OK in 2.450 seconds
[0m23:45:50.601563 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0ecec-26c0-1387-a70f-0dc69bcc6821, command-id=01f0ecec-2839-1f8c-a53c-c476d19381bc) - Closing
[0m23:45:50.603798 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m23:45:50.618651 [debug] [Thread-5 (]: On seed.banking_pipeline.customers: Close
[0m23:45:50.619994 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0ecec-26c0-1387-a70f-0dc69bcc6821) - Closing
[0m23:45:50.763849 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '00017ba8-af48-4784-8d09-bbdccc3ebb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1b463070>]}
[0m23:45:50.767234 [info ] [Thread-5 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 5.40s]
[0m23:45:50.770275 [debug] [Thread-5 (]: Finished running node seed.banking_pipeline.customers
[0m23:45:50.777055 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:45:50.779433 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:50.781881 [info ] [MainThread]: 
[0m23:45:50.783829 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 48.86 seconds (48.86s).
[0m23:45:50.786425 [debug] [MainThread]: Command end result
[0m23:45:50.892465 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:45:50.905666 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:45:50.923379 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:45:50.924777 [info ] [MainThread]: 
[0m23:45:50.926594 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:45:50.928226 [info ] [MainThread]: 
[0m23:45:50.930052 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:45:50.934002 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 52.5247, "process_in_blocks": "0", "process_kernel_time": 0.637123, "process_mem_max_rss": "246220", "process_out_blocks": "0", "process_user_time": 6.33517}
[0m23:45:50.936207 [debug] [MainThread]: Command `dbt seed` succeeded at 23:45:50.935950 after 52.53 seconds
[0m23:45:50.938111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1b1c22f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1cc8d030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7edc1b0c0e50>]}
[0m23:45:50.940932 [debug] [MainThread]: Flushing usage events
[0m23:45:51.372161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:51:50.362527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de90f23e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de7f18280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de7f18220>]}


============================== 23:51:50.370321 | 2f33cd27-eb69-45a8-8011-c23456d88bdc ==============================
[0m23:51:50.370321 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:51:50.372070 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/dagster/app/dbt/logs', 'debug': 'False', 'printer_width': '80', 'static_parser': 'True', 'write_json': 'True', 'indirect_selection': 'eager', 'quiet': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'version_check': 'True', 'log_format': 'default', 'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'empty': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'introspect': 'True'}
[0m23:51:51.204347 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:51:51.205950 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:51:51.207672 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:51:52.221810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de5efdf30>]}
[0m23:51:52.386084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de7ee49a0>]}
[0m23:51:52.389340 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:51:52.584087 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:51:52.964912 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:51:52.966982 [debug] [MainThread]: Partial parsing: updated file: banking_pipeline://models/intermidiate/int_customer_accounts.sql
[0m23:51:53.271391 [info ] [MainThread]: USING CUSTOM SCHEMA MACRO
[0m23:51:53.437029 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:51:53.458462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc1fa6830>]}
[0m23:51:53.686976 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:51:53.695203 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:51:53.720156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc0e7a080>]}
[0m23:51:53.721624 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:51:53.722770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc0e7b220>]}
[0m23:51:53.726189 [info ] [MainThread]: 
[0m23:51:53.728191 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:51:53.729988 [info ] [MainThread]: 
[0m23:51:53.732044 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:51:53.733312 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:51:53.745373 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:51:53.747003 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:51:53.761049 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:51:53.762668 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:51:53.764091 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:54.260818 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-025d-11fe-96e9-22c545ceac90) - Created
[0m23:51:54.806857 [debug] [ThreadPool]: SQL status: OK in 1.040 seconds
[0m23:51:54.811913 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-025d-11fe-96e9-22c545ceac90, command-id=01f0eced-0274-1c2d-be90-3444ffe2108f) - Closing
[0m23:51:54.813742 [debug] [ThreadPool]: On list_workspace: Close
[0m23:51:54.815154 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-025d-11fe-96e9-22c545ceac90) - Closing
[0m23:51:54.958024 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:51:54.959656 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:51:54.969208 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:51:54.971078 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:51:54.972558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:55.347192 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-0306-15ee-a9ea-905f5e9f0f24) - Created
[0m23:51:55.874715 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m23:51:55.879392 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-0306-15ee-a9ea-905f5e9f0f24, command-id=01f0eced-0319-1a70-89e7-c02475ff39a4) - Closing
[0m23:51:55.881346 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:51:55.882813 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-0306-15ee-a9ea-905f5e9f0f24) - Closing
[0m23:51:55.996131 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:51:55.997707 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:51:56.005950 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:51:56.007377 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:51:56.008685 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:56.465534 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-03ae-1fc6-b5a0-984d86df968e) - Created
[0m23:51:57.001321 [debug] [ThreadPool]: SQL status: OK in 0.990 seconds
[0m23:51:57.008479 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-03ae-1fc6-b5a0-984d86df968e, command-id=01f0eced-03c4-1f53-b472-8627ee9204b3) - Closing
[0m23:51:57.011396 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:51:57.013712 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-03ae-1fc6-b5a0-984d86df968e) - Closing
[0m23:51:57.162033 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:51:57.168112 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:51:57.179042 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:51:57.181579 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:51:57.183492 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:57.596913 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-045d-1d7d-8929-accd2f6bc8a7) - Created
[0m23:51:58.029859 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m23:51:58.033773 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-045d-1d7d-8929-accd2f6bc8a7, command-id=01f0eced-0471-151e-b2aa-10b7436a2d97) - Closing
[0m23:51:58.035930 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:51:58.037731 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-045d-1d7d-8929-accd2f6bc8a7) - Closing
[0m23:51:58.166756 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:51:58.172477 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:51:58.191564 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:51:58.193730 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:51:58.195550 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:58.617100 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-04f7-1b56-9d41-5f6588bb318c) - Created
[0m23:51:59.041750 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m23:51:59.045138 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-04f7-1b56-9d41-5f6588bb318c, command-id=01f0eced-050c-1bf8-947c-e05462007f5e) - Closing
[0m23:51:59.046674 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:51:59.048106 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-04f7-1b56-9d41-5f6588bb318c) - Closing
[0m23:51:59.187787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716deac69210>]}
[0m23:51:59.193972 [debug] [Thread-6 (]: Began running node seed.banking_pipeline.accounts
[0m23:51:59.195695 [info ] [Thread-6 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m23:51:59.197404 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m23:51:59.199177 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m23:51:59.201178 [debug] [Thread-6 (]: Began compiling node seed.banking_pipeline.accounts
[0m23:51:59.203072 [debug] [Thread-6 (]: Began executing node seed.banking_pipeline.accounts
[0m23:51:59.210212 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:51:59.211828 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc0d479a0>]}
[0m23:51:59.260078 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:51:59.262120 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de86085b0>]}
[0m23:51:59.289298 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:51:59.291811 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:51:59.294689 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m23:51:59.690039 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-059f-18bb-9417-25842de3d59c) - Created
[0m23:52:01.481008 [debug] [Thread-6 (]: SQL status: OK in 2.190 seconds
[0m23:52:01.483382 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-059f-18bb-9417-25842de3d59c, command-id=01f0eced-05b1-18e5-abde-4b21c19d12d6) - Closing
[0m23:52:01.502614 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:52:01.504570 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m23:52:03.773363 [debug] [Thread-6 (]: SQL status: OK in 2.270 seconds
[0m23:52:03.775446 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-059f-18bb-9417-25842de3d59c, command-id=01f0eced-06c5-1a23-bcf8-a43956b645b3) - Closing
[0m23:52:03.783520 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m23:52:03.807423 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: Close
[0m23:52:03.809256 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-059f-18bb-9417-25842de3d59c) - Closing
[0m23:52:03.949845 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc0e1fa00>]}
[0m23:52:03.951908 [info ] [Thread-6 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 4.75s]
[0m23:52:03.954116 [debug] [Thread-6 (]: Finished running node seed.banking_pipeline.accounts
[0m23:52:03.955976 [debug] [Thread-6 (]: Began running node seed.banking_pipeline.customers
[0m23:52:03.958102 [info ] [Thread-6 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m23:52:03.961478 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m23:52:03.963161 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m23:52:03.964638 [debug] [Thread-6 (]: Began compiling node seed.banking_pipeline.customers
[0m23:52:03.966060 [debug] [Thread-6 (]: Began executing node seed.banking_pipeline.customers
[0m23:52:03.987718 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:52:03.989295 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:52:03.990958 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m23:52:04.366516 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-0867-19e7-be46-79fee4c40a47) - Created
[0m23:52:05.968968 [debug] [Thread-6 (]: SQL status: OK in 1.980 seconds
[0m23:52:05.971425 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-0867-19e7-be46-79fee4c40a47, command-id=01f0eced-0879-1e42-8350-a4648e01cd67) - Closing
[0m23:52:05.975261 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:52:05.976771 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m23:52:07.975075 [debug] [Thread-6 (]: SQL status: OK in 2.000 seconds
[0m23:52:07.977094 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-0867-19e7-be46-79fee4c40a47, command-id=01f0eced-096f-1a29-b1e6-6165f6ed476e) - Closing
[0m23:52:07.979374 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m23:52:07.993478 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: Close
[0m23:52:07.994939 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-0867-19e7-be46-79fee4c40a47) - Closing
[0m23:52:08.128280 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f33cd27-eb69-45a8-8011-c23456d88bdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de93f0dc0>]}
[0m23:52:08.134251 [info ] [Thread-6 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 4.17s]
[0m23:52:08.139291 [debug] [Thread-6 (]: Finished running node seed.banking_pipeline.customers
[0m23:52:08.146220 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:52:08.148736 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:52:08.150996 [info ] [MainThread]: 
[0m23:52:08.153265 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 14.42 seconds (14.42s).
[0m23:52:08.156330 [debug] [MainThread]: Command end result
[0m23:52:08.248374 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:52:08.256699 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:52:08.272415 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:52:08.273812 [info ] [MainThread]: 
[0m23:52:08.275343 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:52:08.276693 [info ] [MainThread]: 
[0m23:52:08.278010 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:52:08.280213 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 18.03632, "process_in_blocks": "0", "process_kernel_time": 0.615895, "process_mem_max_rss": "255340", "process_out_blocks": "808", "process_user_time": 5.934996}
[0m23:52:08.281941 [debug] [MainThread]: Command `dbt seed` succeeded at 23:52:08.281814 after 18.04 seconds
[0m23:52:08.283451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de90f23e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dc2079f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716de7ee49a0>]}
[0m23:52:08.284930 [debug] [MainThread]: Flushing usage events
[0m23:52:08.795051 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:52:20.827193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731f73a320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731e5082b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731e508250>]}


============================== 23:52:20.831937 | ebf4a2d1-8d01-421e-9fc4-ffcea20a551d ==============================
[0m23:52:20.831937 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:52:20.833523 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'use_experimental_parser': 'False', 'log_path': '/opt/dagster/app/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'version_check': 'True', 'profiles_dir': '/opt/dagster/app/dbt', 'write_json': 'True', 'target_path': 'None', 'debug': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'static_parser': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True'}
[0m23:52:21.698780 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:52:21.700849 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:52:21.702526 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:52:22.857897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fd181150>]}
[0m23:52:22.997320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731ef1a5f0>]}
[0m23:52:22.999308 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:52:23.186925 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:52:23.645345 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:52:23.646852 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:52:23.659975 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:52:23.754350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731ec28a30>]}
[0m23:52:23.993578 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:52:24.001960 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:52:24.019367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fc5c1360>]}
[0m23:52:24.020909 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:52:24.022405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fc5c1ba0>]}
[0m23:52:24.025882 [info ] [MainThread]: 
[0m23:52:24.027752 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:52:24.029918 [info ] [MainThread]: 
[0m23:52:24.031849 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:52:24.033222 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:52:24.046304 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:52:24.047906 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:52:24.064845 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:52:24.066828 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:52:24.068802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:24.455271 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-1462-1121-8cec-8fee65a548e2) - Created
[0m23:52:24.756095 [debug] [ThreadPool]: SQL status: OK in 0.690 seconds
[0m23:52:24.761425 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-1462-1121-8cec-8fee65a548e2, command-id=01f0eced-1472-1c06-8f88-25fd3228a0b2) - Closing
[0m23:52:24.763457 [debug] [ThreadPool]: On list_workspace: Close
[0m23:52:24.765052 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-1462-1121-8cec-8fee65a548e2) - Closing
[0m23:52:24.911772 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:52:24.913286 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:52:24.917769 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:52:24.919369 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:52:24.920993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:25.307110 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-14e0-18c9-83c6-1d98a082bf81) - Created
[0m23:52:25.630534 [debug] [ThreadPool]: SQL status: OK in 0.710 seconds
[0m23:52:25.634384 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-14e0-18c9-83c6-1d98a082bf81, command-id=01f0eced-14f4-17c4-b74a-ba3f04b64aba) - Closing
[0m23:52:25.636448 [debug] [ThreadPool]: On list_workspace: Close
[0m23:52:25.638238 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-14e0-18c9-83c6-1d98a082bf81) - Closing
[0m23:52:25.776192 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:52:25.778246 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:52:25.790977 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:52:25.792503 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:52:25.794077 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:26.194630 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-156a-1f36-8b1f-662218744650) - Created
[0m23:52:26.536229 [debug] [ThreadPool]: SQL status: OK in 0.740 seconds
[0m23:52:26.539847 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-156a-1f36-8b1f-662218744650, command-id=01f0eced-157c-1ae2-9082-b94989724b75) - Closing
[0m23:52:26.541936 [debug] [ThreadPool]: On list_workspace: Close
[0m23:52:26.543633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-156a-1f36-8b1f-662218744650) - Closing
[0m23:52:26.683043 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:52:26.685060 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:52:26.699440 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:52:26.701412 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:52:26.703122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:27.113126 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-15f6-1648-aee5-5695a3d4fa0d) - Created
[0m23:52:27.600414 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m23:52:27.606059 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-15f6-1648-aee5-5695a3d4fa0d, command-id=01f0eced-1609-1e26-9070-f717c2bd94a3) - Closing
[0m23:52:27.608613 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:52:27.610594 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-15f6-1648-aee5-5695a3d4fa0d) - Closing
[0m23:52:27.729544 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:52:27.732045 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:52:27.738293 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:52:27.740281 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:52:27.741745 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:28.171609 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-1698-1b0e-8412-6a4fc57ec255) - Created
[0m23:52:28.654847 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m23:52:28.658726 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-1698-1b0e-8412-6a4fc57ec255, command-id=01f0eced-16aa-1674-a50e-1c6061b3eee6) - Closing
[0m23:52:28.660886 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:52:28.662617 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-1698-1b0e-8412-6a4fc57ec255) - Closing
[0m23:52:28.838960 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:52:28.840531 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:52:28.846385 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:52:28.847908 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:52:28.849162 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:29.255721 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-173e-103e-be83-2d27be2f32ff) - Created
[0m23:52:29.717299 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m23:52:29.720826 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-173e-103e-be83-2d27be2f32ff, command-id=01f0eced-1750-17bf-b1e7-c936b97c6bbe) - Closing
[0m23:52:29.722676 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:52:29.724152 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-173e-103e-be83-2d27be2f32ff) - Closing
[0m23:52:29.855621 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:52:29.859473 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:52:29.873894 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:52:29.876591 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:52:29.879119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:30.302405 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-17dd-10b9-b3d7-2784743e60b3) - Created
[0m23:52:30.779843 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m23:52:30.784495 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-17dd-10b9-b3d7-2784743e60b3, command-id=01f0eced-17f0-13a8-a374-b2202c45ffe3) - Closing
[0m23:52:30.787113 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:52:30.789207 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-17dd-10b9-b3d7-2784743e60b3) - Closing
[0m23:52:30.932005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731fa05ab0>]}
[0m23:52:30.937191 [debug] [Thread-8 (]: Began running node model.banking_pipeline.stg_accounts
[0m23:52:30.939506 [info ] [Thread-8 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m23:52:30.941908 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m23:52:30.944046 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m23:52:30.946037 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m23:52:30.967170 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m23:52:30.980142 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.stg_accounts
[0m23:52:30.997495 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:52:31.001186 [warn ] [Thread-8 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:52:31.002978 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fc54cc10>]}
[0m23:52:31.026545 [debug] [Thread-8 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m23:52:31.039714 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m23:52:31.054371 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m23:52:31.056141 [debug] [Thread-8 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m23:52:31.057440 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:52:31.434376 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1889-15be-bd95-aaec8926b61a) - Created
[0m23:52:32.350931 [debug] [Thread-8 (]: SQL status: OK in 1.290 seconds
[0m23:52:32.355132 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-1889-15be-bd95-aaec8926b61a, command-id=01f0eced-189d-11e6-a285-f6261b673e7f) - Closing
[0m23:52:32.387830 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:52:32.394187 [debug] [Thread-8 (]: On model.banking_pipeline.stg_accounts: Close
[0m23:52:32.396234 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1889-15be-bd95-aaec8926b61a) - Closing
[0m23:52:32.555997 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a7321431b70>]}
[0m23:52:32.564290 [info ] [Thread-8 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.61s]
[0m23:52:32.573250 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.stg_accounts
[0m23:52:32.580855 [debug] [Thread-8 (]: Began running node model.banking_pipeline.stg_customers
[0m23:52:32.589115 [info ] [Thread-8 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m23:52:32.597434 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m23:52:32.605059 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m23:52:32.611733 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.stg_customers
[0m23:52:32.637380 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m23:52:32.661188 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.stg_customers
[0m23:52:32.669530 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:52:32.672264 [debug] [Thread-8 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m23:52:32.674160 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m23:52:32.686390 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m23:52:32.687710 [debug] [Thread-8 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m23:52:32.688864 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:52:33.089866 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1987-16db-8fc9-64920b314581) - Created
[0m23:52:33.936530 [debug] [Thread-8 (]: SQL status: OK in 1.250 seconds
[0m23:52:33.938756 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-1987-16db-8fc9-64920b314581, command-id=01f0eced-1999-1c04-b47c-8034584a0eb0) - Closing
[0m23:52:33.940604 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:52:33.942532 [debug] [Thread-8 (]: On model.banking_pipeline.stg_customers: Close
[0m23:52:33.943943 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1987-16db-8fc9-64920b314581) - Closing
[0m23:52:34.091277 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fd0a63b0>]}
[0m23:52:34.098529 [info ] [Thread-8 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.49s]
[0m23:52:34.103458 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.stg_customers
[0m23:52:34.108664 [debug] [Thread-8 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m23:52:34.112083 [info ] [Thread-8 (]: 3 of 4 START sql view model intermediate.int_customer_accounts ................. [RUN]
[0m23:52:34.115233 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m23:52:34.117868 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m23:52:34.120459 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m23:52:34.130785 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m23:52:34.146081 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m23:52:34.150084 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:52:34.152309 [debug] [Thread-8 (]: Creating view `workspace`.`intermediate`.`int_customer_accounts`
[0m23:52:34.154121 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m23:52:34.165177 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m23:52:34.166361 [debug] [Thread-8 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`intermediate`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m23:52:34.167498 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:52:34.570232 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1a68-138f-a00b-b20374fa98b7) - Created
[0m23:52:35.560835 [debug] [Thread-8 (]: SQL status: OK in 1.390 seconds
[0m23:52:35.563299 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-1a68-138f-a00b-b20374fa98b7, command-id=01f0eced-1a7b-1924-aae0-cd7aea1437fe) - Closing
[0m23:52:35.565277 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:52:35.567299 [debug] [Thread-8 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m23:52:35.568809 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1a68-138f-a00b-b20374fa98b7) - Closing
[0m23:52:35.691793 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fc532290>]}
[0m23:52:35.694054 [info ] [Thread-8 (]: 3 of 4 OK created sql view model intermediate.int_customer_accounts ............ [[32mOK[0m in 1.58s]
[0m23:52:35.695993 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m23:52:35.698589 [debug] [Thread-8 (]: Began running node model.banking_pipeline.account_interest_summary
[0m23:52:35.700908 [info ] [Thread-8 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m23:52:35.703517 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m23:52:35.705742 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m23:52:35.707831 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m23:52:35.715566 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m23:52:35.731086 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m23:52:35.756319 [debug] [Thread-8 (]: MATERIALIZING TABLE
[0m23:52:35.770000 [warn ] [Thread-8 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:52:35.772635 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731ec27b20>]}
[0m23:52:35.801430 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m23:52:35.816236 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m23:52:35.817952 [debug] [Thread-8 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`intermediate`.`int_customer_accounts`
WHERE account_type = 'savings'
  
[0m23:52:35.819290 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:52:36.216225 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1b66-1343-81c1-1c3f841b8e4c) - Created
[0m23:52:41.105842 [debug] [Thread-8 (]: SQL status: OK in 5.290 seconds
[0m23:52:41.112130 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-1b66-1343-81c1-1c3f841b8e4c, command-id=01f0eced-1b77-1ad9-8ad0-baa5f4d84123) - Closing
[0m23:52:41.122032 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:52:41.146918 [debug] [Thread-8 (]: On model.banking_pipeline.account_interest_summary: Close
[0m23:52:41.148493 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-1b66-1343-81c1-1c3f841b8e4c) - Closing
[0m23:52:41.283663 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ebf4a2d1-8d01-421e-9fc4-ffcea20a551d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731ec3df30>]}
[0m23:52:41.287878 [info ] [Thread-8 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 5.58s]
[0m23:52:41.291268 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m23:52:41.296582 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:52:41.298777 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:52:41.300997 [info ] [MainThread]: 
[0m23:52:41.303670 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 17.27 seconds (17.27s).
[0m23:52:41.306916 [debug] [MainThread]: Command end result
[0m23:52:41.390249 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:52:41.397758 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:52:41.409899 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:52:41.410940 [info ] [MainThread]: 
[0m23:52:41.412191 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:52:41.413967 [info ] [MainThread]: 
[0m23:52:41.415760 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m23:52:41.418633 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 20.715025, "process_in_blocks": "0", "process_kernel_time": 0.531726, "process_mem_max_rss": "246376", "process_out_blocks": "0", "process_user_time": 6.203479}
[0m23:52:41.421160 [debug] [MainThread]: Command `dbt run` succeeded at 23:52:41.420908 after 20.72 seconds
[0m23:52:41.422741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731f73a320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a72fcddf7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a731e6f7340>]}
[0m23:52:41.424191 [debug] [MainThread]: Flushing usage events
[0m23:52:41.885526 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:52:54.140801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5f0472380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5ef26c280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5ef26c220>]}


============================== 23:52:54.146525 | d2db4d4f-a449-4895-b335-bf6df2278a65 ==============================
[0m23:52:54.146525 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:52:54.148325 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'no_print': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'quiet': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'introspect': 'True', 'write_json': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'partial_parse': 'True', 'warn_error': 'None', 'log_path': '/opt/dagster/app/dbt/logs', 'empty': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:52:54.980799 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:52:54.982646 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:52:54.984537 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:52:55.898400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5ef26c5b0>]}
[0m23:52:56.003864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5eface890>]}
[0m23:52:56.005763 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:52:56.182425 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:52:56.598507 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:52:56.600352 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:52:56.612235 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:52:56.693442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5c9338b80>]}
[0m23:52:56.909859 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:52:56.918920 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:52:56.947154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5c933b0a0>]}
[0m23:52:56.948744 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:52:56.949944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5c933bbe0>]}
[0m23:52:56.954071 [info ] [MainThread]: 
[0m23:52:56.956665 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:52:56.959291 [info ] [MainThread]: 
[0m23:52:56.961087 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:52:56.963369 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:52:56.975907 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:52:56.977621 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:52:56.996672 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:52:56.998246 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:52:56.999662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:57.562333 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-281c-13f6-81e8-3e2ce9cc97f1) - Created
[0m23:52:58.003824 [debug] [ThreadPool]: SQL status: OK in 1.000 seconds
[0m23:52:58.010605 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-281c-13f6-81e8-3e2ce9cc97f1, command-id=01f0eced-282e-1e16-b94d-0bd33bf63808) - Closing
[0m23:52:58.013014 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:52:58.014802 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-281c-13f6-81e8-3e2ce9cc97f1) - Closing
[0m23:52:58.159245 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:52:58.163538 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:52:58.168126 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:52:58.169878 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:52:58.171411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:58.609911 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-28bd-1018-a3f6-5da4431eb69f) - Created
[0m23:52:58.998748 [debug] [ThreadPool]: SQL status: OK in 0.830 seconds
[0m23:52:59.003021 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-28bd-1018-a3f6-5da4431eb69f, command-id=01f0eced-28d1-1712-96c2-f5f5fbf78433) - Closing
[0m23:52:59.005378 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:52:59.007003 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-28bd-1018-a3f6-5da4431eb69f) - Closing
[0m23:52:59.143573 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:52:59.150334 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:52:59.164448 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:52:59.166530 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:52:59.168362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:59.562271 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-294e-14a4-b400-01567a6522c1) - Created
[0m23:52:59.983620 [debug] [ThreadPool]: SQL status: OK in 0.820 seconds
[0m23:52:59.987385 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-294e-14a4-b400-01567a6522c1, command-id=01f0eced-2960-146b-979f-20de0d35b52b) - Closing
[0m23:52:59.989423 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:52:59.991001 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-294e-14a4-b400-01567a6522c1) - Closing
[0m23:53:00.104733 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:53:00.106130 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:53:00.112828 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:53:00.114095 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:53:00.115342 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:00.529872 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-29e2-1cba-acd9-492d2fc483c5) - Created
[0m23:53:00.957357 [debug] [ThreadPool]: SQL status: OK in 0.840 seconds
[0m23:53:00.961000 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-29e2-1cba-acd9-492d2fc483c5, command-id=01f0eced-29f3-1a5c-9aa7-8122cf1ce39c) - Closing
[0m23:53:00.962863 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:53:00.964418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-29e2-1cba-acd9-492d2fc483c5) - Closing
[0m23:53:01.108041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2db4d4f-a449-4895-b335-bf6df2278a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5f1fc9240>]}
[0m23:53:01.119086 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:53:01.121792 [info ] [Thread-5 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m23:53:01.125143 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m23:53:01.127398 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m23:53:01.129627 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:53:01.160074 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:53:01.173673 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:53:01.192485 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:53:01.207418 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:53:01.209089 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m23:53:01.210364 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:01.644875 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2a8b-1c9c-912d-6ef0f7910f89) - Created
[0m23:53:02.738467 [debug] [Thread-5 (]: SQL status: OK in 1.530 seconds
[0m23:53:02.756056 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2a8b-1c9c-912d-6ef0f7910f89, command-id=01f0eced-2a9e-140a-b889-5d8b98b1d1c5) - Closing
[0m23:53:02.768575 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m23:53:02.771894 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2a8b-1c9c-912d-6ef0f7910f89) - Closing
[0m23:53:02.907131 [info ] [Thread-5 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.78s]
[0m23:53:02.909278 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:53:02.910923 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:53:02.912858 [info ] [Thread-5 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m23:53:02.915673 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m23:53:02.917256 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m23:53:02.918643 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:53:02.927693 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:53:02.941776 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:53:02.945934 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:53:02.958716 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:53:02.960021 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m23:53:02.961093 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:03.350550 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2b90-1d5e-b544-9c602ed94fe4) - Created
[0m23:53:03.878968 [debug] [Thread-5 (]: SQL status: OK in 0.920 seconds
[0m23:53:03.882958 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2b90-1d5e-b544-9c602ed94fe4, command-id=01f0eced-2ba1-19f4-9340-3bb97d400436) - Closing
[0m23:53:03.885495 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m23:53:03.887187 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2b90-1d5e-b544-9c602ed94fe4) - Closing
[0m23:53:04.024254 [info ] [Thread-5 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.11s]
[0m23:53:04.027289 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:53:04.030012 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:53:04.032836 [info ] [Thread-5 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m23:53:04.035778 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m23:53:04.038479 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m23:53:04.040819 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:53:04.063768 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:53:04.076711 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:53:04.081464 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:53:04.091457 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:53:04.092907 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:04.093983 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:04.480952 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2c3e-1471-a9c3-36d9bd1bf203) - Created
[0m23:53:05.123543 [debug] [Thread-5 (]: SQL status: OK in 1.030 seconds
[0m23:53:05.127087 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2c3e-1471-a9c3-36d9bd1bf203, command-id=01f0eced-2c4e-14ad-9a59-d2db77bae76f) - Closing
[0m23:53:05.129644 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m23:53:05.131109 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2c3e-1471-a9c3-36d9bd1bf203) - Closing
[0m23:53:05.261791 [info ] [Thread-5 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.22s]
[0m23:53:05.269890 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:53:05.275534 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:53:05.279088 [info ] [Thread-5 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m23:53:05.282465 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m23:53:05.284678 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m23:53:05.286819 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:53:05.300117 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:53:05.315570 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:53:05.319962 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:53:05.333123 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:53:05.334527 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m23:53:05.335677 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:05.714255 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2cf9-1e6c-b852-1da1491ebb6d) - Created
[0m23:53:06.498701 [debug] [Thread-5 (]: SQL status: OK in 1.160 seconds
[0m23:53:06.502612 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2cf9-1e6c-b852-1da1491ebb6d, command-id=01f0eced-2d0a-1711-89f0-aff3dd1cda17) - Closing
[0m23:53:06.504794 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m23:53:06.506581 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2cf9-1e6c-b852-1da1491ebb6d) - Closing
[0m23:53:06.646101 [info ] [Thread-5 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.36s]
[0m23:53:06.649070 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:53:06.651376 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:53:06.653339 [info ] [Thread-5 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m23:53:06.656286 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m23:53:06.658113 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m23:53:06.659820 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:53:06.671358 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:53:06.683398 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:53:06.688729 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:53:06.700686 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:53:06.702251 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:06.703618 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:07.153638 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2dd4-1721-87a3-83fbe6a24abf) - Created
[0m23:53:07.891908 [debug] [Thread-5 (]: SQL status: OK in 1.190 seconds
[0m23:53:07.896001 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2dd4-1721-87a3-83fbe6a24abf, command-id=01f0eced-2de6-18f9-8ed1-097f8f901220) - Closing
[0m23:53:07.898374 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m23:53:07.900118 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2dd4-1721-87a3-83fbe6a24abf) - Closing
[0m23:53:08.029937 [info ] [Thread-5 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.37s]
[0m23:53:08.031987 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:53:08.034123 [debug] [Thread-5 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:53:08.036283 [info ] [Thread-5 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m23:53:08.038954 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m23:53:08.040628 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m23:53:08.041881 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:53:08.051893 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:53:08.072474 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:53:08.077395 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:53:08.091195 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:53:08.092380 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m23:53:08.093267 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:08.476020 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2e9f-1204-882f-b42d21d8a561) - Created
[0m23:53:09.134772 [debug] [Thread-5 (]: SQL status: OK in 1.040 seconds
[0m23:53:09.138636 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2e9f-1204-882f-b42d21d8a561, command-id=01f0eced-2eb0-1549-8f06-cfb5c8ec9c36) - Closing
[0m23:53:09.140837 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m23:53:09.142494 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2e9f-1204-882f-b42d21d8a561) - Closing
[0m23:53:09.289514 [info ] [Thread-5 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.25s]
[0m23:53:09.291739 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:53:09.294087 [debug] [Thread-5 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:53:09.296132 [info ] [Thread-5 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m23:53:09.299168 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m23:53:09.300882 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m23:53:09.302416 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:53:09.313910 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:53:09.328458 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:53:09.333096 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:53:09.347045 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:53:09.348458 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m23:53:09.349680 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:09.818502 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2f6b-1d2d-9ddf-355194dffacb) - Created
[0m23:53:10.432749 [debug] [Thread-5 (]: SQL status: OK in 1.080 seconds
[0m23:53:10.436862 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-2f6b-1d2d-9ddf-355194dffacb, command-id=01f0eced-2f7e-1321-94ad-fb12d3c61e97) - Closing
[0m23:53:10.439068 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m23:53:10.440894 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-2f6b-1d2d-9ddf-355194dffacb) - Closing
[0m23:53:10.573832 [info ] [Thread-5 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.27s]
[0m23:53:10.580469 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:53:10.585103 [debug] [Thread-5 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:53:10.588565 [info ] [Thread-5 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m23:53:10.591815 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m23:53:10.593851 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m23:53:10.596033 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:53:10.613590 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:53:10.625438 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:53:10.629685 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:53:10.641485 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:53:10.642657 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:10.643712 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:11.034309 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-3025-14c5-bca0-7732567a6003) - Created
[0m23:53:11.647829 [debug] [Thread-5 (]: SQL status: OK in 1.000 seconds
[0m23:53:11.651491 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3025-14c5-bca0-7732567a6003, command-id=01f0eced-3035-183b-af35-52060abe7da2) - Closing
[0m23:53:11.653682 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m23:53:11.655400 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-3025-14c5-bca0-7732567a6003) - Closing
[0m23:53:11.786304 [info ] [Thread-5 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.19s]
[0m23:53:11.789898 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:53:11.793479 [debug] [Thread-5 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:53:11.795803 [info ] [Thread-5 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m23:53:11.798468 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m23:53:11.800566 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m23:53:11.803337 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:53:11.813018 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:53:11.824859 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:53:11.830988 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:53:11.849863 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:53:11.851379 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:11.852699 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:12.236827 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-30db-1d0a-bca7-1857cd8da288) - Created
[0m23:53:12.849906 [debug] [Thread-5 (]: SQL status: OK in 1.000 seconds
[0m23:53:12.859507 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-30db-1d0a-bca7-1857cd8da288, command-id=01f0eced-30ee-1337-a7a4-d142ad5321d5) - Closing
[0m23:53:12.864902 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m23:53:12.869205 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-30db-1d0a-bca7-1857cd8da288) - Closing
[0m23:53:13.009074 [info ] [Thread-5 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.21s]
[0m23:53:13.015661 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:53:13.019604 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:53:13.022914 [info ] [Thread-5 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m23:53:13.026088 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m23:53:13.028054 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m23:53:13.030113 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:53:13.032048 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:53:13.081675 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:13.083873 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:53:13.085601 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:13.474701 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4) - Created
[0m23:53:13.803854 [debug] [Thread-5 (]: SQL status: OK in 0.720 seconds
[0m23:53:13.808634 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4, command-id=01f0eced-31aa-1473-9951-01b67ea33003) - Closing
[0m23:53:13.840722 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:13.868552 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:13.929941 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:13.932163 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:53:14.330839 [debug] [Thread-5 (]: SQL status: OK in 0.400 seconds
[0m23:53:14.333845 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4, command-id=01f0eced-31f3-1e07-ba43-fbe928f2d9ee) - Closing
[0m23:53:14.338801 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:14.340843 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m23:53:14.589549 [debug] [Thread-5 (]: SQL status: OK in 0.250 seconds
[0m23:53:14.594538 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4, command-id=01f0eced-322e-1b3e-8494-4c46fcc254fc) - Closing
[0m23:53:14.608997 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:14.626314 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:14.628250 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:53:15.179849 [debug] [Thread-5 (]: SQL status: OK in 0.550 seconds
[0m23:53:15.184675 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4, command-id=01f0eced-325e-1e43-b73c-95e06ac05967) - Closing
[0m23:53:15.193046 [debug] [Thread-5 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:53:15.198483 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:53:15.200313 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:53:15.434949 [debug] [Thread-5 (]: SQL status: OK in 0.230 seconds
[0m23:53:15.438400 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4, command-id=01f0eced-32b1-1eb2-88bf-2f740b5ae27f) - Closing
[0m23:53:15.453093 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m23:53:15.455594 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-319a-1f24-9216-a2ebeb7e8ba4) - Closing
[0m23:53:15.599695 [info ] [Thread-5 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.57s]
[0m23:53:15.608460 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:53:15.613491 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:53:15.617097 [info ] [Thread-5 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m23:53:15.620327 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m23:53:15.622584 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m23:53:15.626087 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:53:15.629057 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:53:15.646203 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:15.647990 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:53:15.649556 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:16.038798 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7) - Created
[0m23:53:16.397297 [debug] [Thread-5 (]: SQL status: OK in 0.750 seconds
[0m23:53:16.400352 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7, command-id=01f0eced-3332-101b-acf7-719e1808325f) - Closing
[0m23:53:16.404806 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:16.429058 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:16.449733 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:16.451258 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:53:16.753977 [debug] [Thread-5 (]: SQL status: OK in 0.300 seconds
[0m23:53:16.766039 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7, command-id=01f0eced-3371-1dfa-ab9e-7ff71c3d3604) - Closing
[0m23:53:16.777033 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:16.779789 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m23:53:17.012576 [debug] [Thread-5 (]: SQL status: OK in 0.230 seconds
[0m23:53:17.016333 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7, command-id=01f0eced-33a3-1c47-95d1-fa93d29d0eca) - Closing
[0m23:53:17.020894 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:17.040233 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:17.041884 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:53:17.819954 [debug] [Thread-5 (]: SQL status: OK in 0.780 seconds
[0m23:53:17.828319 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7, command-id=01f0eced-33ca-1f02-984c-5149f122e714) - Closing
[0m23:53:17.833746 [debug] [Thread-5 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:53:17.835655 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:53:17.837289 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:53:18.049568 [debug] [Thread-5 (]: SQL status: OK in 0.210 seconds
[0m23:53:18.053419 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7, command-id=01f0eced-3443-1d90-bfc5-252ea522d388) - Closing
[0m23:53:18.058504 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m23:53:18.060763 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-3321-17e7-9ff6-fc0a340f37d7) - Closing
[0m23:53:18.194893 [info ] [Thread-5 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.57s]
[0m23:53:18.197823 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:53:18.200889 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:53:18.203952 [info ] [Thread-5 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m23:53:18.206695 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m23:53:18.208232 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m23:53:18.209586 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:53:18.211047 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:53:18.232471 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:18.234395 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:53:18.235860 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:53:18.648054 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-34af-1531-92dd-c961351c3de1) - Created
[0m23:53:18.951741 [debug] [Thread-5 (]: SQL status: OK in 0.720 seconds
[0m23:53:18.958472 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-34c0-100f-b732-201214995c92) - Closing
[0m23:53:18.967207 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:18.990789 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:19.014283 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:19.016531 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

    
[0m23:53:19.318480 [debug] [Thread-5 (]: SQL status: OK in 0.300 seconds
[0m23:53:19.325010 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-34f8-159c-86f8-cda230be9fb4) - Closing
[0m23:53:19.328876 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:19.330933 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:53:19.798817 [debug] [Thread-5 (]: SQL status: OK in 0.470 seconds
[0m23:53:19.804782 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-3528-1924-a74c-8d1a551e453a) - Closing
[0m23:53:19.812212 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:19.814545 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m23:53:20.039531 [debug] [Thread-5 (]: SQL status: OK in 0.220 seconds
[0m23:53:20.043596 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-3571-1773-87b0-d6c501d5d875) - Closing
[0m23:53:20.047111 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:20.064827 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:20.066758 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:53:20.337556 [debug] [Thread-5 (]: SQL status: OK in 0.270 seconds
[0m23:53:20.342318 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-3598-17d6-a116-2d693fb9096a) - Closing
[0m23:53:20.348683 [debug] [Thread-5 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m23:53:20.350816 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:53:20.352354 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m23:53:20.601165 [debug] [Thread-5 (]: SQL status: OK in 0.250 seconds
[0m23:53:20.604494 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-34af-1531-92dd-c961351c3de1, command-id=01f0eced-35c3-1d17-a9a0-cd56e0f25dad) - Closing
[0m23:53:20.610853 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m23:53:20.613539 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-34af-1531-92dd-c961351c3de1) - Closing
[0m23:53:20.758426 [info ] [Thread-5 (]: 12 of 12 PASS account_interest_summary::non_savings_account_filtered_out ....... [[32mPASS[0m in 2.55s]
[0m23:53:20.763940 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:53:20.771507 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:53:20.773410 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:53:20.775609 [info ] [MainThread]: 
[0m23:53:20.777485 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 23.81 seconds (23.81s).
[0m23:53:20.783718 [debug] [MainThread]: Command end result
[0m23:53:21.014684 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:53:21.022796 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:53:21.038758 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:53:21.040292 [info ] [MainThread]: 
[0m23:53:21.042016 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:53:21.044035 [info ] [MainThread]: 
[0m23:53:21.046279 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m23:53:21.049565 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 27.024618, "process_in_blocks": "0", "process_kernel_time": 0.655662, "process_mem_max_rss": "256204", "process_out_blocks": "0", "process_user_time": 6.712546}
[0m23:53:21.052073 [debug] [MainThread]: Command `dbt test` succeeded at 23:53:21.051874 after 27.03 seconds
[0m23:53:21.053597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5f0472380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5efa71300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78e5f070f2e0>]}
[0m23:53:21.055088 [debug] [MainThread]: Flushing usage events
[0m23:53:21.539793 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:57:39.850445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b2292440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b10dc2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b10dc250>]}


============================== 23:57:39.858002 | 5f4eefca-fb7d-4217-a275-4f43adfb0c87 ==============================
[0m23:57:39.858002 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:57:39.859418 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'printer_width': '80', 'introspect': 'True', 'debug': 'False', 'partial_parse': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'use_colors': 'True', 'fail_fast': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'invocation_command': 'dbt seed --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'static_parser': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'log_format': 'default', 'warn_error': 'None', 'cache_selected_only': 'False'}
[0m23:57:40.871317 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:57:40.872970 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:57:40.874300 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:57:42.071529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b3db90c0>]}
[0m23:57:42.228247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b1255cc0>]}
[0m23:57:42.230219 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:57:42.475022 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:57:43.102570 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:57:43.104551 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:57:43.120213 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:57:43.221783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b17c0b20>]}
[0m23:57:43.487957 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:57:43.498515 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:57:43.531331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75278b128c10>]}
[0m23:57:43.533589 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:57:43.536011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b17153c0>]}
[0m23:57:43.539637 [info ] [MainThread]: 
[0m23:57:43.541438 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:57:43.543127 [info ] [MainThread]: 
[0m23:57:43.545053 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:57:43.546522 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:43.560949 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:57:43.562966 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:57:43.581621 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:57:43.583967 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:57:43.586516 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:44.109188 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d2e7-19a7-90c9-1ee757707aab) - Created
[0m23:57:44.663021 [debug] [ThreadPool]: SQL status: OK in 1.080 seconds
[0m23:57:44.675677 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-d2e7-19a7-90c9-1ee757707aab, command-id=01f0eced-d2fc-19c1-99f0-77e80f824e83) - Closing
[0m23:57:44.678387 [debug] [ThreadPool]: On list_workspace: Close
[0m23:57:44.680584 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d2e7-19a7-90c9-1ee757707aab) - Closing
[0m23:57:44.836794 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:57:44.838450 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:57:44.848182 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:57:44.850119 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:57:44.852603 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:45.250570 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d397-100d-b62b-fc5030a264a0) - Created
[0m23:57:45.754777 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m23:57:45.759956 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-d397-100d-b62b-fc5030a264a0, command-id=01f0eced-d3a8-1396-a678-c6d0cea9447c) - Closing
[0m23:57:45.761967 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:57:45.763483 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d397-100d-b62b-fc5030a264a0) - Closing
[0m23:57:45.920854 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:57:45.925224 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:57:45.945907 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:57:45.948074 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:57:45.949807 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:46.349730 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d43f-1055-ace2-89236b7af76a) - Created
[0m23:57:46.834953 [debug] [ThreadPool]: SQL status: OK in 0.890 seconds
[0m23:57:46.841975 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-d43f-1055-ace2-89236b7af76a, command-id=01f0eced-d453-17c8-a41b-969ac1d6b4ea) - Closing
[0m23:57:46.846231 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:57:46.849126 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d43f-1055-ace2-89236b7af76a) - Closing
[0m23:57:46.991409 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:57:46.995344 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:57:47.007059 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:57:47.009328 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:57:47.010965 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:47.424133 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d4e1-1db5-94fb-3fe9bdaa5fd9) - Created
[0m23:57:47.953424 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m23:57:47.960860 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-d4e1-1db5-94fb-3fe9bdaa5fd9, command-id=01f0eced-d4f4-1c4c-a9df-830b7db52871) - Closing
[0m23:57:47.963775 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:57:47.966215 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d4e1-1db5-94fb-3fe9bdaa5fd9) - Closing
[0m23:57:48.094874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:57:48.097131 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:57:48.106156 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:57:48.108410 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:57:48.110094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:48.501503 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d588-1381-ada1-b0182352c30c) - Created
[0m23:57:49.046723 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m23:57:49.055204 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-d588-1381-ada1-b0182352c30c, command-id=01f0eced-d598-15eb-b743-7aac5dab0241) - Closing
[0m23:57:49.058014 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:57:49.060000 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-d588-1381-ada1-b0182352c30c) - Closing
[0m23:57:49.224276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b3bcb940>]}
[0m23:57:49.236730 [debug] [Thread-6 (]: Began running node seed.banking_pipeline.accounts
[0m23:57:49.239199 [info ] [Thread-6 (]: 1 of 2 START seed file raw.accounts ............................................ [RUN]
[0m23:57:49.241880 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.accounts) - Creating connection
[0m23:57:49.244168 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.banking_pipeline.accounts'
[0m23:57:49.245905 [debug] [Thread-6 (]: Began compiling node seed.banking_pipeline.accounts
[0m23:57:49.247769 [debug] [Thread-6 (]: Began executing node seed.banking_pipeline.accounts
[0m23:57:49.255864 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:57:49.257812 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752789853430>]}
[0m23:57:49.309814 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:57:49.311465 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x752789851630>]}
[0m23:57:49.342894 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:57:49.344645 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.accounts"} */

    create or replace table `workspace`.`raw`.`accounts` (`AccountID` string ,`CustomerID` bigint ,`Balance` bigint ,`AccountType` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:57:49.346375 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m23:57:49.763829 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-d646-1cf1-8db8-a4f4884bd339) - Created
[0m23:57:51.632114 [debug] [Thread-6 (]: SQL status: OK in 2.290 seconds
[0m23:57:51.634297 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-d646-1cf1-8db8-a4f4884bd339, command-id=01f0eced-d659-116e-8576-f166223fa3cc) - Closing
[0m23:57:51.656124 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.accounts"
[0m23:57:51.658151 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: 
          insert overwrite `workspace`.`raw`.`accounts` values
          (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s)
      ...
[0m23:57:53.711075 [debug] [Thread-6 (]: SQL status: OK in 2.050 seconds
[0m23:57:53.714171 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-d646-1cf1-8db8-a4f4884bd339, command-id=01f0eced-d77a-1717-a084-7275a24ad1b8) - Closing
[0m23:57:53.728256 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.banking_pipeline.accounts"
[0m23:57:53.756328 [debug] [Thread-6 (]: On seed.banking_pipeline.accounts: Close
[0m23:57:53.758071 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-d646-1cf1-8db8-a4f4884bd339) - Closing
[0m23:57:53.908860 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b3fe9a50>]}
[0m23:57:53.915178 [info ] [Thread-6 (]: 1 of 2 OK loaded seed file raw.accounts ........................................ [[32mINSERT 7[0m in 4.66s]
[0m23:57:53.919464 [debug] [Thread-6 (]: Finished running node seed.banking_pipeline.accounts
[0m23:57:53.922871 [debug] [Thread-6 (]: Began running node seed.banking_pipeline.customers
[0m23:57:53.925715 [info ] [Thread-6 (]: 2 of 2 START seed file raw.customers ........................................... [RUN]
[0m23:57:53.928451 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.banking_pipeline.customers) - Creating connection
[0m23:57:53.930464 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.banking_pipeline.customers'
[0m23:57:53.932484 [debug] [Thread-6 (]: Began compiling node seed.banking_pipeline.customers
[0m23:57:53.934300 [debug] [Thread-6 (]: Began executing node seed.banking_pipeline.customers
[0m23:57:53.949116 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:57:53.950608 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "seed.banking_pipeline.customers"} */

    create or replace table `workspace`.`raw`.`customers` (`CustomerID` bigint ,`Name` string ,`HasLoan` string )
    
    using delta
  
    
    
    
    
    
  
[0m23:57:53.952055 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m23:57:54.349300 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-d902-1e55-96c5-ef24b1f5701f) - Created
[0m23:57:56.180029 [debug] [Thread-6 (]: SQL status: OK in 2.230 seconds
[0m23:57:56.182490 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-d902-1e55-96c5-ef24b1f5701f, command-id=01f0eced-d914-17ce-a4b9-9785cb4687a8) - Closing
[0m23:57:56.188018 [debug] [Thread-6 (]: Using databricks connection "seed.banking_pipeline.customers"
[0m23:57:56.189977 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: 
          insert overwrite `workspace`.`raw`.`customers` values
          (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s)
      ...
[0m23:57:57.996301 [debug] [Thread-6 (]: SQL status: OK in 1.800 seconds
[0m23:57:58.000195 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0eced-d902-1e55-96c5-ef24b1f5701f, command-id=01f0eced-da2c-1e5f-8db3-31b289a8175d) - Closing
[0m23:57:58.005737 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.banking_pipeline.customers"
[0m23:57:58.026544 [debug] [Thread-6 (]: On seed.banking_pipeline.customers: Close
[0m23:57:58.028084 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0eced-d902-1e55-96c5-ef24b1f5701f) - Closing
[0m23:57:58.158179 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f4eefca-fb7d-4217-a275-4f43adfb0c87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b2595270>]}
[0m23:57:58.161089 [info ] [Thread-6 (]: 2 of 2 OK loaded seed file raw.customers ....................................... [[32mINSERT 6[0m in 4.23s]
[0m23:57:58.163701 [debug] [Thread-6 (]: Finished running node seed.banking_pipeline.customers
[0m23:57:58.168199 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:57:58.170951 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:58.173842 [info ] [MainThread]: 
[0m23:57:58.176031 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 14.63 seconds (14.63s).
[0m23:57:58.178722 [debug] [MainThread]: Command end result
[0m23:57:58.274515 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:57:58.283178 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:57:58.301388 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:57:58.307817 [info ] [MainThread]: 
[0m23:57:58.312072 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:57:58.315616 [info ] [MainThread]: 
[0m23:57:58.318212 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:57:58.322766 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 18.59766, "process_in_blocks": "0", "process_kernel_time": 0.813317, "process_mem_max_rss": "246320", "process_out_blocks": "712", "process_user_time": 6.589851}
[0m23:57:58.325098 [debug] [MainThread]: Command `dbt seed` succeeded at 23:57:58.324859 after 18.60 seconds
[0m23:57:58.326903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b2292440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b21fb3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7527b1937790>]}
[0m23:57:58.328536 [debug] [MainThread]: Flushing usage events
[0m23:57:58.806823 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:58:04.492525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f2084e260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f1f5f4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f1f5f41f0>]}


============================== 23:58:04.496459 | f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8 ==============================
[0m23:58:04.496459 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:58:04.497920 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'printer_width': '80', 'use_experimental_parser': 'False', 'profiles_dir': '/opt/dagster/app/dbt', 'no_print': 'None', 'quiet': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt run --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'use_colors': 'True', 'log_path': '/opt/dagster/app/dbt/logs', 'version_check': 'True', 'write_json': 'True', 'debug': 'False', 'target_path': 'None', 'warn_error': 'None', 'empty': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager'}
[0m23:58:05.528427 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:58:05.530621 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:58:05.532429 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:58:06.633666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f1f7c8bb0>]}
[0m23:58:06.756324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725efa287040>]}
[0m23:58:06.758544 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:58:06.969401 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:58:07.494469 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:58:07.496704 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:58:07.511938 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:58:07.645687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef96e3e50>]}
[0m23:58:07.894962 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:58:07.903705 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:58:07.926665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef97018d0>]}
[0m23:58:07.928159 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:58:07.929446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef9701990>]}
[0m23:58:07.932442 [info ] [MainThread]: 
[0m23:58:07.934210 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:58:07.936230 [info ] [MainThread]: 
[0m23:58:07.938199 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:58:07.939682 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:58:07.951623 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:58:07.953270 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:58:07.970417 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:58:07.972141 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:58:07.973619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:08.404187 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e164-195a-94d1-ba371e4e9048) - Created
[0m23:58:08.731830 [debug] [ThreadPool]: SQL status: OK in 0.760 seconds
[0m23:58:08.742054 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e164-195a-94d1-ba371e4e9048, command-id=01f0eced-e176-1024-809e-dcf45a1db18f) - Closing
[0m23:58:08.744563 [debug] [ThreadPool]: On list_workspace: Close
[0m23:58:08.746473 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e164-195a-94d1-ba371e4e9048) - Closing
[0m23:58:08.881237 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:58:08.885581 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:58:08.897215 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:58:08.900247 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:58:08.902940 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:09.367032 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e1f4-1640-87a4-036393c08962) - Created
[0m23:58:09.966675 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m23:58:09.973644 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e1f4-1640-87a4-036393c08962, command-id=01f0eced-e209-15ef-852d-5772d2a11332) - Closing
[0m23:58:09.976502 [debug] [ThreadPool]: On list_workspace: Close
[0m23:58:09.978529 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e1f4-1640-87a4-036393c08962) - Closing
[0m23:58:10.119943 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m23:58:10.124837 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m23:58:10.146948 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m23:58:10.149198 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m23:58:10.151035 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:10.571250 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e2af-16e6-9099-61b524899f79) - Created
[0m23:58:10.902344 [debug] [ThreadPool]: SQL status: OK in 0.750 seconds
[0m23:58:10.916445 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e2af-16e6-9099-61b524899f79, command-id=01f0eced-e2bf-1ec7-8b35-35cf7c9c18fa) - Closing
[0m23:58:10.921783 [debug] [ThreadPool]: On list_workspace: Close
[0m23:58:10.924873 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e2af-16e6-9099-61b524899f79) - Closing
[0m23:58:11.080136 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:58:11.081549 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:58:11.094444 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:58:11.095924 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:58:11.097356 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:11.477576 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e339-1e0e-9657-a8b201b3de07) - Created
[0m23:58:11.904763 [debug] [ThreadPool]: SQL status: OK in 0.810 seconds
[0m23:58:11.910557 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e339-1e0e-9657-a8b201b3de07, command-id=01f0eced-e34b-1ad8-a61e-0e4da3361d3c) - Closing
[0m23:58:11.913026 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:58:11.915072 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e339-1e0e-9657-a8b201b3de07) - Closing
[0m23:58:12.037672 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:58:12.039449 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:58:12.043482 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:58:12.045000 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:58:12.046461 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:12.419661 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e3c9-1373-b14f-be972a15b9c1) - Created
[0m23:58:12.796598 [debug] [ThreadPool]: SQL status: OK in 0.750 seconds
[0m23:58:12.799783 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e3c9-1373-b14f-be972a15b9c1, command-id=01f0eced-e3da-13f2-b641-e59fa501b296) - Closing
[0m23:58:12.801699 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:58:12.803400 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e3c9-1373-b14f-be972a15b9c1) - Closing
[0m23:58:12.957458 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:58:12.965564 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:58:12.995598 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:58:12.999976 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:58:13.004412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:13.417484 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e460-1e96-935d-a8ebaf6cd9a3) - Created
[0m23:58:13.809672 [debug] [ThreadPool]: SQL status: OK in 0.810 seconds
[0m23:58:13.818025 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e460-1e96-935d-a8ebaf6cd9a3, command-id=01f0eced-e473-1fac-8f97-3548cd0a6855) - Closing
[0m23:58:13.821969 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:58:13.824436 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e460-1e96-935d-a8ebaf6cd9a3) - Closing
[0m23:58:13.964420 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:58:13.972219 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:58:13.995762 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:58:13.998389 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:58:14.000887 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:14.385149 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e4f4-12cc-813f-f1efff782cb7) - Created
[0m23:58:14.797739 [debug] [ThreadPool]: SQL status: OK in 0.800 seconds
[0m23:58:14.801125 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-e4f4-12cc-813f-f1efff782cb7, command-id=01f0eced-e508-1068-8b5a-fb522b8b69bb) - Closing
[0m23:58:14.803482 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:58:14.805035 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-e4f4-12cc-813f-f1efff782cb7) - Closing
[0m23:58:14.943351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f2212e800>]}
[0m23:58:14.953627 [debug] [Thread-8 (]: Began running node model.banking_pipeline.stg_accounts
[0m23:58:14.956623 [info ] [Thread-8 (]: 1 of 4 START sql view model staging.stg_accounts ............................... [RUN]
[0m23:58:14.959238 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_accounts) - Creating connection
[0m23:58:14.961208 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_accounts'
[0m23:58:14.963101 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.stg_accounts
[0m23:58:14.982185 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.stg_accounts"
[0m23:58:14.994887 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.stg_accounts
[0m23:58:15.013435 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:58:15.017317 [warn ] [Thread-8 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:58:15.019381 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef9687a90>]}
[0m23:58:15.037404 [debug] [Thread-8 (]: Creating view `workspace`.`staging`.`stg_accounts`
[0m23:58:15.048076 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.stg_accounts"
[0m23:58:15.060664 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.stg_accounts"
[0m23:58:15.061980 [debug] [Thread-8 (]: On model.banking_pipeline.stg_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_accounts"} */

  
  
  create or replace view `workspace`.`staging`.`stg_accounts`
  
  as (
    select
    trim(accountid) as account_id,
    cast(trim(customerid) as int) as customer_id,

    cast(coalesce(balance,0) as decimal(12,2)) as balance,

    lower(trim(accounttype)) as account_type

from `workspace`.`raw`.`accounts`
where accountid is not null
  )

[0m23:58:15.062980 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:58:15.437662 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e597-11c7-b2d8-2db20bd9bd94) - Created
[0m23:58:16.233997 [debug] [Thread-8 (]: SQL status: OK in 1.170 seconds
[0m23:58:16.241946 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-e597-11c7-b2d8-2db20bd9bd94, command-id=01f0eced-e5a8-165d-bd09-a6cffbd951b8) - Closing
[0m23:58:16.262601 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:58:16.266752 [debug] [Thread-8 (]: On model.banking_pipeline.stg_accounts: Close
[0m23:58:16.268476 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e597-11c7-b2d8-2db20bd9bd94) - Closing
[0m23:58:16.416316 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f22521870>]}
[0m23:58:16.422061 [info ] [Thread-8 (]: 1 of 4 OK created sql view model staging.stg_accounts .......................... [[32mOK[0m in 1.45s]
[0m23:58:16.427720 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.stg_accounts
[0m23:58:16.431680 [debug] [Thread-8 (]: Began running node model.banking_pipeline.stg_customers
[0m23:58:16.435072 [info ] [Thread-8 (]: 2 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m23:58:16.439386 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.stg_customers) - Creating connection
[0m23:58:16.442112 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.stg_customers'
[0m23:58:16.444429 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.stg_customers
[0m23:58:16.451102 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.stg_customers"
[0m23:58:16.466546 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.stg_customers
[0m23:58:16.475488 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:58:16.478032 [debug] [Thread-8 (]: Creating view `workspace`.`staging`.`stg_customers`
[0m23:58:16.479910 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.stg_customers"
[0m23:58:16.493494 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.stg_customers"
[0m23:58:16.494757 [debug] [Thread-8 (]: On model.banking_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.stg_customers"} */

  
  
  create or replace view `workspace`.`staging`.`stg_customers`
  
  as (
    select
    cast(trim(customerid) as int) as customer_id,
    initcap(trim(name)) as customer_name,

    case
        when lower(trim(hasloan)) in ('yes', 'y', 'true') then true
        when lower(trim(hasloan)) in ('no', 'n', 'false') then false
        else false
    end as has_loan

from `workspace`.`raw`.`customers`
where customerid is not null
  )

[0m23:58:16.495824 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:58:16.878590 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e671-1450-95d6-b29ca97ce378) - Created
[0m23:58:17.572331 [debug] [Thread-8 (]: SQL status: OK in 1.080 seconds
[0m23:58:17.574581 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-e671-1450-95d6-b29ca97ce378, command-id=01f0eced-e681-1b29-b9db-3158fd284be4) - Closing
[0m23:58:17.576533 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:58:17.578866 [debug] [Thread-8 (]: On model.banking_pipeline.stg_customers: Close
[0m23:58:17.580386 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e671-1450-95d6-b29ca97ce378) - Closing
[0m23:58:17.709219 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725efa2c8a60>]}
[0m23:58:17.711261 [info ] [Thread-8 (]: 2 of 4 OK created sql view model staging.stg_customers ......................... [[32mOK[0m in 1.27s]
[0m23:58:17.712843 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.stg_customers
[0m23:58:17.715197 [debug] [Thread-8 (]: Began running node model.banking_pipeline.int_customer_accounts
[0m23:58:17.716988 [info ] [Thread-8 (]: 3 of 4 START sql view model intermediate.int_customer_accounts ................. [RUN]
[0m23:58:17.718824 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.int_customer_accounts) - Creating connection
[0m23:58:17.720652 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.int_customer_accounts'
[0m23:58:17.722442 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.int_customer_accounts
[0m23:58:17.730678 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.int_customer_accounts"
[0m23:58:17.746129 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.int_customer_accounts
[0m23:58:17.750902 [debug] [Thread-8 (]: MATERIALIZING VIEW
[0m23:58:17.754388 [debug] [Thread-8 (]: Creating view `workspace`.`intermediate`.`int_customer_accounts`
[0m23:58:17.756841 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.int_customer_accounts"
[0m23:58:17.771571 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.int_customer_accounts"
[0m23:58:17.773214 [debug] [Thread-8 (]: On model.banking_pipeline.int_customer_accounts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.int_customer_accounts"} */

  
  
  create or replace view `workspace`.`intermediate`.`int_customer_accounts`
  
  as (
    select
    a.account_id,
    a.customer_id,
    c.customer_name,
    c.has_loan,
    a.balance,
    a.account_type

from `workspace`.`staging`.`stg_accounts` a
join `workspace`.`staging`.`stg_customers` c
  on a.customer_id = c.customer_id
where a.account_type = 'savings'
  )

[0m23:58:17.774717 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:58:18.173901 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e736-1b63-b1b1-28433d6d2e09) - Created
[0m23:58:19.181371 [debug] [Thread-8 (]: SQL status: OK in 1.410 seconds
[0m23:58:19.187210 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-e736-1b63-b1b1-28433d6d2e09, command-id=01f0eced-e748-19ad-afbe-0779c2fd828c) - Closing
[0m23:58:19.192842 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:58:19.198846 [debug] [Thread-8 (]: On model.banking_pipeline.int_customer_accounts: Close
[0m23:58:19.200507 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e736-1b63-b1b1-28433d6d2e09) - Closing
[0m23:58:19.316297 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef9694bb0>]}
[0m23:58:19.320329 [info ] [Thread-8 (]: 3 of 4 OK created sql view model intermediate.int_customer_accounts ............ [[32mOK[0m in 1.60s]
[0m23:58:19.323572 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.int_customer_accounts
[0m23:58:19.327927 [debug] [Thread-8 (]: Began running node model.banking_pipeline.account_interest_summary
[0m23:58:19.329912 [info ] [Thread-8 (]: 4 of 4 START sql table model marts.account_interest_summary .................... [RUN]
[0m23:58:19.332294 [debug] [Thread-8 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.banking_pipeline.account_interest_summary) - Creating connection
[0m23:58:19.334276 [debug] [Thread-8 (]: Acquiring new databricks connection 'model.banking_pipeline.account_interest_summary'
[0m23:58:19.335796 [debug] [Thread-8 (]: Began compiling node model.banking_pipeline.account_interest_summary
[0m23:58:19.346954 [debug] [Thread-8 (]: Writing injected SQL for node "model.banking_pipeline.account_interest_summary"
[0m23:58:19.362748 [debug] [Thread-8 (]: Began executing node model.banking_pipeline.account_interest_summary
[0m23:58:19.396765 [debug] [Thread-8 (]: MATERIALIZING TABLE
[0m23:58:19.413972 [warn ] [Thread-8 (]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m23:58:19.416231 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef9737ca0>]}
[0m23:58:19.448065 [debug] [Thread-8 (]: Writing runtime sql for node "model.banking_pipeline.account_interest_summary"
[0m23:58:19.462350 [debug] [Thread-8 (]: Using databricks connection "model.banking_pipeline.account_interest_summary"
[0m23:58:19.463914 [debug] [Thread-8 (]: On model.banking_pipeline.account_interest_summary: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "model.banking_pipeline.account_interest_summary"} */

  
    
        create or replace table `workspace`.`marts`.`account_interest_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from `workspace`.`intermediate`.`int_customer_accounts`
WHERE account_type = 'savings'
  
[0m23:58:19.465330 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m23:58:19.834372 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e835-15ee-b7a7-88658f91d0df) - Created
[0m23:58:23.559113 [debug] [Thread-8 (]: SQL status: OK in 4.090 seconds
[0m23:58:23.567965 [debug] [Thread-8 (]: Databricks adapter: Cursor(session-id=01f0eced-e835-15ee-b7a7-88658f91d0df, command-id=01f0eced-e845-104d-8fac-2c6384a4da46) - Closing
[0m23:58:23.586408 [debug] [Thread-8 (]: Applying tags to relation None
[0m23:58:23.614706 [debug] [Thread-8 (]: On model.banking_pipeline.account_interest_summary: Close
[0m23:58:23.616557 [debug] [Thread-8 (]: Databricks adapter: Connection(session-id=01f0eced-e835-15ee-b7a7-88658f91d0df) - Closing
[0m23:58:23.740506 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9d9a923-a2f2-4775-8f05-f0b2fb4cd8f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725ef9703e50>]}
[0m23:58:23.742708 [info ] [Thread-8 (]: 4 of 4 OK created sql table model marts.account_interest_summary ............... [[32mOK[0m in 4.41s]
[0m23:58:23.744239 [debug] [Thread-8 (]: Finished running node model.banking_pipeline.account_interest_summary
[0m23:58:23.747852 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:58:23.749542 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:58:23.751429 [info ] [MainThread]: 
[0m23:58:23.752869 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 15.81 seconds (15.81s).
[0m23:58:23.756799 [debug] [MainThread]: Command end result
[0m23:58:23.856189 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:58:23.866211 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:58:23.883399 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:58:23.885017 [info ] [MainThread]: 
[0m23:58:23.886628 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:58:23.888260 [info ] [MainThread]: 
[0m23:58:23.891036 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m23:58:23.895020 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.53492, "process_in_blocks": "0", "process_kernel_time": 0.692786, "process_mem_max_rss": "246280", "process_out_blocks": "0", "process_user_time": 6.943791}
[0m23:58:23.896743 [debug] [MainThread]: Command `dbt run` succeeded at 23:58:23.896621 after 19.54 seconds
[0m23:58:23.898301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f2084e260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f20aa0220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725f1f7c8bb0>]}
[0m23:58:23.899733 [debug] [MainThread]: Flushing usage events
[0m23:58:24.345746 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:58:30.054312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733201c9e230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733200a4c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733200a4c250>]}


============================== 23:58:30.059487 | e9d47b15-0d74-44f9-be2f-20312576f8c5 ==============================
[0m23:58:30.059487 [info ] [MainThread]: Running with dbt=1.10.18
[0m23:58:30.061459 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/dagster/app/dbt/logs', 'printer_width': '80', 'no_print': 'None', 'profiles_dir': '/opt/dagster/app/dbt', 'version_check': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt test --project-dir /opt/dagster/app/dbt --profiles-dir /opt/dagster/app/dbt --target dev', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'introspect': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'target_path': 'None', 'static_parser': 'True', 'debug': 'False', 'log_format': 'default', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False'}
[0m23:58:31.037491 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:58:31.039144 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:58:31.040608 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:58:32.100024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7331fea2b310>]}
[0m23:58:32.240283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733200a2eda0>]}
[0m23:58:32.242241 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m23:58:32.464785 [debug] [MainThread]: checksum: b3bb00889c1b57d51d407ca814a17daa4f1347dc3f6b0f29771353a169cea8c9, vars: {}, profile: , target: dev, version: 1.10.18
[0m23:58:33.053753 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:58:33.055698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:58:33.070526 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.banking_pipeline.intermediate
[0m23:58:33.171915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7331dab20a60>]}
[0m23:58:33.420951 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:58:33.428555 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:58:33.460631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7331dab23a30>]}
[0m23:58:33.462132 [info ] [MainThread]: Found 4 models, 2 seeds, 9 data tests, 2 sources, 711 macros, 3 unit tests
[0m23:58:33.463412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332011b8f40>]}
[0m23:58:33.467065 [info ] [MainThread]: 
[0m23:58:33.469530 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:58:33.471183 [info ] [MainThread]: 
[0m23:58:33.472964 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:58:33.474470 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:58:33.486084 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_raw) - Creating connection
[0m23:58:33.487804 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_raw'
[0m23:58:33.508050 [debug] [ThreadPool]: Using databricks connection "list_workspace_raw"
[0m23:58:33.509917 [debug] [ThreadPool]: On list_workspace_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'raw'

  
[0m23:58:33.511395 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:33.898623 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f096-1580-a69e-2b3e23d95435) - Created
[0m23:58:34.452928 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m23:58:34.458575 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-f096-1580-a69e-2b3e23d95435, command-id=01f0eced-f0a8-1189-ad4f-267edbb3a5fd) - Closing
[0m23:58:34.460278 [debug] [ThreadPool]: On list_workspace_raw: Close
[0m23:58:34.461596 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f096-1580-a69e-2b3e23d95435) - Closing
[0m23:58:34.592877 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_intermediate) - Creating connection
[0m23:58:34.597912 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_intermediate'
[0m23:58:34.609170 [debug] [ThreadPool]: Using databricks connection "list_workspace_intermediate"
[0m23:58:34.611782 [debug] [ThreadPool]: On list_workspace_intermediate: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_intermediate"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'intermediate'

  
[0m23:58:34.614280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:35.005250 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f140-1dba-8d1b-bb7809d8f951) - Created
[0m23:58:35.443109 [debug] [ThreadPool]: SQL status: OK in 0.830 seconds
[0m23:58:35.446721 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-f140-1dba-8d1b-bb7809d8f951, command-id=01f0eced-f150-142f-9c4c-fca5c0174518) - Closing
[0m23:58:35.448762 [debug] [ThreadPool]: On list_workspace_intermediate: Close
[0m23:58:35.450299 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f140-1dba-8d1b-bb7809d8f951) - Closing
[0m23:58:35.573865 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_staging) - Creating connection
[0m23:58:35.577318 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_staging'
[0m23:58:35.592760 [debug] [ThreadPool]: Using databricks connection "list_workspace_staging"
[0m23:58:35.595395 [debug] [ThreadPool]: On list_workspace_staging: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'staging'

  
[0m23:58:35.597424 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:35.981887 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f1d4-115e-9c44-2e31527f04f9) - Created
[0m23:58:36.611754 [debug] [ThreadPool]: SQL status: OK in 1.010 seconds
[0m23:58:36.620514 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-f1d4-115e-9c44-2e31527f04f9, command-id=01f0eced-f1e5-15bd-a257-7dd4c7f081b9) - Closing
[0m23:58:36.624171 [debug] [ThreadPool]: On list_workspace_staging: Close
[0m23:58:36.626507 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f1d4-115e-9c44-2e31527f04f9) - Closing
[0m23:58:36.759120 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_marts) - Creating connection
[0m23:58:36.764253 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_marts'
[0m23:58:36.782339 [debug] [ThreadPool]: Using databricks connection "list_workspace_marts"
[0m23:58:36.785459 [debug] [ThreadPool]: On list_workspace_marts: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "connection_name": "list_workspace_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'marts'

  
[0m23:58:36.788284 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:58:37.183855 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f28d-1b5e-a896-73182acee631) - Created
[0m23:58:37.757496 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m23:58:37.760841 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eced-f28d-1b5e-a896-73182acee631, command-id=01f0eced-f29c-1601-80ac-150dd6fe09e6) - Closing
[0m23:58:37.762616 [debug] [ThreadPool]: On list_workspace_marts: Close
[0m23:58:37.764039 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eced-f28d-1b5e-a896-73182acee631) - Closing
[0m23:58:37.893632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9d47b15-0d74-44f9-be2f-20312576f8c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7331fea22590>]}
[0m23:58:37.898436 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:58:37.900309 [info ] [Thread-5 (]: 1 of 12 START test not_null_account_interest_summary_interest_rate ............. [RUN]
[0m23:58:37.902583 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc) - Creating connection
[0m23:58:37.904615 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc'
[0m23:58:37.905978 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:58:37.933562 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:58:37.949297 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:58:37.975031 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:58:37.991596 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"
[0m23:58:37.993248 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select interest_rate
from `workspace`.`marts`.`account_interest_summary`
where interest_rate is null



  
  
      
    ) dbt_internal_test
[0m23:58:37.994703 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:38.365624 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f341-1259-afc3-3f851b82bf8d) - Created
[0m23:58:38.910484 [debug] [Thread-5 (]: SQL status: OK in 0.920 seconds
[0m23:58:38.914453 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f341-1259-afc3-3f851b82bf8d, command-id=01f0eced-f350-1ed6-92ae-a7a0ef7b472f) - Closing
[0m23:58:38.920634 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc: Close
[0m23:58:38.922567 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f341-1259-afc3-3f851b82bf8d) - Closing
[0m23:58:39.045979 [info ] [Thread-5 (]: 1 of 12 PASS not_null_account_interest_summary_interest_rate ................... [[32mPASS[0m in 1.14s]
[0m23:58:39.048075 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_interest_rate.9b0df67ddc
[0m23:58:39.049833 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:58:39.051778 [info ] [Thread-5 (]: 2 of 12 START test not_null_account_interest_summary_new_balance ............... [RUN]
[0m23:58:39.054522 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f) - Creating connection
[0m23:58:39.056371 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f'
[0m23:58:39.057787 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:58:39.064835 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:58:39.078325 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:58:39.084927 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:58:39.097577 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"
[0m23:58:39.099064 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select new_balance
from `workspace`.`marts`.`account_interest_summary`
where new_balance is null



  
  
      
    ) dbt_internal_test
[0m23:58:39.100358 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:39.499798 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f3ed-1ee8-91d8-75c051424b6b) - Created
[0m23:58:40.097950 [debug] [Thread-5 (]: SQL status: OK in 1.000 seconds
[0m23:58:40.101515 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f3ed-1ee8-91d8-75c051424b6b, command-id=01f0eced-f402-1696-9034-e4e815930d47) - Closing
[0m23:58:40.103900 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f: Close
[0m23:58:40.105663 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f3ed-1ee8-91d8-75c051424b6b) - Closing
[0m23:58:40.257446 [info ] [Thread-5 (]: 2 of 12 PASS not_null_account_interest_summary_new_balance ..................... [[32mPASS[0m in 1.20s]
[0m23:58:40.259409 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_account_interest_summary_new_balance.526243b16f
[0m23:58:40.261138 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:58:40.262751 [info ] [Thread-5 (]: 3 of 12 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m23:58:40.264770 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m23:58:40.266443 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108'
[0m23:58:40.268030 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:58:40.279899 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:58:40.293565 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:58:40.297735 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:58:40.312985 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"
[0m23:58:40.314515 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`staging`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m23:58:40.315710 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:40.686737 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f4a2-1ac4-b1b2-93bd8489ff02) - Created
[0m23:58:41.453563 [debug] [Thread-5 (]: SQL status: OK in 1.140 seconds
[0m23:58:41.461641 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f4a2-1ac4-b1b2-93bd8489ff02, command-id=01f0eced-f4b3-139e-b2c4-137ea4cd5f96) - Closing
[0m23:58:41.465654 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108: Close
[0m23:58:41.468874 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f4a2-1ac4-b1b2-93bd8489ff02) - Closing
[0m23:58:41.616523 [info ] [Thread-5 (]: 3 of 12 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 1.35s]
[0m23:58:41.625518 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_account_id.182dfbc108
[0m23:58:41.632270 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:58:41.638780 [info ] [Thread-5 (]: 4 of 12 START test not_null_stg_accounts_balance ............................... [RUN]
[0m23:58:41.644115 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_accounts_balance.508c44cded) - Creating connection
[0m23:58:41.647402 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_accounts_balance.508c44cded'
[0m23:58:41.650467 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:58:41.664117 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:58:41.679383 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:58:41.683758 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:58:41.698802 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"
[0m23:58:41.700330 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_accounts_balance.508c44cded"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select balance
from `workspace`.`staging`.`stg_accounts`
where balance is null



  
  
      
    ) dbt_internal_test
[0m23:58:41.701531 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:42.074915 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f576-147a-b887-aac43c25277b) - Created
[0m23:58:42.641805 [debug] [Thread-5 (]: SQL status: OK in 0.940 seconds
[0m23:58:42.647886 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f576-147a-b887-aac43c25277b, command-id=01f0eced-f587-10c7-99c6-ca2ebf54ae71) - Closing
[0m23:58:42.651828 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_accounts_balance.508c44cded: Close
[0m23:58:42.655255 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f576-147a-b887-aac43c25277b) - Closing
[0m23:58:42.772888 [info ] [Thread-5 (]: 4 of 12 PASS not_null_stg_accounts_balance ..................................... [[32mPASS[0m in 1.13s]
[0m23:58:42.774748 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_accounts_balance.508c44cded
[0m23:58:42.776312 [debug] [Thread-5 (]: Began running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:58:42.777699 [info ] [Thread-5 (]: 5 of 12 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m23:58:42.779412 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa) - Creating connection
[0m23:58:42.780658 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa'
[0m23:58:42.781940 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:58:42.792728 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:58:42.805303 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:58:42.813076 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:58:42.824068 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m23:58:42.825539 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `workspace`.`staging`.`stg_customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:58:42.826677 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:43.242629 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f628-1b1c-b63e-b3e5fbe6fdc3) - Created
[0m23:58:43.783199 [debug] [Thread-5 (]: SQL status: OK in 0.960 seconds
[0m23:58:43.787410 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f628-1b1c-b63e-b3e5fbe6fdc3, command-id=01f0eced-f638-1e02-926e-a8d317501f85) - Closing
[0m23:58:43.789718 [debug] [Thread-5 (]: On test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m23:58:43.791411 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f628-1b1c-b63e-b3e5fbe6fdc3) - Closing
[0m23:58:43.905400 [info ] [Thread-5 (]: 5 of 12 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.13s]
[0m23:58:43.908803 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m23:58:43.911432 [debug] [Thread-5 (]: Began running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:58:43.913888 [info ] [Thread-5 (]: 6 of 12 START test source_not_null_raw_accounts_AccountID ...................... [RUN]
[0m23:58:43.917088 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175) - Creating connection
[0m23:58:43.919541 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175'
[0m23:58:43.922749 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:58:43.936854 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:58:43.951603 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:58:43.957767 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:58:43.969804 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"
[0m23:58:43.972218 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select AccountID
from `workspace`.`raw`.`accounts`
where AccountID is null



  
  
      
    ) dbt_internal_test
[0m23:58:43.973682 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:44.407783 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f6d9-1c70-bc80-7d10b6ce6b25) - Created
[0m23:58:44.835882 [debug] [Thread-5 (]: SQL status: OK in 0.860 seconds
[0m23:58:44.840363 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f6d9-1c70-bc80-7d10b6ce6b25, command-id=01f0eced-f6eb-1e45-9c92-d12bcf9de08b) - Closing
[0m23:58:44.842850 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175: Close
[0m23:58:44.844665 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f6d9-1c70-bc80-7d10b6ce6b25) - Closing
[0m23:58:44.982352 [info ] [Thread-5 (]: 6 of 12 PASS source_not_null_raw_accounts_AccountID ............................ [[32mPASS[0m in 1.07s]
[0m23:58:44.984249 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.source_not_null_raw_accounts_AccountID.d8f8e0c175
[0m23:58:44.985833 [debug] [Thread-5 (]: Began running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:58:44.987525 [info ] [Thread-5 (]: 7 of 12 START test source_not_null_raw_customers_CustomerID .................... [RUN]
[0m23:58:44.989798 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3) - Creating connection
[0m23:58:44.991283 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3'
[0m23:58:44.992610 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:58:45.000582 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:58:45.016901 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:58:45.022580 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:58:45.036105 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"
[0m23:58:45.038099 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select CustomerID
from `workspace`.`raw`.`customers`
where CustomerID is null



  
  
      
    ) dbt_internal_test
[0m23:58:45.040001 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:45.440810 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f777-1d4a-a99d-9bab0b248b09) - Created
[0m23:58:45.916728 [debug] [Thread-5 (]: SQL status: OK in 0.880 seconds
[0m23:58:45.925986 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f777-1d4a-a99d-9bab0b248b09, command-id=01f0eced-f787-1bb5-b36b-e06bba25edff) - Closing
[0m23:58:45.929254 [debug] [Thread-5 (]: On test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3: Close
[0m23:58:45.931530 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f777-1d4a-a99d-9bab0b248b09) - Closing
[0m23:58:46.073952 [info ] [Thread-5 (]: 7 of 12 PASS source_not_null_raw_customers_CustomerID .......................... [[32mPASS[0m in 1.08s]
[0m23:58:46.077989 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.source_not_null_raw_customers_CustomerID.4ba89b58e3
[0m23:58:46.080556 [debug] [Thread-5 (]: Began running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:58:46.082907 [info ] [Thread-5 (]: 8 of 12 START test unique_stg_accounts_account_id .............................. [RUN]
[0m23:58:46.085551 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m23:58:46.088112 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79'
[0m23:58:46.090550 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:58:46.104204 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:58:46.118750 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:58:46.124810 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:58:46.138964 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"
[0m23:58:46.141114 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:58:46.142773 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:46.527198 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f81d-1c12-9de2-46861a5c7820) - Created
[0m23:58:47.074620 [debug] [Thread-5 (]: SQL status: OK in 0.930 seconds
[0m23:58:47.078081 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f81d-1c12-9de2-46861a5c7820, command-id=01f0eced-f82e-1f4c-9f04-bfeb8e3aaa63) - Closing
[0m23:58:47.080050 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79: Close
[0m23:58:47.081633 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f81d-1c12-9de2-46861a5c7820) - Closing
[0m23:58:47.199559 [info ] [Thread-5 (]: 8 of 12 PASS unique_stg_accounts_account_id .................................... [[32mPASS[0m in 1.11s]
[0m23:58:47.201166 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.unique_stg_accounts_account_id.cdf6252c79
[0m23:58:47.202242 [debug] [Thread-5 (]: Began running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:58:47.203285 [info ] [Thread-5 (]: 9 of 12 START test unique_stg_customers_customer_id ............................ [RUN]
[0m23:58:47.205111 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.banking_pipeline.unique_stg_customers_customer_id.c7614daada) - Creating connection
[0m23:58:47.206438 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.banking_pipeline.unique_stg_customers_customer_id.c7614daada'
[0m23:58:47.207520 [debug] [Thread-5 (]: Began compiling node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:58:47.215370 [debug] [Thread-5 (]: Writing injected SQL for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:58:47.227527 [debug] [Thread-5 (]: Began executing node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:58:47.231621 [debug] [Thread-5 (]: Writing runtime sql for node "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:58:47.247818 [debug] [Thread-5 (]: Using databricks connection "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"
[0m23:58:47.249192 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "test.banking_pipeline.unique_stg_customers_customer_id.c7614daada"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `workspace`.`staging`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:58:47.250344 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:47.727602 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f8cc-12fb-8ee3-842ec588e6bd) - Created
[0m23:58:48.251169 [debug] [Thread-5 (]: SQL status: OK in 1.000 seconds
[0m23:58:48.255726 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f8cc-12fb-8ee3-842ec588e6bd, command-id=01f0eced-f8e6-1e3e-ad6f-d5a26b14c858) - Closing
[0m23:58:48.258040 [debug] [Thread-5 (]: On test.banking_pipeline.unique_stg_customers_customer_id.c7614daada: Close
[0m23:58:48.259472 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f8cc-12fb-8ee3-842ec588e6bd) - Closing
[0m23:58:48.388529 [info ] [Thread-5 (]: 9 of 12 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.18s]
[0m23:58:48.397013 [debug] [Thread-5 (]: Finished running node test.banking_pipeline.unique_stg_customers_customer_id.c7614daada
[0m23:58:48.402411 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:58:48.407024 [info ] [Thread-5 (]: 10 of 12 START unit_test account_interest_summary::interest_rate_with_loan_mid_balance  [RUN]
[0m23:58:48.410875 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance) - Creating connection
[0m23:58:48.413412 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance'
[0m23:58:48.415655 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:58:48.417773 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:58:48.467595 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:48.469426 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:58:48.471053 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:48.887222 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f984-1393-918b-d135b2d72e9b) - Created
[0m23:58:49.262297 [debug] [Thread-5 (]: SQL status: OK in 0.790 seconds
[0m23:58:49.265409 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f984-1393-918b-d135b2d72e9b, command-id=01f0eced-f996-1d0f-a982-b2caf61d22e9) - Closing
[0m23:58:49.290259 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.316565 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.380874 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.382925 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  
    create or replace temporary view `interest_rate_with_loan_mid_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:58:49.679368 [debug] [Thread-5 (]: SQL status: OK in 0.290 seconds
[0m23:58:49.685991 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f984-1393-918b-d135b2d72e9b, command-id=01f0eced-f9e2-13d5-bfb1-aebb8f23e8cb) - Closing
[0m23:58:49.694777 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.697067 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_with_loan_mid_balance__dbt_tmp` AS JSON

  
[0m23:58:49.909045 [debug] [Thread-5 (]: SQL status: OK in 0.210 seconds
[0m23:58:49.915094 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f984-1393-918b-d135b2d72e9b, command-id=01f0eced-fa11-194f-9e22-9ccab98be58c) - Closing
[0m23:58:49.939229 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.959472 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:49.961082 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT001' as string)
 as `account_id`, cast(201 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(15000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(201 as int)
 as `customer_id`, cast('UT001' as string)
 as `account_id`, cast(15000 as decimal(12, 2))
 as `original_balance`, cast(0.02 as decimal(14, 3))
 as `interest_rate`, cast(300 as decimal(27, 5))
 as `annual_interest_amount`, cast(15300 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:58:50.343589 [debug] [Thread-5 (]: SQL status: OK in 0.380 seconds
[0m23:58:50.352494 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f984-1393-918b-d135b2d72e9b, command-id=01f0eced-fa3b-1a26-b394-de833c4edcf3) - Closing
[0m23:58:50.366732 [debug] [Thread-5 (]: Applying DROP to: `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:58:50.374378 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"
[0m23:58:50.376234 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance"} */
DROP VIEW IF EXISTS `interest_rate_with_loan_mid_balance__dbt_tmp`
[0m23:58:50.646555 [debug] [Thread-5 (]: SQL status: OK in 0.270 seconds
[0m23:58:50.650573 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-f984-1393-918b-d135b2d72e9b, command-id=01f0eced-fa79-1f69-9db0-f9ab319182a4) - Closing
[0m23:58:50.662953 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance: Close
[0m23:58:50.665668 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-f984-1393-918b-d135b2d72e9b) - Closing
[0m23:58:50.807961 [info ] [Thread-5 (]: 10 of 12 PASS account_interest_summary::interest_rate_with_loan_mid_balance .... [[32mPASS[0m in 2.40s]
[0m23:58:50.810967 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_with_loan_mid_balance
[0m23:58:50.813150 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:58:50.815005 [info ] [Thread-5 (]: 11 of 12 START unit_test account_interest_summary::interest_rate_without_loan_low_balance  [RUN]
[0m23:58:50.817312 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance) - Creating connection
[0m23:58:50.819097 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance'
[0m23:58:50.820820 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:58:50.822782 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:58:50.840617 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:50.842413 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:58:50.843825 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:51.235812 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae) - Created
[0m23:58:51.551238 [debug] [Thread-5 (]: SQL status: OK in 0.710 seconds
[0m23:58:51.565375 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae, command-id=01f0eced-fafc-18ef-b37d-6c5f2d4b4dd5) - Closing
[0m23:58:51.577443 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:51.606089 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:51.633650 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:51.635635 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  
    create or replace temporary view `interest_rate_without_loan_low_balance__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:58:51.941246 [debug] [Thread-5 (]: SQL status: OK in 0.300 seconds
[0m23:58:51.944580 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae, command-id=01f0eced-fb3a-10cf-8b6a-7e15542d974a) - Closing
[0m23:58:51.950141 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:51.951956 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */

    
  DESCRIBE TABLE EXTENDED `interest_rate_without_loan_low_balance__dbt_tmp` AS JSON

  
[0m23:58:52.225855 [debug] [Thread-5 (]: SQL status: OK in 0.270 seconds
[0m23:58:52.229143 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae, command-id=01f0eced-fb69-1a5c-ac10-5cf51dec3565) - Closing
[0m23:58:52.233110 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:52.247733 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:52.248982 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT002' as string)
 as `account_id`, cast(202 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(False as boolean)
 as `has_loan`, cast(5000 as decimal(12, 2))
 as `balance`, cast('savings' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select cast(202 as int)
 as `customer_id`, cast('UT002' as string)
 as `account_id`, cast(5000 as decimal(12, 2))
 as `original_balance`, cast(0.01 as decimal(14, 3))
 as `interest_rate`, cast(50 as decimal(27, 5))
 as `annual_interest_amount`, cast(5050 as decimal(28, 5))
 as `new_balance`
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:58:52.608137 [debug] [Thread-5 (]: SQL status: OK in 0.360 seconds
[0m23:58:52.615065 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae, command-id=01f0eced-fb97-1332-975b-293cc2c3e4f5) - Closing
[0m23:58:52.621149 [debug] [Thread-5 (]: Applying DROP to: `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:58:52.624454 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"
[0m23:58:52.626384 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance"} */
DROP VIEW IF EXISTS `interest_rate_without_loan_low_balance__dbt_tmp`
[0m23:58:52.871339 [debug] [Thread-5 (]: SQL status: OK in 0.240 seconds
[0m23:58:52.874694 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae, command-id=01f0eced-fbd0-170e-8912-03676f2a69b3) - Closing
[0m23:58:52.879433 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance: Close
[0m23:58:52.880766 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-faeb-1b5e-9693-e25a5cf054ae) - Closing
[0m23:58:53.052316 [info ] [Thread-5 (]: 11 of 12 PASS account_interest_summary::interest_rate_without_loan_low_balance . [[32mPASS[0m in 2.23s]
[0m23:58:53.058535 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.interest_rate_without_loan_low_balance
[0m23:58:53.062305 [debug] [Thread-5 (]: Began running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:58:53.065662 [info ] [Thread-5 (]: 12 of 12 START unit_test account_interest_summary::non_savings_account_filtered_out  [RUN]
[0m23:58:53.069532 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out) - Creating connection
[0m23:58:53.073116 [debug] [Thread-5 (]: Acquiring new databricks connection 'unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out'
[0m23:58:53.077240 [debug] [Thread-5 (]: Began compiling node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:58:53.080581 [debug] [Thread-5 (]: Began executing node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:58:53.111313 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:53.113340 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`intermediate`.`int_customer_accounts` AS JSON

  
[0m23:58:53.115156 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m23:58:53.524841 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-fc49-1519-b50f-4df961524d9a) - Created
[0m23:58:53.822934 [debug] [Thread-5 (]: SQL status: OK in 0.710 seconds
[0m23:58:53.831485 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fc59-1fb5-bba5-d2a179fed264) - Closing
[0m23:58:53.842993 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:53.867928 [debug] [Thread-5 (]: Writing injected SQL for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:53.896056 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:53.897946 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

    
[0m23:58:54.192678 [debug] [Thread-5 (]: SQL status: OK in 0.290 seconds
[0m23:58:54.197326 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fc94-1f11-a41a-990a96e72ac8) - Closing
[0m23:58:54.201287 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:54.203122 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  
    create or replace temporary view `non_savings_account_filtered_out__dbt_tmp` as
      select * from (
        with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
    ) as __dbt_sbq
    where false
    limit 0

  
  
[0m23:58:54.511243 [debug] [Thread-5 (]: SQL status: OK in 0.310 seconds
[0m23:58:54.514999 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fcc1-1b3c-80c3-9109a29d09f4) - Closing
[0m23:58:54.521279 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:54.524170 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */

    
  DESCRIBE TABLE EXTENDED `non_savings_account_filtered_out__dbt_tmp` AS JSON

  
[0m23:58:54.763746 [debug] [Thread-5 (]: SQL status: OK in 0.240 seconds
[0m23:58:54.770043 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fcf3-1318-a09c-bf8f6b91beba) - Closing
[0m23:58:54.773927 [debug] [Thread-5 (]: Writing runtime sql for node "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:54.795667 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:54.797250 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    `customer_id`,`account_id`,`original_balance`,`interest_rate`,`annual_interest_amount`,`new_balance`, 'actual' as `actual_or_expected`
  from (
    with __dbt__cte__int_customer_accounts as (

-- Fixture for int_customer_accounts
select cast('UT003' as string)
 as `account_id`, cast(203 as int)
 as `customer_id`, cast(null as string) as `customer_name`, cast(True as boolean)
 as `has_loan`, cast(20000 as decimal(12, 2))
 as `balance`, cast('checking' as string)
 as `account_type`
) select
    customer_id,
    account_id,
    balance as original_balance,

    case
        when balance < 10000 then 0.01
        when balance between 10000 and 20000 then 0.015
        when balance > 20000 then 0.02
    end
    +
    case when has_loan then 0.005 else 0 end
    as interest_rate,

    balance *
    (
        case
            when balance < 10000 then 0.01
            when balance between 10000 and 20000 then 0.015
            when balance > 20000 then 0.02
        end
        +
        case when has_loan then 0.005 else 0 end
    ) as annual_interest_amount,

    balance +
    (
        balance *
        (
            case
                when balance < 10000 then 0.01
                when balance between 10000 and 20000 then 0.015
                when balance > 20000 then 0.02
            end
            +
            case when has_loan then 0.005 else 0 end
        )
    ) as new_balance

from __dbt__cte__int_customer_accounts
WHERE account_type = 'savings'
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    `customer_id`, `account_id`, `original_balance`, `interest_rate`, `annual_interest_amount`, `new_balance`, 'expected' as `actual_or_expected`
  from (
    select * from dbt_internal_unit_test_actual
    limit 0
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m23:58:55.076876 [debug] [Thread-5 (]: SQL status: OK in 0.280 seconds
[0m23:58:55.081051 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fd1b-1f36-8c19-63f540c5ff49) - Closing
[0m23:58:55.086961 [debug] [Thread-5 (]: Applying DROP to: `non_savings_account_filtered_out__dbt_tmp`
[0m23:58:55.088910 [debug] [Thread-5 (]: Using databricks connection "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"
[0m23:58:55.091297 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: /* {"app": "dbt", "dbt_version": "1.10.18", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "banking_dbt", "target_name": "dev", "node_id": "unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out"} */
DROP VIEW IF EXISTS `non_savings_account_filtered_out__dbt_tmp`
[0m23:58:55.360066 [debug] [Thread-5 (]: SQL status: OK in 0.270 seconds
[0m23:58:55.362318 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0eced-fc49-1519-b50f-4df961524d9a, command-id=01f0eced-fd49-1592-869e-8bea852bb942) - Closing
[0m23:58:55.365410 [debug] [Thread-5 (]: On unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out: Close
[0m23:58:55.367097 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0eced-fc49-1519-b50f-4df961524d9a) - Closing
[0m23:58:55.511222 [info ] [Thread-5 (]: 12 of 12 PASS account_interest_summary::non_savings_account_filtered_out ....... [[32mPASS[0m in 2.44s]
[0m23:58:55.513574 [debug] [Thread-5 (]: Finished running node unit_test.banking_pipeline.account_interest_summary.non_savings_account_filtered_out
[0m23:58:55.517177 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m23:58:55.518564 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:58:55.520617 [info ] [MainThread]: 
[0m23:58:55.522105 [info ] [MainThread]: Finished running 9 data tests, 3 unit tests in 0 hours 0 minutes and 22.05 seconds (22.05s).
[0m23:58:55.528059 [debug] [MainThread]: Command end result
[0m23:58:55.840937 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/dagster/app/dbt/target/manifest.json
[0m23:58:55.849370 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/dagster/app/dbt/target/semantic_manifest.json
[0m23:58:55.867196 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/dagster/app/dbt/target/run_results.json
[0m23:58:55.868591 [info ] [MainThread]: 
[0m23:58:55.869981 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:58:55.871305 [info ] [MainThread]: 
[0m23:58:55.872745 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m23:58:55.876104 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 25.95023, "process_in_blocks": "0", "process_kernel_time": 0.714962, "process_mem_max_rss": "256048", "process_out_blocks": "0", "process_user_time": 7.632925}
[0m23:58:55.877696 [debug] [MainThread]: Command `dbt test` succeeded at 23:58:55.877579 after 25.95 seconds
[0m23:58:55.879266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733201c9e230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733201eed6f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73320123a1a0>]}
[0m23:58:55.880774 [debug] [MainThread]: Flushing usage events
[0m23:58:56.360417 [debug] [MainThread]: An error was encountered while trying to flush usage events
